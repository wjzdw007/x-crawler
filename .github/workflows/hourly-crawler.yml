name: Hourly X Crawler

on:
  # å®šæ—¶ä»»åŠ¡ - æ¯2å°æ—¶è¿è¡Œä¸€æ¬¡
  schedule:
    - cron: '0 */2 * * *'  # æ¯2å°æ—¶çš„æ•´ç‚¹è¿è¡Œ (00:00, 02:00, 04:00, ...)

  # æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:
    inputs:
      tweet_count:
        description: 'è¦çˆ¬å–çš„æ¨æ–‡æ•°é‡'
        required: false
        default: '200'

jobs:
  crawl-only:
    runs-on: ubuntu-latest
    permissions:
      contents: write      # å…è®¸æ¨é€ä»£ç 

    steps:
      # 1. æ£€å‡ºä»£ç 
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 2. è®¾ç½® Python ç¯å¢ƒ
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # 3. å®‰è£… Python ä¾èµ–
      - name: ğŸ“š Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. å®‰è£… Playwright å’Œç³»ç»Ÿä¾èµ–
      - name: ğŸ­ Install Playwright with dependencies
        run: |
          playwright install --with-deps chromium

      # 5. é…ç½®ç¯å¢ƒå˜é‡
      - name: âš™ï¸ Configure environment
        run: |
          cat > .env << EOF
          # Xå¹³å°è®¤è¯
          X_AUTH_TOKEN=${{ secrets.X_AUTH_TOKEN }}
          X_CT0_TOKEN=${{ secrets.X_CT0_TOKEN }}
          X_CSRF_TOKEN=${{ secrets.X_CSRF_TOKEN }}
          X_BEARER_TOKEN=${{ secrets.X_BEARER_TOKEN }}

          # LLMé…ç½® (æŠ“å–ä¸éœ€è¦ï¼Œä½†é…ç½®æ–‡ä»¶å¯èƒ½éœ€è¦)
          OPENROUTER_API_KEY=${{ secrets.OPENROUTER_API_KEY }}
          OPENAI_MODEL=${{ secrets.OPENAI_MODEL }}

          # ä»£ç†é…ç½® (å¯é€‰)
          HTTP_PROXY=${{ secrets.HTTP_PROXY }}
          HTTPS_PROXY=${{ secrets.HTTPS_PROXY }}

          # çˆ¬è™«é…ç½®
          REQUESTS_PER_HOUR=400
          DAILY_TWEET_COUNT=100
          DATA_DIR=crawler_data
          EOF

      # 6. è¿è¡Œçˆ¬è™« (åªæŠ“å–ï¼Œä¸ç”Ÿæˆæ€»ç»“)
      - name: ğŸ•·ï¸ Crawl tweets (no summary)
        run: |
          TWEET_COUNT="${{ github.event.inputs.tweet_count || '200' }}"
          echo "æŠ“å– $TWEET_COUNT æ¡æ¨æ–‡ (ä¸ç”Ÿæˆæ€»ç»“)"
          python run_crawler.py --count $TWEET_COUNT
        continue-on-error: true

      # 7. ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š
      - name: ğŸ“Š Generate report
        if: always()
        run: |
          echo "## ğŸ¤– æŠ“å–è¿è¡ŒæŠ¥å‘Š" > report.md
          echo "" >> report.md
          echo "- ğŸ• è¿è¡Œæ—¶é—´: $(date +'%Y-%m-%d %H:%M:%S')" >> report.md
          echo "- ğŸ¯ ç›®æ ‡æ•°é‡: ${{ github.event.inputs.tweet_count || '200' }} æ¡" >> report.md
          echo "- âš¡ ä»»åŠ¡ç±»å‹: æ•°æ®æŠ“å– (ä¸ç”Ÿæˆæ€»ç»“)" >> report.md
          echo "" >> report.md

          # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶
          if [ -d "crawler_data/users_daily" ]; then
            USER_COUNT=$(ls -1 crawler_data/users_daily/*.json 2>/dev/null | wc -l)
            echo "- ğŸ‘¥ ç”¨æˆ·æ•°æ®æ–‡ä»¶: $USER_COUNT ä¸ª" >> report.md
          fi

          if [ -d "crawler_data/daily_posts" ]; then
            POST_COUNT=$(ls -1 crawler_data/daily_posts/*.json 2>/dev/null | wc -l)
            echo "- ğŸ“ æ—¥å¿—æ–‡ä»¶: $POST_COUNT ä¸ª" >> report.md
          fi

          cat report.md

      # 8. æäº¤æ•°æ®åˆ°ä»“åº“
      - name: ğŸ’¾ Commit and push data
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # åªæ·»åŠ çˆ¬è™«æ•°æ®æ–‡ä»¶
          git add crawler_data/

          # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
          if git diff --staged --quiet; then
            echo "æ²¡æœ‰æ–°æ•°æ®éœ€è¦æäº¤"
          else
            git commit -m "ğŸ¤– æŠ“å–æ›´æ–°: $(date +'%Y-%m-%d %H:%M') æ•°æ®"
            git push
          fi

      # 9. ä¸Šä¼ è¿è¡ŒæŠ¥å‘Š
      - name: ğŸ“¤ Upload report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: crawler-report-${{ github.run_number }}
          path: |
            report.md
            crawler_data/daily_posts/*.json
          retention-days: 7

      # 10. å¤±è´¥é€šçŸ¥
      - name: ğŸ› Create Issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ğŸš¨ Hourly Crawler Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## æŠ“å–å¤±è´¥æŠ¥å‘Š

              **æ—¶é—´**: ${new Date().toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai' })}
              **å·¥ä½œæµ**: ${context.workflow}

              ğŸ”— [æŸ¥çœ‹è¿è¡Œæ—¥å¿—](${context.payload.repository.html_url}/actions/runs/${context.runId})

              ### å¯èƒ½åŸå› 
              - [ ] X è®¤è¯ token è¿‡æœŸ
              - [ ] ç½‘ç»œè¿æ¥é—®é¢˜
              - [ ] API é™æµ

              ---
              *æ­¤ Issue ç”± GitHub Actions è‡ªåŠ¨åˆ›å»º*`,
              labels: ['bot', 'crawler-failure']
            });
