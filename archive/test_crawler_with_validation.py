#!/usr/bin/env python3
"""
çˆ¬è™«é›†æˆæµ‹è¯• - ä½¿ç”¨éªŒè¯ç³»ç»Ÿç¡®ä¿æ•°æ®è´¨é‡
"""

from crawler import XCrawler
from tools.validator import DataValidator
import json
from datetime import datetime

def test_crawler_with_validation():
    print("ğŸ§ª çˆ¬è™«é›†æˆæµ‹è¯• - åŒ…å«æ•°æ®è´¨é‡éªŒè¯")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XCrawler()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = DataValidator()
    
    print("\nâš ï¸ æ³¨æ„ï¼šæ­¤æµ‹è¯•éœ€è¦æœ‰æ•ˆçš„è®¤è¯ä¿¡æ¯")
    print("è¯·ç¡®ä¿å·²é…ç½®ä»¥ä¸‹å†…å®¹ï¼š")
    print("1. æœ‰æ•ˆçš„cookies (auth_token, ct0)")
    print("2. æ­£ç¡®çš„Bearer token")
    print("3. å…¶ä»–å¿…éœ€çš„è®¤è¯headers")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªå°è§„æ¨¡çš„APIå“åº”æ¥æµ‹è¯•è§£æé€»è¾‘
    print("\nğŸ”„ æµ‹è¯•1: è§£æé€»è¾‘éªŒè¯")
    
    # ä½¿ç”¨ä¸é»„é‡‘æ•°æ®é›†ç›¸åŒçš„APIå“åº”æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    test_response_file = "analysis_data/api_responses/response_213433_687388.json"
    
    try:
        with open(test_response_file, 'r', encoding='utf-8') as f:
            response_data = json.load(f)
        
        # æå–æ•°æ®
        api_data = response_data.get('data', {})
        tweets = crawler.extract_tweets_from_response(api_data)
        
        print(f"âœ… æˆåŠŸæå– {len(tweets)} æ¡æ¨æ–‡")
        
        # æ˜¾ç¤ºå‰3æ¡æ¨æ–‡æ‘˜è¦
        for i, tweet in enumerate(tweets[:3]):
            print(f"  æ¨æ–‡ {i+1}:")
            print(f"    ID: {tweet.get('id')}")
            print(f"    æ–‡æœ¬: {tweet.get('text', '')[:50]}...")
            print(f"    ç”¨æˆ·: {tweet.get('user', {}).get('name', 'N/A')} (@{tweet.get('user', {}).get('screen_name', 'N/A')})")
            print(f"    åª’ä½“: {len(tweet.get('media', []))} ä¸ªæ–‡ä»¶")
            
        # éªŒè¯æ•°æ®è´¨é‡
        print(f"\nğŸ” æ•°æ®è´¨é‡éªŒè¯:")
        golden_dataset = "golden_dataset/golden_dataset_20250911_220234.json"
        validation_results = validator.comprehensive_validation(tweets, golden_dataset)
        
        # æ‰“å°éªŒè¯ç»“æœ
        validator.print_validation_summary(validation_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = validator.generate_validation_report(validation_results, tweets)
        
        # æµ‹è¯•ç»“æœè¯„ä¼°
        scores = [result.score for result in validation_results.values()]
        overall_score = sum(scores) / len(scores) if scores else 0
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"æ€»ä½“è¯„åˆ†: {overall_score:.1f}/100")
        
        if overall_score >= 80:
            print("âœ… æµ‹è¯•é€šè¿‡ - çˆ¬è™«è§£æé€»è¾‘å·¥ä½œæ­£å¸¸")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ - éœ€è¦ä¿®å¤è§£æé€»è¾‘")
            
        print(f"è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ”„ æµ‹è¯•2: è®¤è¯çŠ¶æ€æ£€æŸ¥")
    
    # æ£€æŸ¥æ˜¯å¦èƒ½è¿›è¡Œå®é™…çš„HTTPè¯·æ±‚
    print("æ£€æŸ¥è®¤è¯é…ç½®...")
    
    # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„è®¤è¯æ£€æŸ¥é€»è¾‘
    # ä½†ç›®å‰æˆ‘ä»¬æ²¡æœ‰çœŸå®çš„cookiesï¼Œæ‰€ä»¥è·³è¿‡
    print("âš ï¸ è·³è¿‡å®é™…HTTPè¯·æ±‚æµ‹è¯• - éœ€è¦æœ‰æ•ˆè®¤è¯")
    
    print("\nğŸ“‹ åç»­æ­¥éª¤:")
    print("1. é…ç½®æœ‰æ•ˆçš„è®¤è¯ä¿¡æ¯")
    print("2. æµ‹è¯•å®é™…çš„æ—¶é—´çº¿APIè¯·æ±‚")
    print("3. éªŒè¯åˆ†é¡µå’Œæ¸¸æ ‡æœºåˆ¶")
    print("4. å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘")
    print("5. å»ºç«‹ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶")

def extract_cookies_instruction():
    """æä¾›æå–cookiesçš„æŒ‡å¯¼"""
    print("\nğŸ”‘ è®¤è¯ä¿¡æ¯é…ç½®æŒ‡å¯¼:")
    print("1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ https://x.com/home")
    print("2. æ‰“å¼€å¼€å‘è€…å·¥å…· (F12)")
    print("3. åˆ‡æ¢åˆ°Networkæ ‡ç­¾")
    print("4. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°æ—¶é—´çº¿APIè¯·æ±‚")
    print("5. å¤åˆ¶è¯·æ±‚å¤´ä¸­çš„ä»¥ä¸‹ä¿¡æ¯:")
    print("   - Cookie: auth_token=...; ct0=...")
    print("   - Authorization: Bearer ...")
    print("   - X-Csrf-Token: ...")
    print("6. å°†è¿™äº›ä¿¡æ¯é…ç½®åˆ°çˆ¬è™«ä¸­")

if __name__ == "__main__":
    test_crawler_with_validation()
    extract_cookies_instruction()