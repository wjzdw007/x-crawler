#!/usr/bin/env python3
"""
X HTTPçˆ¬è™« - åŸºäºæ·±åº¦åˆ†æç»“æœå®ç°
è§£å†³äº†æ‰€æœ‰å·²çŸ¥æ ¸å¿ƒé—®é¢˜ï¼š
1. âœ… å¸–å­å†…å®¹å®Œæ•´æ€§ - ä½¿ç”¨ legacy.full_text
2. âœ… è½¬æ¨åŸå¸–è·å– - ä½¿ç”¨ retweeted_status_result
3. âœ… åª’ä½“æ–‡ä»¶URL - ä½¿ç”¨ extended_entities.media
4. âœ… æ•°æ®å‡†ç¡®æ€§ - æ­£ç¡®çš„GraphQLè·¯å¾„
"""

import json
import requests
import time
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
import random

class XCrawler:
    def __init__(self, data_dir="crawler_data", config_file="config.json"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
        for subdir in ["daily_posts", "media", "summaries", "logs"]:
            (self.data_dir / subdir).mkdir(exist_ok=True)
        
        self.config_file = Path(config_file)
        self.config = self.load_config()
        self.session = requests.Session()
        self.setup_session()
        
        # APIç«¯ç‚¹ - åŸºäºåˆ†æç»“æœ
        self.api_endpoints = {
            "recommended": "xNGIIoXaz9DyeBXBfn3AjA/HomeLatestTimeline",
            "following": "1_nms9JVtHQxTw8VwZJciQ/HomeTimeline"
        }
        
        # åŸºç¡€URL
        self.base_url = "https://x.com/i/api/graphql"
        
        # è¯·æ±‚è®¡æ•°å’Œé™æµ
        self.request_count = 0
        self.last_request_time = 0
        self.rate_limit = self.config.get('settings', {}).get('requests_per_hour', 400)
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "authentication": {"cookies": {}, "headers": {}},
            "settings": {"requests_per_hour": 400, "retry_attempts": 3, "timeout": 30}
        }
        
    def setup_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        # åŸºç¡€headers - æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://x.com/home',
            'Origin': 'https://x.com',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
        })
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¤è¯ä¿¡æ¯
        self.load_authentication()
    
    def load_authentication(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½è®¤è¯ä¿¡æ¯"""
        auth_config = self.config.get('authentication', {})
        
        # åŠ è½½cookies
        cookies = auth_config.get('cookies', {})
        for key, value in cookies.items():
            if value and value != f"YOUR_{key.upper()}_HERE":
                self.session.cookies.set(key, value)
        
        # åŠ è½½headers
        headers = auth_config.get('headers', {})
        for key, value in headers.items():
            if value and not value.startswith("YOUR_"):
                self.session.headers[key] = value
        
        # æ£€æŸ¥è®¤è¯æ˜¯å¦é…ç½®
        has_auth = any(
            v and not str(v).startswith("YOUR_") 
            for v in {**cookies, **headers}.values()
        )
        
        if not has_auth:
            print("âš ï¸ æœªæ£€æµ‹åˆ°è®¤è¯é…ç½®")
            print("è¯·å¤åˆ¶ config_template.json ä¸º config.json å¹¶å¡«å…¥æ­£ç¡®çš„è®¤è¯ä¿¡æ¯")
    
    def rate_limit_check(self):
        """æ£€æŸ¥å’Œæ‰§è¡Œé™æµ"""
        current_time = time.time()
        
        # æ¯å°æ—¶é‡ç½®è®¡æ•°
        if current_time - self.last_request_time > 3600:
            self.request_count = 0
        
        # æ£€æŸ¥é™æµ
        if self.request_count >= self.rate_limit:
            wait_time = 3600 - (current_time - self.last_request_time)
            if wait_time > 0:
                print(f"â° è¾¾åˆ°é™æµä¸Šé™ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
                time.sleep(wait_time)
                self.request_count = 0
        
        # éšæœºå»¶è¿Ÿé˜²æ­¢æ£€æµ‹
        delay = random.uniform(1.0, 3.0)
        time.sleep(delay)
        
        self.request_count += 1
        self.last_request_time = current_time
    
    def get_timeline_params(self, timeline_type: str = "recommended", cursor: Optional[str] = None) -> Dict:
        """è·å–æ—¶é—´çº¿è¯·æ±‚å‚æ•°"""
        variables = {
            "count": 20,
            "includePromotedContent": True,
            "latestControlAvailable": True,
            "requestContext": "launch",
            "withCommunity": True
        }
        
        if cursor:
            variables["cursor"] = cursor
        
        features = {
            "rweb_video_screen_enabled": False,
            "payments_enabled": False,
            "profile_label_improvements_pcf_label_in_post_enabled": True,
            "rweb_tipjar_consumption_enabled": True,
            "verified_phone_label_enabled": False,
            "creator_subscriptions_tweet_preview_api_enabled": True,
            "responsive_web_graphql_timeline_navigation_enabled": True,
            "responsive_web_graphql_skip_user_profile_image_extensions_enabled": False,
            "premium_content_api_read_enabled": False,
            "communities_web_enable_tweet_community_results_fetch": True,
            "c9s_tweet_anatomy_moderator_badge_enabled": True,
            "responsive_web_grok_analyze_button_fetch_trends_enabled": False,
            "responsive_web_grok_analyze_post_followups_enabled": True,
            "responsive_web_jetfuel_frame": True,
            "responsive_web_grok_share_attachment_enabled": True,
            "articles_preview_enabled": True,
            "responsive_web_edit_tweet_api_enabled": True,
            "graphql_is_translatable_rweb_tweet_is_translatable_enabled": True,
            "view_counts_everywhere_api_enabled": True,
            "longform_notetweets_consumption_enabled": True,
            "responsive_web_twitter_article_tweet_consumption_enabled": True,
            "tweet_awards_web_tipping_enabled": False,
            "responsive_web_grok_show_grok_translated_post": False,
            "responsive_web_grok_analysis_button_from_backend": True,
            "creator_subscriptions_quote_tweet_preview_enabled": False,
            "freedom_of_speech_not_reach_fetch_enabled": True,
            "standardized_nudges_misinfo": True,
            "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": True,
            "longform_notetweets_rich_text_read_enabled": True,
            "longform_notetweets_inline_media_enabled": True,
            "responsive_web_grok_image_annotation_enabled": True,
            "responsive_web_grok_imagine_annotation_enabled": True,
            "responsive_web_grok_community_note_auto_translation_is_enabled": False,
            "responsive_web_enhance_cards_enabled": False
        }
        
        return {
            "variables": json.dumps(variables),
            "features": json.dumps(features)
        }
    
    def make_timeline_request(self, timeline_type: str = "recommended", cursor: Optional[str] = None) -> Optional[Dict]:
        """å‘èµ·æ—¶é—´çº¿è¯·æ±‚"""
        self.rate_limit_check()
        
        endpoint = self.api_endpoints[timeline_type]
        url = f"{self.base_url}/{endpoint}"
        params = self.get_timeline_params(timeline_type, cursor)
        
        try:
            print(f"ğŸ”„ è¯·æ±‚ {timeline_type} æ—¶é—´çº¿...")
            response = self.session.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                print("âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾…åé‡è¯•...")
                time.sleep(60)
                return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text[:200]}")
                return None
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def parse_tweet(self, tweet_data: Dict) -> Optional[Dict]:
        """è§£ææ¨æ–‡æ•°æ® - åŸºäºåˆ†æç»“æœçš„å®Œæ•´å®ç°"""
        try:
            # åŸºç¡€æ¨æ–‡ä¿¡æ¯
            tweet = {
                'id': tweet_data.get('rest_id'),
                'text': '',
                'created_at': tweet_data.get('legacy', {}).get('created_at'),
                'lang': tweet_data.get('legacy', {}).get('lang'),
                'media': [],
                'retweet': None,
                'quoted': None,
                'user': None,
                'stats': {
                    'retweet_count': tweet_data.get('legacy', {}).get('retweet_count', 0),
                    'favorite_count': tweet_data.get('legacy', {}).get('favorite_count', 0),
                    'reply_count': tweet_data.get('legacy', {}).get('reply_count', 0),
                    'quote_count': tweet_data.get('legacy', {}).get('quote_count', 0)
                }
            }
            
            # æå–æ–‡æœ¬å†…å®¹ - å¤„ç†é•¿æ–‡æ¨æ–‡å’Œæ™®é€šæ¨æ–‡
            if 'note_tweet' in tweet_data:
                # é•¿æ–‡æ¨æ–‡
                note_tweet_result = tweet_data.get('note_tweet', {}).get('note_tweet_results', {}).get('result', {})
                if note_tweet_result:
                    tweet['text'] = note_tweet_result.get('text', '')
            
            if not tweet['text']:
                # æ™®é€šæ¨æ–‡ - ä½¿ç”¨ legacy.full_text
                tweet['text'] = tweet_data.get('legacy', {}).get('full_text', '')
            
            # è§£æç”¨æˆ·ä¿¡æ¯ - ä¿®æ­£å­—æ®µè·¯å¾„
            user_results = tweet_data.get('core', {}).get('user_results', {}).get('result', {})
            if user_results:
                tweet['user'] = {
                    'id': user_results.get('rest_id'),
                    'name': user_results.get('core', {}).get('name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    'screen_name': user_results.get('core', {}).get('screen_name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    'description': user_results.get('legacy', {}).get('description'),
                    'followers_count': user_results.get('legacy', {}).get('followers_count', 0),
                    'friends_count': user_results.get('legacy', {}).get('friends_count', 0),
                    'verified': user_results.get('verification', {}).get('verified', False),  # ä¿®æ­£è·¯å¾„
                    'is_blue_verified': user_results.get('is_blue_verified', False)
                }
            
            # è§£æåª’ä½“æ–‡ä»¶ - åŸºäºåˆ†æç»“æœ
            extended_entities = tweet_data.get('legacy', {}).get('extended_entities', {})
            if 'media' in extended_entities:
                for media_item in extended_entities['media']:
                    media_entry = {
                        'type': media_item.get('type'),
                        'id': media_item.get('id_str'),
                        'url': None
                    }
                    
                    if media_item['type'] == 'video':
                        # è§†é¢‘å¤„ç† - é€‰æ‹©æœ€é«˜è´¨é‡
                        variants = media_item.get('video_info', {}).get('variants', [])
                        best_variant = None
                        highest_bitrate = 0
                        
                        for variant in variants:
                            if variant.get('content_type') == 'video/mp4':
                                bitrate = variant.get('bitrate', 0)
                                if bitrate > highest_bitrate:
                                    highest_bitrate = bitrate
                                    best_variant = variant
                        
                        if best_variant:
                            media_entry['url'] = best_variant['url']
                            media_entry['bitrate'] = best_variant.get('bitrate')
                    
                    elif media_item['type'] in ['photo', 'animated_gif']:
                        # å›¾ç‰‡å¤„ç†
                        media_entry['url'] = media_item.get('media_url_https')
                    
                    if media_entry['url']:
                        tweet['media'].append(media_entry)
            
            # å¤„ç†è½¬æ¨ - åŸºäºåˆ†æç»“æœï¼Œæ”¯æŒTweetWithVisibilityResultsç»“æ„
            if 'retweeted_status_result' in tweet_data.get('legacy', {}):
                retweet_result = tweet_data['legacy']['retweeted_status_result'].get('result')
                if retweet_result:
                    # å¤„ç†TweetWithVisibilityResultsç»“æ„
                    if retweet_result.get('__typename') == 'TweetWithVisibilityResults':
                        # æ•°æ®åµŒå¥—åœ¨tweetå­—æ®µä¸­
                        actual_tweet = retweet_result.get('tweet')
                        if actual_tweet:
                            tweet['retweet'] = self.parse_tweet(actual_tweet)
                    else:
                        # æ™®é€šTweetç»“æ„
                        tweet['retweet'] = self.parse_tweet(retweet_result)
            
            # å¤„ç†å¼•ç”¨æ¨æ–‡
            if 'quoted_status_result' in tweet_data:
                quoted_result = tweet_data['quoted_status_result'].get('result')
                if quoted_result:
                    tweet['quoted'] = self.parse_tweet(quoted_result)
            
            return tweet
            
        except Exception as e:
            print(f"âŒ è§£ææ¨æ–‡å¤±è´¥: {e}")
            return None
    
    def extract_tweets_from_response(self, data: Dict) -> List[Dict]:
        """ä»å“åº”ä¸­æå–æ¨æ–‡æ•°æ® - åŸºäºåˆ†æç»“æœ"""
        tweets = []
        
        try:
            # åŸºäºåˆ†æç»“æœçš„æ•°æ®è·¯å¾„
            home_timeline = data.get('data', {}).get('home', {}).get('home_timeline_urt', {})
            instructions = home_timeline.get('instructions', [])
            
            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])
                    
                    for entry in entries:
                        entry_id = entry.get('entryId', '')
                        
                        # æ¨æ–‡æ¡ç›®
                        if 'tweet-' in entry_id:
                            content = entry.get('content', {})
                            item_content = content.get('itemContent', {})
                            tweet_results = item_content.get('tweet_results', {})
                            tweet_data = tweet_results.get('result', {})
                            
                            if tweet_data.get('__typename') == 'Tweet':
                                parsed_tweet = self.parse_tweet(tweet_data)
                                if parsed_tweet:
                                    tweets.append(parsed_tweet)
                        
                        # æ¸¸æ ‡å¤„ç† - ç”¨äºåˆ†é¡µ
                        elif 'cursor-' in entry_id:
                            cursor_content = entry.get('content', {})
                            if cursor_content.get('cursorType') == 'Bottom':
                                cursor_value = cursor_content.get('value')
                                # ä¿å­˜cursorç”¨äºä¸‹æ¬¡è¯·æ±‚
                                self.last_cursor = cursor_value
                                print(f"ğŸ”— æ‰¾åˆ°ä¸‹ä¸€é¡µcursor: {cursor_value[:50]}...")
        
        except Exception as e:
            print(f"âŒ æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
        
        return tweets
    
    def crawl_daily_posts(self, timeline_type: str = "recommended", max_pages: int = 5, target_count: Optional[int] = None) -> List[Dict]:
        """çˆ¬å–æ—¥æ¨æ–‡ - æ”¯æŒç²¾ç¡®æ•°é‡æ§åˆ¶"""
        if target_count is None:
            target_count = self.config.get("targets", {}).get("daily_tweet_count", 100)
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {timeline_type} æ—¶é—´çº¿...")
        print(f"ğŸ¯ ç›®æ ‡æ¨æ–‡æ•°: {target_count} æ¡")
        
        all_tweets = []
        cursor = None
        
        for page in range(max_pages):
            print(f"ğŸ“„ çˆ¬å–ç¬¬ {page + 1} é¡µ...")
            
            response_data = self.make_timeline_request(timeline_type, cursor)
            if not response_data:
                print("âŒ è¯·æ±‚å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                break
            
            tweets = self.extract_tweets_from_response(response_data)
            if not tweets:
                print("âš ï¸ æœªæ‰¾åˆ°æ¨æ–‡æ•°æ®ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è®¤è¯çŠ¶æ€")
                break
            
            all_tweets.extend(tweets)
            print(f"âœ… æœ¬é¡µè·å– {len(tweets)} æ¡æ¨æ–‡ (ç´¯è®¡: {len(all_tweets)} æ¡)")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(all_tweets) >= target_count:
                print(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {target_count} æ¡ï¼Œç»“æŸçˆ¬å–")
                all_tweets = all_tweets[:target_count]  # ç²¾ç¡®æˆªå–
                break
            
            # æ›´æ–°cursorç”¨äºä¸‹ä¸€é¡µ
            cursor = getattr(self, 'last_cursor', None)
            if not cursor:
                print("ğŸ“ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µcursorï¼Œç»“æŸçˆ¬å–")
                break
        
        # ä¿å­˜æ•°æ®
        if all_tweets:
            self.save_daily_data(all_tweets, timeline_type)
        
        print(f"ğŸ‰ æ€»å…±çˆ¬å– {len(all_tweets)} æ¡æ¨æ–‡")
        return all_tweets
    
    def save_daily_data(self, tweets: List[Dict], timeline_type: str):
        """ä¿å­˜æ—¥æ•°æ®"""
        today = datetime.now().strftime('%Y%m%d')
        filename = f"{today}_{timeline_type}_posts.json"
        filepath = self.data_dir / "daily_posts" / filename
        
        data = {
            "date": today,
            "timeline_type": timeline_type,
            "crawl_time": datetime.now().isoformat(),
            "tweet_count": len(tweets),
            "tweets": tweets
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {filepath}")
        
        # ç”Ÿæˆç®€è¦ç»Ÿè®¡
        self.generate_stats(tweets, timeline_type)
    
    def generate_stats(self, tweets: List[Dict], timeline_type: str):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "æ€»æ¨æ–‡æ•°": len(tweets),
            "åŸåˆ›æ¨æ–‡": len([t for t in tweets if not t.get('retweet')]),
            "è½¬æ¨": len([t for t in tweets if t.get('retweet')]),
            "åŒ…å«åª’ä½“": len([t for t in tweets if t.get('media')]),
            "åŒ…å«è§†é¢‘": len([t for t in tweets if any(m.get('type') == 'video' for m in t.get('media', []))]),
            "åŒ…å«å›¾ç‰‡": len([t for t in tweets if any(m.get('type') == 'photo' for m in t.get('media', []))]),
        }
        
        print(f"\nğŸ“Š {timeline_type} æ—¶é—´çº¿ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– X HTTPçˆ¬è™«å¯åŠ¨")
    print("åŸºäºæ·±åº¦APIåˆ†æç»“æœå®ç°")
    
    crawler = XCrawler()
    
    # æ£€æŸ¥è®¤è¯çŠ¶æ€
    print("\nâš ï¸ é‡è¦æé†’:")
    print("è¯·ç¡®ä¿å·²å®Œæˆä»¥ä¸‹æ­¥éª¤:")
    print("1. åœ¨æµè§ˆå™¨ä¸­ç™»å½•Xè´¦å·")
    print("2. æ‰‹åŠ¨æå–å¹¶é…ç½®å¿…è¦çš„è®¤è¯cookies")
    print("3. åŒ…æ‹¬: auth_token, ct0, bearer_tokenç­‰")
    print("\nå¦‚æœªå®Œæˆè®¤è¯é…ç½®ï¼Œè¯·å…ˆè¿è¡Œ tools/analyzer.py è·å–cookies")
    
    # å¼€å§‹çˆ¬å–
    input("\næŒ‰å›è½¦é”®å¼€å§‹çˆ¬å–...")
    
    # çˆ¬å–æ¨èæ—¶é—´çº¿
    tweets = crawler.crawl_daily_posts("recommended", max_pages=3)
    
    if tweets:
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼è·å¾— {len(tweets)} æ¡æ¨æ–‡")
        print("æ•°æ®ä¿å­˜åœ¨ crawler_data/daily_posts/ ç›®å½•")
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è®¤è¯é…ç½®")

if __name__ == "__main__":
    main()