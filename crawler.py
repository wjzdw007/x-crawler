#!/usr/bin/env python3
"""
X HTTPçˆ¬è™« - åŸºäºæ·±åº¦åˆ†æç»“æœå®ç°
è§£å†³äº†æ‰€æœ‰å·²çŸ¥æ ¸å¿ƒé—®é¢˜ï¼š
1. âœ… å¸–å­å†…å®¹å®Œæ•´æ€§ - ä½¿ç”¨ legacy.full_text
2. âœ… è½¬æ¨åŸå¸–è·å– - ä½¿ç”¨ retweeted_status_result
3. âœ… åª’ä½“æ–‡ä»¶URL - ä½¿ç”¨ extended_entities.media
4. âœ… æ•°æ®å‡†ç¡®æ€§ - æ­£ç¡®çš„GraphQLè·¯å¾„
"""

import json
import requests
import time
import os
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
import random
from config_loader import ConfigLoader

class XCrawler:
    def __init__(self, data_dir="crawler_data", config_file="config.json"):
        # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
        self.data_dir = Path(os.getenv('DATA_DIR', data_dir))
        self.data_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
        for subdir in ["daily_posts", "users_daily", "raw_responses", "user_summaries", "prompts"]:
            (self.data_dir / subdir).mkdir(exist_ok=True)

        # ä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨
        self.config_loader = ConfigLoader(config_file)
        self.config = self.config_loader.config
        self.session = requests.Session()
        self.setup_session()
        
        # APIç«¯ç‚¹ - åŸºäºåˆ†æç»“æœ
        self.api_endpoints = {
            "recommended": "xNGIIoXaz9DyeBXBfn3AjA/HomeLatestTimeline",
            "following": "1_nms9JVtHQxTw8VwZJciQ/HomeTimeline"
        }
        
        # åŸºç¡€URL
        self.base_url = "https://x.com/i/api/graphql"
        
        # è¯·æ±‚è®¡æ•°å’Œé™æµ
        self.request_count = 0
        self.last_request_time = 0
        self.rate_limit = self.config.get('settings', {}).get('requests_per_hour', 400)
    
        
    def setup_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        # åŸºç¡€headers - æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://x.com/home',
            'Origin': 'https://x.com',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
        })

        # ç¡®ä¿requestsä¼šè‡ªåŠ¨å¤„ç†gzipè§£å‹
        self.session.trust_env = False

        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        proxy_settings = self.config_loader.get_proxy_settings()
        if proxy_settings:
            self.session.proxies = proxy_settings
            print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_settings}")

        # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¤è¯ä¿¡æ¯
        self.load_authentication()
    
    def load_authentication(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½è®¤è¯ä¿¡æ¯"""
        auth_config = self.config.get('authentication', {})
        
        # åŠ è½½cookies
        cookies = auth_config.get('cookies', {})
        for key, value in cookies.items():
            if value and value != f"YOUR_{key.upper()}_HERE":
                self.session.cookies.set(key, value)
        
        # åŠ è½½headers
        headers = auth_config.get('headers', {})
        for key, value in headers.items():
            if value and not value.startswith("YOUR_"):
                self.session.headers[key] = value
        
        # æ£€æŸ¥è®¤è¯æ˜¯å¦é…ç½®
        has_auth = any(
            v and not str(v).startswith("YOUR_") 
            for v in {**cookies, **headers}.values()
        )
        
        if not has_auth:
            print("âš ï¸ æœªæ£€æµ‹åˆ°è®¤è¯é…ç½®")
            print("è¯·å¤åˆ¶ config_template.json ä¸º config.json å¹¶å¡«å…¥æ­£ç¡®çš„è®¤è¯ä¿¡æ¯")
    
    def rate_limit_check(self):
        """æ£€æŸ¥å’Œæ‰§è¡Œé™æµ"""
        current_time = time.time()
        
        # æ¯å°æ—¶é‡ç½®è®¡æ•°
        if current_time - self.last_request_time > 3600:
            self.request_count = 0
        
        # æ£€æŸ¥é™æµ
        if self.request_count >= self.rate_limit:
            wait_time = 3600 - (current_time - self.last_request_time)
            if wait_time > 0:
                print(f"â° è¾¾åˆ°é™æµä¸Šé™ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
                time.sleep(wait_time)
                self.request_count = 0
        
        # éšæœºå»¶è¿Ÿé˜²æ­¢æ£€æµ‹
        delay = random.uniform(1.0, 3.0)
        time.sleep(delay)
        
        self.request_count += 1
        self.last_request_time = current_time
    
    def get_timeline_params(self, timeline_type: str = "recommended", cursor: Optional[str] = None) -> Dict:
        """è·å–æ—¶é—´çº¿è¯·æ±‚å‚æ•°"""
        variables = {
            "count": 20,
            "includePromotedContent": True,
            "latestControlAvailable": True,
            "requestContext": "launch",
            "withCommunity": True
        }
        
        if cursor:
            variables["cursor"] = cursor
        
        features = {
            "rweb_video_screen_enabled": False,
            "payments_enabled": False,
            "profile_label_improvements_pcf_label_in_post_enabled": True,
            "rweb_tipjar_consumption_enabled": True,
            "verified_phone_label_enabled": False,
            "creator_subscriptions_tweet_preview_api_enabled": True,
            "responsive_web_graphql_timeline_navigation_enabled": True,
            "responsive_web_graphql_skip_user_profile_image_extensions_enabled": False,
            "premium_content_api_read_enabled": False,
            "communities_web_enable_tweet_community_results_fetch": True,
            "c9s_tweet_anatomy_moderator_badge_enabled": True,
            "responsive_web_grok_analyze_button_fetch_trends_enabled": False,
            "responsive_web_grok_analyze_post_followups_enabled": True,
            "responsive_web_jetfuel_frame": True,
            "responsive_web_grok_share_attachment_enabled": True,
            "articles_preview_enabled": True,
            "responsive_web_edit_tweet_api_enabled": True,
            "graphql_is_translatable_rweb_tweet_is_translatable_enabled": True,
            "view_counts_everywhere_api_enabled": True,
            "longform_notetweets_consumption_enabled": True,
            "responsive_web_twitter_article_tweet_consumption_enabled": True,
            "tweet_awards_web_tipping_enabled": False,
            "responsive_web_grok_show_grok_translated_post": False,
            "responsive_web_grok_analysis_button_from_backend": True,
            "creator_subscriptions_quote_tweet_preview_enabled": False,
            "freedom_of_speech_not_reach_fetch_enabled": True,
            "standardized_nudges_misinfo": True,
            "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": True,
            "longform_notetweets_rich_text_read_enabled": True,
            "longform_notetweets_inline_media_enabled": True,
            "responsive_web_grok_image_annotation_enabled": True,
            "responsive_web_grok_imagine_annotation_enabled": True,
            "responsive_web_grok_community_note_auto_translation_is_enabled": False,
            "responsive_web_enhance_cards_enabled": False
        }
        
        return {
            "variables": json.dumps(variables),
            "features": json.dumps(features)
        }
    
    def make_timeline_request(self, timeline_type: str = "recommended", cursor: Optional[str] = None) -> Optional[Dict]:
        """å‘èµ·æ—¶é—´çº¿è¯·æ±‚"""
        self.rate_limit_check()
        
        endpoint = self.api_endpoints[timeline_type]
        url = f"{self.base_url}/{endpoint}"
        params = self.get_timeline_params(timeline_type, cursor)
        
        try:
            print(f"ğŸ”„ è¯·æ±‚ {timeline_type} æ—¶é—´çº¿...")
            response = self.session.get(url, params=params, timeout=30)

            if response.status_code == 200:
                # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹å’Œç¼–ç 
                content_type = response.headers.get('Content-Type', '')
                print(f"ğŸ“„ å“åº”ç±»å‹: {content_type}")

                # å°è¯•è§£ç å“åº”å†…å®¹
                try:
                    response_data = response.json()
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                    print(f"ğŸ“ å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
                    print(f"ğŸ“ Content-Encoding: {response.headers.get('Content-Encoding', 'None')}")

                    # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºäºŒè¿›åˆ¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯gzipå‹ç¼©çš„ï¼‰
                    content_sample = response.content[:100]
                    is_binary = any(b < 32 or b > 126 for b in content_sample if b not in [9, 10, 13])

                    if is_binary:
                        print("ğŸ”§ æ£€æµ‹åˆ°äºŒè¿›åˆ¶å†…å®¹ï¼Œå°è¯•è§£å‹...")
                        content_encoding = response.headers.get('Content-Encoding', '').lower()

                        try:
                            if 'br' in content_encoding:
                                print("ğŸ”§ ä½¿ç”¨Brotliè§£å‹...")
                                import brotli
                                decompressed = brotli.decompress(response.content)
                            elif 'gzip' in content_encoding:
                                print("ğŸ”§ ä½¿ç”¨Gzipè§£å‹...")
                                import gzip
                                decompressed = gzip.decompress(response.content)
                            else:
                                print("ğŸ”§ å°è¯•è‡ªåŠ¨è§£å‹...")
                                # å°è¯•å¤šç§è§£å‹æ–¹å¼
                                try:
                                    import brotli
                                    decompressed = brotli.decompress(response.content)
                                    print("âœ… Brotliè§£å‹æˆåŠŸ")
                                except:
                                    import gzip
                                    decompressed = gzip.decompress(response.content)
                                    print("âœ… Gzipè§£å‹æˆåŠŸ")

                            response_data = json.loads(decompressed.decode('utf-8'))
                            print("âœ… æˆåŠŸè§£å‹å¹¶è§£æJSON")
                        except Exception as decomp_e:
                            print(f"âŒ è§£å‹å¤±è´¥: {decomp_e}")
                            return None
                    else:
                        print(f"ğŸ“ å“åº”å‰100å­—ç¬¦: {repr(response.text[:100])}")
                        return None

                # ä¿å­˜åŸå§‹APIå“åº”ç”¨äºåˆ†æ
                self.save_raw_response(response, url, params, timeline_type)

                return response_data
            elif response.status_code == 429:
                print("âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾…åé‡è¯•...")
                time.sleep(60)
                return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text[:200]}")
                return None
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def save_raw_response(self, response, url: str, params: dict, timeline_type: str):
        """ä¿å­˜åŸå§‹APIå“åº”ç”¨äºåˆ†æ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
            raw_dir = self.data_dir / "raw_responses"
            raw_dir.mkdir(exist_ok=True)

            filename = f"{timestamp}_{timeline_type}_response.json"
            filepath = raw_dir / filename

            raw_data = {
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "status": response.status_code,
                "headers": dict(response.headers),
                "params": params,
                "data": response.json() if response.status_code == 200 else {"error": response.text}
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(raw_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ åŸå§‹å“åº”å·²ä¿å­˜: {filename}")

            # ä¿å­˜åè‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶
            self.cleanup_old_raw_responses(days_to_keep=3)

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åŸå§‹å“åº”å¤±è´¥: {e}")

    def cleanup_old_raw_responses(self, days_to_keep: int = 3):
        """æ¸…ç†æ—§çš„raw_responsesæ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘Nå¤©çš„"""
        try:
            raw_dir = self.data_dir / "raw_responses"
            if not raw_dir.exists():
                return

            from datetime import timedelta
            cutoff_time = datetime.now() - timedelta(days=days_to_keep)

            deleted_count = 0
            total_size = 0

            for filepath in raw_dir.glob("*.json"):
                # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šYYYYMMDD_HHMMSS_mmmï¼‰
                try:
                    filename = filepath.stem
                    date_part = filename.split('_')[0]  # YYYYMMDD
                    file_date = datetime.strptime(date_part, '%Y%m%d')

                    if file_date < cutoff_time:
                        file_size = filepath.stat().st_size
                        filepath.unlink()
                        deleted_count += 1
                        total_size += file_size
                except (ValueError, IndexError):
                    # æ–‡ä»¶åæ ¼å¼ä¸ç¬¦ï¼Œè·³è¿‡
                    continue

            if deleted_count > 0:
                print(f"ğŸ—‘ï¸  æ¸…ç†æ—§å“åº”æ–‡ä»¶: {deleted_count} ä¸ª ({total_size / 1024 / 1024:.1f} MB)")

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†raw_responseså¤±è´¥: {e}")
    
    def parse_tweet(self, tweet_data: Dict) -> Optional[Dict]:
        """è§£ææ¨æ–‡æ•°æ® - åŸºäºåˆ†æç»“æœçš„å®Œæ•´å®ç°"""
        try:
            # åŸºç¡€æ¨æ–‡ä¿¡æ¯
            tweet = {
                'id': tweet_data.get('rest_id'),
                'text': '',
                'created_at': tweet_data.get('legacy', {}).get('created_at'),
                'lang': tweet_data.get('legacy', {}).get('lang'),
                'media': [],
                'retweet': None,
                'quoted': None,
                'user': None,
                'stats': {
                    'retweet_count': tweet_data.get('legacy', {}).get('retweet_count', 0),
                    'favorite_count': tweet_data.get('legacy', {}).get('favorite_count', 0),
                    'reply_count': tweet_data.get('legacy', {}).get('reply_count', 0),
                    'quote_count': tweet_data.get('legacy', {}).get('quote_count', 0)
                }
            }
            
            # æå–æ–‡æœ¬å†…å®¹ - å¤„ç†é•¿æ–‡æ¨æ–‡å’Œæ™®é€šæ¨æ–‡
            if 'note_tweet' in tweet_data:
                # é•¿æ–‡æ¨æ–‡
                note_tweet_result = tweet_data.get('note_tweet', {}).get('note_tweet_results', {}).get('result', {})
                if note_tweet_result:
                    tweet['text'] = note_tweet_result.get('text', '')
            
            if not tweet['text']:
                # æ™®é€šæ¨æ–‡ - ä½¿ç”¨ legacy.full_text
                tweet['text'] = tweet_data.get('legacy', {}).get('full_text', '')
            
            # è§£æç”¨æˆ·ä¿¡æ¯ - ä¿®æ­£å­—æ®µè·¯å¾„
            user_results = tweet_data.get('core', {}).get('user_results', {}).get('result', {})
            if user_results:
                tweet['user'] = {
                    'id': user_results.get('rest_id'),
                    'name': user_results.get('core', {}).get('name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    'screen_name': user_results.get('core', {}).get('screen_name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    'description': user_results.get('legacy', {}).get('description'),
                    'followers_count': user_results.get('legacy', {}).get('followers_count', 0),
                    'friends_count': user_results.get('legacy', {}).get('friends_count', 0),
                    'verified': user_results.get('verification', {}).get('verified', False),  # ä¿®æ­£è·¯å¾„
                    'is_blue_verified': user_results.get('is_blue_verified', False)
                }
            
            # è§£æåª’ä½“æ–‡ä»¶ - åŸºäºåˆ†æç»“æœ
            extended_entities = tweet_data.get('legacy', {}).get('extended_entities', {})
            if 'media' in extended_entities:
                for media_item in extended_entities['media']:
                    media_entry = {
                        'type': media_item.get('type'),
                        'id': media_item.get('id_str'),
                        'url': None
                    }
                    
                    if media_item['type'] == 'video':
                        # è§†é¢‘å¤„ç† - é€‰æ‹©æœ€é«˜è´¨é‡
                        variants = media_item.get('video_info', {}).get('variants', [])
                        best_variant = None
                        highest_bitrate = 0
                        
                        for variant in variants:
                            if variant.get('content_type') == 'video/mp4':
                                bitrate = variant.get('bitrate', 0)
                                if bitrate > highest_bitrate:
                                    highest_bitrate = bitrate
                                    best_variant = variant
                        
                        if best_variant:
                            media_entry['url'] = best_variant['url']
                            media_entry['bitrate'] = best_variant.get('bitrate')
                    
                    elif media_item['type'] in ['photo', 'animated_gif']:
                        # å›¾ç‰‡å¤„ç†
                        media_entry['url'] = media_item.get('media_url_https')
                    
                    if media_entry['url']:
                        tweet['media'].append(media_entry)
            
            # å¤„ç†è½¬æ¨ - åŸºäºåˆ†æç»“æœï¼Œæ”¯æŒTweetWithVisibilityResultsç»“æ„
            if 'retweeted_status_result' in tweet_data.get('legacy', {}):
                retweet_result = tweet_data['legacy']['retweeted_status_result'].get('result')
                if retweet_result:
                    # å¤„ç†TweetWithVisibilityResultsç»“æ„
                    if retweet_result.get('__typename') == 'TweetWithVisibilityResults':
                        # æ•°æ®åµŒå¥—åœ¨tweetå­—æ®µä¸­
                        actual_tweet = retweet_result.get('tweet')
                        if actual_tweet:
                            tweet['retweet'] = self.parse_tweet(actual_tweet)
                    else:
                        # æ™®é€šTweetç»“æ„
                        tweet['retweet'] = self.parse_tweet(retweet_result)
            
            # å¤„ç†å¼•ç”¨æ¨æ–‡
            if 'quoted_status_result' in tweet_data:
                quoted_result = tweet_data['quoted_status_result'].get('result')
                if quoted_result:
                    tweet['quoted'] = self.parse_tweet(quoted_result)
            
            return tweet
            
        except Exception as e:
            print(f"âŒ è§£ææ¨æ–‡å¤±è´¥: {e}")
            return None
    
    def extract_tweets_from_response(self, data: Dict) -> List[Dict]:
        """ä»å“åº”ä¸­æå–æ¨æ–‡æ•°æ® - åŸºäºåˆ†æç»“æœ"""
        tweets = []
        
        try:
            # åŸºäºåˆ†æç»“æœçš„æ•°æ®è·¯å¾„
            home_timeline = data.get('data', {}).get('home', {}).get('home_timeline_urt', {})
            instructions = home_timeline.get('instructions', [])
            
            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])
                    
                    for entry in entries:
                        entry_id = entry.get('entryId', '')
                        
                        # æ¨æ–‡æ¡ç›® - æ’é™¤promoted-tweetå¹¿å‘Š
                        if entry_id.startswith('tweet-'):
                            content = entry.get('content', {})
                            item_content = content.get('itemContent', {})
                            tweet_results = item_content.get('tweet_results', {})
                            tweet_data = tweet_results.get('result', {})
                            
                            if tweet_data.get('__typename') == 'Tweet':
                                parsed_tweet = self.parse_tweet(tweet_data)
                                if parsed_tweet:
                                    tweets.append(parsed_tweet)
                        
                        # å¯¹è¯æ¨¡å— - åŒ…å«å¤šæ¡ç›¸å…³æ¨æ–‡
                        elif entry_id.startswith('home-conversation-'):
                            content = entry.get('content', {})
                            if content.get('entryType') == 'TimelineTimelineModule':
                                items = content.get('items', [])
                                for item in items:
                                    item_content = item.get('item', {}).get('itemContent', {})
                                    if item_content.get('itemType') == 'TimelineTweet':
                                        tweet_results = item_content.get('tweet_results', {})
                                        tweet_data = tweet_results.get('result', {})
                                        
                                        if tweet_data.get('__typename') == 'Tweet':
                                            parsed_tweet = self.parse_tweet(tweet_data)
                                            if parsed_tweet:
                                                tweets.append(parsed_tweet)
                        
                        # æ¸¸æ ‡å¤„ç† - ç”¨äºåˆ†é¡µ
                        elif 'cursor-' in entry_id:
                            cursor_content = entry.get('content', {})
                            if cursor_content.get('cursorType') == 'Bottom':
                                cursor_value = cursor_content.get('value')
                                # ä¿å­˜cursorç”¨äºä¸‹æ¬¡è¯·æ±‚
                                self.last_cursor = cursor_value
                                print(f"ğŸ”— æ‰¾åˆ°ä¸‹ä¸€é¡µcursor: {cursor_value[:50]}...")
        
        except Exception as e:
            print(f"âŒ æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
        
        return tweets
    
    def crawl_daily_posts(self, timeline_type: str = "recommended", max_pages: int = None, target_count: Optional[int] = None) -> List[Dict]:
        """çˆ¬å–æ—¥æ¨æ–‡ - æ”¯æŒç²¾ç¡®æ•°é‡æ§åˆ¶ï¼Œå®æ—¶å»é‡"""
        if target_count is None:
            target_count = self.config.get("targets", {}).get("daily_tweet_count", 100)

        print(f"ğŸš€ å¼€å§‹çˆ¬å– {timeline_type} æ—¶é—´çº¿...")
        print(f"ğŸ¯ ç›®æ ‡æ¨æ–‡æ•°: {target_count} æ¡")

        # ä½¿ç”¨å­—å…¸å­˜å‚¨æ¨æ–‡ï¼Œè‡ªåŠ¨å»é‡
        unique_tweets = {}
        cursor = None
        page = 0

        # å¦‚æœæŒ‡å®šäº†max_pageså°±ä½¿ç”¨ï¼Œå¦åˆ™æ— é™åˆ¶ç›´åˆ°è¾¾åˆ°target_countæˆ–æ— æ›´å¤šæ•°æ®
        while max_pages is None or page < max_pages:
            page += 1
            print(f"ğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ...")

            response_data = self.make_timeline_request(timeline_type, cursor)
            if not response_data:
                print("âŒ è¯·æ±‚å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                break

            tweets = self.extract_tweets_from_response(response_data)
            if not tweets:
                print("âš ï¸ æœªæ‰¾åˆ°æ¨æ–‡æ•°æ®ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è®¤è¯çŠ¶æ€")
                break

            # å®æ—¶å»é‡ï¼šåªæ·»åŠ æ–°æ¨æ–‡
            new_count = 0
            for tweet in tweets:
                tweet_id = tweet.get('id')
                if tweet_id and tweet_id not in unique_tweets:
                    unique_tweets[tweet_id] = tweet
                    new_count += 1

            print(f"âœ… æœ¬é¡µè·å– {len(tweets)} æ¡æ¨æ–‡ (æ–°å¢: {new_count} æ¡, é‡å¤: {len(tweets) - new_count} æ¡, ç´¯è®¡: {len(unique_tweets)} æ¡)")

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(unique_tweets) >= target_count:
                print(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {target_count} æ¡ï¼Œç»“æŸçˆ¬å–")
                break

            # æ›´æ–°cursorç”¨äºä¸‹ä¸€é¡µ
            cursor = getattr(self, 'last_cursor', None)
            if not cursor:
                print("ğŸ“ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µcursorï¼Œç»“æŸçˆ¬å–")
                break

        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´å€’åºæ’åºï¼Œç„¶åç²¾ç¡®æˆªå–
        from dateutil.parser import parse as parse_date
        all_tweets = sorted(
            unique_tweets.values(),
            key=lambda t: parse_date(t.get('created_at', '1970-01-01')),
            reverse=True
        )[:target_count]

        # ä¿å­˜æ•°æ®
        if all_tweets:
            self.save_daily_data(all_tweets, timeline_type)
            # æŒ‰ç”¨æˆ·åˆ†ç»„ä¿å­˜å½“å¤©æ•°æ®
            self.save_by_user_daily(all_tweets)

        print(f"ğŸ‰ æ€»å…±çˆ¬å– {len(all_tweets)} æ¡å”¯ä¸€æ¨æ–‡")
        return all_tweets
    
    def save_daily_data(self, tweets: List[Dict], timeline_type: str):
        """ä¿å­˜æ—¥æ•°æ® - æ”¯æŒå¤šæ¬¡æŠ“å–åˆå¹¶å»é‡"""
        today = datetime.now().strftime('%Y%m%d')
        filename = f"{today}_{timeline_type}_posts.json"
        filepath = self.data_dir / "daily_posts" / filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ•°æ®
        existing_tweets = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_tweets = existing_data.get('tweets', [])
                print(f"ğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®: {len(existing_tweets)} æ¡æ¨æ–‡")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")

        # åˆå¹¶æ¨æ–‡å¹¶å»é‡ï¼ˆåŸºäºæ¨æ–‡IDï¼‰
        all_tweets = existing_tweets + tweets
        unique_tweets = {}

        for tweet in all_tweets:
            tweet_id = tweet.get('id')
            if tweet_id and tweet_id not in unique_tweets:
                unique_tweets[tweet_id] = tweet

        # æŒ‰æ—¶é—´å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        from dateutil.parser import parse as parse_date
        sorted_tweets = sorted(
            unique_tweets.values(),
            key=lambda t: parse_date(t.get('created_at', '1970-01-01')),
            reverse=True
        )

        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        data = {
            "date": today,
            "timeline_type": timeline_type,
            "last_crawl_time": datetime.now().isoformat(),
            "tweet_count": len(sorted_tweets),
            "unique_tweet_count": len(sorted_tweets),
            "total_crawled": len(all_tweets),
            "duplicates_removed": len(all_tweets) - len(sorted_tweets),
            "tweets": sorted_tweets
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        action = "æ›´æ–°" if existing_tweets else "åˆ›å»º"
        print(f"ğŸ’¾ æ•°æ®å·²{action}: {filepath}")
        print(f"   æœ¬æ¬¡æŠ“å–: {len(tweets)} æ¡, ç´¯è®¡: {len(sorted_tweets)} æ¡ (å»é‡: {len(all_tweets) - len(sorted_tweets)} æ¡)")

        # ç”Ÿæˆç®€è¦ç»Ÿè®¡
        self.generate_stats(sorted_tweets, timeline_type)
    
    def generate_stats(self, tweets: List[Dict], timeline_type: str):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "æ€»æ¨æ–‡æ•°": len(tweets),
            "åŸåˆ›æ¨æ–‡": len([t for t in tweets if not t.get('retweet')]),
            "è½¬æ¨": len([t for t in tweets if t.get('retweet')]),
            "åŒ…å«åª’ä½“": len([t for t in tweets if t.get('media')]),
            "åŒ…å«è§†é¢‘": len([t for t in tweets if any(m.get('type') == 'video' for m in t.get('media', []))]),
            "åŒ…å«å›¾ç‰‡": len([t for t in tweets if any(m.get('type') == 'photo' for m in t.get('media', []))]),
        }
        
        print(f"\nğŸ“Š {timeline_type} æ—¶é—´çº¿ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    def save_by_user_daily(self, tweets: List[Dict]):
        """æŒ‰ç”¨æˆ·å’Œæ—¥æœŸåˆ†ç»„ä¿å­˜æ‰€æœ‰æ¨æ–‡æ•°æ®"""
        from dateutil.parser import parse as parse_date
        import os
        
        users_dir = self.data_dir / "users_daily"
        
        print(f"\nğŸ‘¥ æŒ‰ç”¨æˆ·å’Œæ—¥æœŸåˆ†ç»„ä¿å­˜æ¨æ–‡...")
        
        # æŒ‰ç”¨æˆ·å’Œæ—¥æœŸåŒé‡åˆ†ç»„ {user: {date: [tweets]}}
        user_date_tweets = {}
        total_processed = 0
        
        for tweet in tweets:
            try:
                tweet_date = parse_date(tweet.get('created_at', ''))
                tweet_date_str = tweet_date.strftime('%Y%m%d')
                
                # è·å–ç”¨æˆ·ä¿¡æ¯
                user = tweet.get('user', {})
                screen_name = user.get('screen_name', 'unknown')
                
                # åˆå§‹åŒ–åµŒå¥—å­—å…¸ç»“æ„
                if screen_name not in user_date_tweets:
                    user_date_tweets[screen_name] = {}
                if tweet_date_str not in user_date_tweets[screen_name]:
                    user_date_tweets[screen_name][tweet_date_str] = []
                
                user_date_tweets[screen_name][tweet_date_str].append(tweet)
                total_processed += 1
                
            except Exception as e:
                print(f"âš ï¸ è§£ææ¨æ–‡æ—¶é—´å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“… å¤„ç†æ¨æ–‡: {total_processed}/{len(tweets)} æ¡")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_users = len(user_date_tweets)
        total_files = sum(len(date_tweets) for date_tweets in user_date_tweets.values())
        print(f"ğŸ‘¤ æ¶‰åŠç”¨æˆ·æ•°: {total_users} ä¸ª")
        print(f"ğŸ“‚ å°†ç”Ÿæˆæ–‡ä»¶æ•°: {total_files} ä¸ª")
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªæ—¥æœŸä¿å­˜æ•°æ®
        for screen_name, date_tweets in user_date_tweets.items():
            for date_str, user_tweet_list in date_tweets.items():
                self._save_user_tweets_by_date(screen_name, date_str, user_tweet_list, users_dir)
        
        print(f"âœ… ç”¨æˆ·åˆ†ç»„ä¿å­˜å®Œæˆ")
    
    def _save_user_tweets(self, screen_name: str, new_tweets: List[Dict], users_dir: Path):
        """ä¿å­˜æˆ–åˆå¹¶ç”¨æˆ·çš„æ¨æ–‡æ•°æ®"""
        import os
        from dateutil.parser import parse as parse_date
        
        today = datetime.now().strftime('%Y%m%d')
        filename = f"{screen_name}_{today}.json"
        filepath = users_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ•°æ®
        existing_tweets = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_tweets = existing_data.get('tweets', [])
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        
        # åˆå¹¶æ¨æ–‡å¹¶å»é‡ï¼ˆåŸºäºæ¨æ–‡IDï¼‰
        all_tweets = existing_tweets + new_tweets
        unique_tweets = {}
        
        # å…ˆä¿å­˜ç”¨æˆ·ä¿¡æ¯ï¼Œå†ç§»é™¤å†—ä½™å­—æ®µ
        user_info = {}
        for tweet in all_tweets:
            tweet_id = tweet.get('id')
            if tweet_id and tweet_id not in unique_tweets:
                # ä¿å­˜æœ€æ–°çš„ç”¨æˆ·ä¿¡æ¯
                if tweet.get('user'):
                    user_info = tweet['user']
                
                # ç§»é™¤å†—ä½™çš„ç”¨æˆ·ä¿¡æ¯ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»æŒ‰ç”¨æˆ·åˆ†ç»„
                clean_tweet = tweet.copy()
                clean_tweet.pop('user', None)  # ç§»é™¤é¡¶å±‚userå­—æ®µ
                unique_tweets[tweet_id] = clean_tweet
        
        # æŒ‰æ—¶é—´æ­£åºæ’åº
        sorted_tweets = sorted(
            unique_tweets.values(),
            key=lambda t: parse_date(t.get('created_at', '1970-01-01'))
        )
        
        # æ„å»ºä¿å­˜æ•°æ®
        save_data = {
            "user": {
                "screen_name": screen_name,
                "name": user_info.get('name', ''),
                "description": user_info.get('description', ''),
                "followers_count": user_info.get('followers_count', 0),
                "verified": user_info.get('verified', False),
                "is_blue_verified": user_info.get('is_blue_verified', False)
            },
            "date": today,
            "last_updated": datetime.now().isoformat(),
            "tweet_count": len(sorted_tweets),
            "tweets": sorted_tweets
        }
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        action = "æ›´æ–°" if existing_tweets else "åˆ›å»º"
        print(f"  ğŸ“„ {action} @{screen_name}: {len(sorted_tweets)} æ¡æ¨æ–‡ -> {filename}")
    
    def _save_user_tweets_by_date(self, screen_name: str, date_str: str, new_tweets: List[Dict], users_dir: Path):
        """æŒ‰æ—¥æœŸä¿å­˜æˆ–åˆå¹¶ç”¨æˆ·çš„æ¨æ–‡æ•°æ®"""
        import os
        from dateutil.parser import parse as parse_date
        
        filename = f"{screen_name}_{date_str}.json"
        filepath = users_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ•°æ®
        existing_tweets = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_tweets = existing_data.get('tweets', [])
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        
        # åˆå¹¶æ¨æ–‡å¹¶å»é‡ï¼ˆåŸºäºæ¨æ–‡IDï¼‰
        all_tweets = existing_tweets + new_tweets
        unique_tweets = {}
        
        # å…ˆä¿å­˜ç”¨æˆ·ä¿¡æ¯ï¼Œå†ç§»é™¤å†—ä½™å­—æ®µ
        user_info = {}
        for tweet in all_tweets:
            tweet_id = tweet.get('id')
            if tweet_id and tweet_id not in unique_tweets:
                # ä¿å­˜æœ€æ–°çš„ç”¨æˆ·ä¿¡æ¯
                if tweet.get('user'):
                    user_info = tweet['user']
                
                # ç§»é™¤å†—ä½™çš„ç”¨æˆ·ä¿¡æ¯ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»æŒ‰ç”¨æˆ·åˆ†ç»„
                clean_tweet = tweet.copy()
                clean_tweet.pop('user', None)  # ç§»é™¤é¡¶å±‚userå­—æ®µ
                unique_tweets[tweet_id] = clean_tweet
        
        # æŒ‰æ—¶é—´æ­£åºæ’åº
        sorted_tweets = sorted(
            unique_tweets.values(),
            key=lambda t: parse_date(t.get('created_at', '1970-01-01'))
        )
        
        # æ„å»ºä¿å­˜æ•°æ®
        save_data = {
            "user": {
                "screen_name": screen_name,
                "name": user_info.get('name', ''),
                "description": user_info.get('description', ''),
                "followers_count": user_info.get('followers_count', 0),
                "verified": user_info.get('verified', False),
                "is_blue_verified": user_info.get('is_blue_verified', False)
            },
            "date": date_str,
            "last_updated": datetime.now().isoformat(),
            "tweet_count": len(sorted_tweets),
            "tweets": sorted_tweets
        }
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        action = "æ›´æ–°" if existing_tweets else "åˆ›å»º"
        print(f"  ğŸ“„ {action} @{screen_name}[{date_str}]: {len(sorted_tweets)} æ¡æ¨æ–‡ -> {filename}")
    
    def generate_user_summaries_for_yesterday(self, force_overwrite: bool = False):
        """ç”Ÿæˆæ˜¨å¤©æ‰€æœ‰ç”¨æˆ·çš„ä¸ªäººæ¨æ–‡æ€»ç»“"""
        from datetime import datetime, timedelta
        from summarizer import TwitterSummarizer
        
        # è®¡ç®—æ˜¨å¤©æ—¥æœŸ
        yesterday = datetime.now() - timedelta(days=1)
        yesterday_str = yesterday.strftime('%Y%m%d')
        
        print(f"\nğŸ¤– å¼€å§‹ç”Ÿæˆæ˜¨å¤©({yesterday_str})çš„ç”¨æˆ·æ€»ç»“...")
        
        users_dir = self.data_dir / "users_daily"
        summaries_dir = self.data_dir / "user_summaries"
        summaries_dir.mkdir(exist_ok=True)
        
        # æŸ¥æ‰¾æ˜¨å¤©çš„æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶
        yesterday_files = list(users_dir.glob(f"*_{yesterday_str}.json"))
        
        if not yesterday_files:
            print(f"ğŸ“­ æœªæ‰¾åˆ°æ˜¨å¤©({yesterday_str})çš„ç”¨æˆ·æ•°æ®æ–‡ä»¶")
            return
        
        print(f"ğŸ“‚ å‘ç° {len(yesterday_files)} ä¸ªç”¨æˆ·æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ€»ç»“å™¨
        summarizer = TwitterSummarizer()
        summarized_count = 0
        skipped_count = 0
        
        for user_file in yesterday_files:
            try:
                # è§£ææ–‡ä»¶åè·å–ç”¨æˆ·å
                username = user_file.stem.replace(f"_{yesterday_str}", "")
                summary_filename = f"{username}_{yesterday_str}_summary.json"
                summary_filepath = summaries_dir / summary_filename
                
                # æ£€æŸ¥æ€»ç»“æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if summary_filepath.exists() and not force_overwrite:
                    print(f"  â­ï¸  è·³è¿‡ @{username}: æ€»ç»“å·²å­˜åœ¨")
                    skipped_count += 1
                    continue
                # è¯»å–ç”¨æˆ·æ¨æ–‡æ•°æ®
                with open(user_file, 'r', encoding='utf-8') as f:
                    user_data = json.load(f)
                
                tweets = user_data.get('tweets', [])
                user_info = user_data.get('user', {})
                
                if summary_filepath.exists() and force_overwrite:
                    print(f"  ğŸ”„ å¼ºåˆ¶è¦†ç›– @{username} æ€»ç»“ ({len(tweets)}æ¡æ¨æ–‡)...")
                else:
                    print(f"  ğŸ”„ ç”Ÿæˆ @{username} æ€»ç»“ ({len(tweets)}æ¡æ¨æ–‡)...")
                
                if not tweets:
                    print(f"  âš ï¸  è·³è¿‡ @{username}: æ— æ¨æ–‡æ•°æ®")
                    continue
                
                # ç”Ÿæˆä¸ªäººæ€»ç»“
                summary_result = summarizer.generate_summary(tweets, f"user_daily", user_info)
                
                # ç›´æ¥ä¿å­˜å¤§æ¨¡å‹çš„å›å¤ä¸ºmarkdownæ–‡ä»¶
                md_filename = f"{username}_{yesterday_str}_summary.md"
                md_filepath = summaries_dir / md_filename

                # åˆ¤æ–­æ˜¯è¦†ç›–è¿˜æ˜¯åˆ›å»º
                was_existing = md_filepath.exists()

                # ç›´æ¥ä¿å­˜å¤§æ¨¡å‹çš„æ€»ç»“å†…å®¹
                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(summary_result.get('summary', 'æš‚æ— æ€»ç»“å†…å®¹'))
                
                action = "è¦†ç›–" if (was_existing and force_overwrite) else "åˆ›å»º"
                print(f"  âœ… {action}å®Œæˆ @{username}: {md_filename}")
                summarized_count += 1
                
            except Exception as e:
                print(f"  âŒ å¤„ç† {user_file.name} å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š ç”¨æˆ·æ€»ç»“å®Œæˆ:")
        print(f"  âœ… æ–°ç”Ÿæˆ: {summarized_count} ä¸ª")
        print(f"  â­ï¸  å·²è·³è¿‡: {skipped_count} ä¸ª")
        print(f"  ğŸ“ æ€»ç»“ç›®å½•: {summaries_dir}")
    
    def generate_user_summaries_for_date(self, date_str: str, force_overwrite: bool = False):
        """ä¸ºæŒ‡å®šæ—¥æœŸçš„ç”¨æˆ·æ•°æ®ç”Ÿæˆæ€»ç»“"""
        from summarizer import TwitterSummarizer
        
        print(f"\nğŸ¤– å¼€å§‹ç”ŸæˆæŒ‡å®šæ—¥æœŸ({date_str})çš„ç”¨æˆ·æ€»ç»“...")
        
        users_dir = self.data_dir / "users_daily"
        summaries_dir = self.data_dir / "user_summaries"
        summaries_dir.mkdir(exist_ok=True)
        
        # æŸ¥æ‰¾æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶
        date_files = list(users_dir.glob(f"*_{date_str}.json"))
        
        if not date_files:
            print(f"ğŸ“­ æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸ({date_str})çš„ç”¨æˆ·æ•°æ®æ–‡ä»¶")
            print(f"ğŸ” æ£€æŸ¥ç›®å½•: {users_dir}")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(date_files)} ä¸ªç”¨æˆ·æ•°æ®æ–‡ä»¶")
        
        # åˆ›å»ºæ€»ç»“å™¨
        summarizer = TwitterSummarizer()
        
        processed_count = 0
        skipped_count = 0
        
        for user_file in date_files:
            try:
                # æå–ç”¨æˆ·å
                user_name = user_file.stem.split('_')[0]  # filename: username_YYYYMMDD.json
                
                # ç”Ÿæˆæ€»ç»“æ–‡ä»¶è·¯å¾„ï¼ˆæ”¹ä¸ºmarkdownæ ¼å¼ï¼‰
                summary_filename = f"{user_name}_{date_str}_summary.md"
                summary_path = summaries_dir / summary_filename
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶è¦†ç›–
                if summary_path.exists() and not force_overwrite:
                    print(f"  â­ï¸  è·³è¿‡ @{user_name} - æ€»ç»“å·²å­˜åœ¨")
                    skipped_count += 1
                    continue
                
                # åŠ è½½ç”¨æˆ·æ•°æ®
                with open(user_file, 'r', encoding='utf-8') as f:
                    user_data = json.load(f)
                
                tweets = user_data.get('tweets', [])
                user_info = user_data.get('user', {})
                
                if not tweets:
                    print(f"  âš ï¸   @{user_name} - æ— æ¨æ–‡æ•°æ®")
                    continue
                
                # ç”Ÿæˆæ€»ç»“
                print(f"  ğŸ”„ å¤„ç† @{user_name} ({len(tweets)} æ¡æ¨æ–‡)")
                summary_result = summarizer.generate_summary(tweets, 'user_daily', user_info)
                
                # ç›´æ¥ä¿å­˜å¤§æ¨¡å‹çš„æ€»ç»“å†…å®¹
                with open(summary_path, 'w', encoding='utf-8') as f:
                    f.write(summary_result.get('summary', 'æš‚æ— æ€»ç»“å†…å®¹'))
                
                print(f"  âœ… @{user_name} æ€»ç»“å®Œæˆ")
                processed_count += 1
                
            except Exception as e:
                print(f"  âŒ @{user_name} å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ“Š ç”¨æˆ·æ€»ç»“ç”Ÿæˆå®Œæˆ:")
        print(f"  âœ… å·²å¤„ç†: {processed_count} ä¸ª")
        print(f"  â­ï¸  å·²è·³è¿‡: {skipped_count} ä¸ª")
        print(f"  ğŸ“ æ€»ç»“ç›®å½•: {summaries_dir}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– X HTTPçˆ¬è™«å¯åŠ¨")
    print("åŸºäºæ·±åº¦APIåˆ†æç»“æœå®ç°")
    
    crawler = XCrawler()
    
    # æ£€æŸ¥è®¤è¯çŠ¶æ€
    print("\nâš ï¸ é‡è¦æé†’:")
    print("è¯·ç¡®ä¿å·²å®Œæˆä»¥ä¸‹æ­¥éª¤:")
    print("1. åœ¨æµè§ˆå™¨ä¸­ç™»å½•Xè´¦å·")
    print("2. æ‰‹åŠ¨æå–å¹¶é…ç½®å¿…è¦çš„è®¤è¯cookies")
    print("3. åŒ…æ‹¬: auth_token, ct0, bearer_tokenç­‰")
    print("\nå¦‚æœªå®Œæˆè®¤è¯é…ç½®ï¼Œè¯·å…ˆè¿è¡Œ tools/analyzer.py è·å–cookies")
    
    # å¼€å§‹çˆ¬å–
    input("\næŒ‰å›è½¦é”®å¼€å§‹çˆ¬å–...")
    
    # çˆ¬å–æ¨èæ—¶é—´çº¿
    tweets = crawler.crawl_daily_posts("recommended", max_pages=3)
    
    if tweets:
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼è·å¾— {len(tweets)} æ¡æ¨æ–‡")
        print("æ•°æ®ä¿å­˜åœ¨ crawler_data/daily_posts/ ç›®å½•")
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è®¤è¯é…ç½®")

if __name__ == "__main__":
    main()