{
  "user": {
    "screen_name": "dotey",
    "name": "",
    "description": "",
    "followers_count": 0,
    "verified": false,
    "is_blue_verified": false
  },
  "date": "20251004",
  "last_updated": "2025-10-15T01:37:39.405449",
  "tweet_count": 18,
  "tweets": [
    {
      "id": "1974272972803219521",
      "text": "RT @oops073111: 我有个想法\n十八线小演员想要接戏的\n应该注册 sora2\n让大家那他做客串",
      "created_at": "Sat Oct 04 00:38:51 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": {
        "id": "1974249715374985505",
        "text": "我有个想法\n十八线小演员想要接戏的\n应该注册 sora2\n让大家那他做客串",
        "created_at": "Fri Oct 03 23:06:26 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1918985445188505600",
          "name": "Menschheit🌾@claudebuddy.fun",
          "screen_name": "oops073111",
          "description": "家人陪伴第一、乐观积极、持续学习、保持好奇、年轻心态 🧣 https://t.co/rtDVH1lndg,主理人，国内第一家主打共同成长的AI编程服务平台。（目前用户量：200+，付费用户100+） 463✉️ oops0731111 可以链接到我（Ai编程用户免费赠送codex、cc的使用体验）",
          "followers_count": 468,
          "friends_count": 363,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 1,
          "favorite_count": 11,
          "reply_count": 6,
          "quote_count": 1
        }
      },
      "quoted": null,
      "stats": {
        "retweet_count": 1,
        "favorite_count": 0,
        "reply_count": 0,
        "quote_count": 0
      }
    },
    {
      "id": "1974284895250767960",
      "text": "Sora 产品动态 1\nby Sam Altman\n\n随着Sora上线，我们开始快速从用户、版权方和其他利益相关方的反馈中学习和改进。尽管上线前我们进行了大量讨论，但有了真正的产品后，我们才能跳出理论，踏实地解决实际问题。\n\n我们很快会做出两项改变（未来当然还有更多）：\n\n首先，我们将给版权方提供对人物角色创作更精细的控制权，这一点类似于此前对肖像权的“用户主动选择”（opt-in）模式，但控制更加细致。\n\n我们听到不少版权方对这种全新的“互动式粉丝创作”（interactive fan fiction）感到非常兴奋，认为这是一种非常有价值的新型互动方式。但同时，他们也希望明确控制自己的角色如何被使用，甚至选择完全不允许任何使用。我们相信版权方会各自尝试不同策略，而我们将统一标准，把决定权交到版权方手中（当然，我们希望这个产品足够优秀，吸引越来越多的版权方主动参与进来）。\n\n在实现的过程中，可能会出现一些边缘情况，有少数违规的内容通过了审核，技术上的完善需要持续迭代。尤其值得一提的是，我们注意到来自日本的惊人创意表现力，用户与日本内容之间的深厚联结令我们非常惊叹！\n\n其次，我们必须开始考虑视频生成带来的成本问题。我们发现每个用户的创作量远超预期，而许多视频的受众其实非常有限。因此，我们计划尝试一种收入分享模式：用户创作的视频如果涉及版权方的角色，我们会与版权方分享相应的收入。具体的模式还需要不断试验和调整，但我们会尽快启动这项计划。我们希望，这种全新的互动带来的价值甚至超过收入分成本身，当然，能同时实现价值和收益最好不过了。\n\n在未来一段时间，我们将高速迭代、快速调整，这种感觉让我想起了ChatGPT最初上线时的情况。我们会做出一些明智的决定，也一定会犯下一些错误，但我们会及时听取反馈，迅速修正问题。我们计划首先在Sora内部快速迭代，然后再将成功经验逐渐推广到我们的其他产品中去。",
      "created_at": "Sat Oct 04 01:26:14 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974272833875329113",
        "text": "Sora update #1:\n\nhttps://t.co/DC9ZpR7cSC",
        "created_at": "Sat Oct 04 00:38:18 +0000 2025",
        "lang": "en",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1605",
          "name": "Sam Altman",
          "screen_name": "sama",
          "description": "AI is cool i guess",
          "followers_count": 4027263,
          "friends_count": 971,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 809,
          "favorite_count": 4351,
          "reply_count": 1148,
          "quote_count": 421
        }
      },
      "stats": {
        "retweet_count": 7,
        "favorite_count": 29,
        "reply_count": 6,
        "quote_count": 5
      }
    },
    {
      "id": "1974287055996731573",
      "text": "软件正在吞噬劳动力：一场静默却深刻的产业革命\n\n从“软件吞噬世界”到“软件吞噬劳动力”：一场价值万亿的范式转移\n\n十多年前，当马克·安德森（Marc Andreessen）预言“软件正在吞噬世界”时，我们见证了软件从边缘工具跃升为商业核心的壮丽景象。如今，这场由数字驱动的革命正在迎来它的下一个宏大篇章：软件，尤其是人工智能（AI）赋能的软件，正以我们前所未有的速度和深度，吞噬着劳动力市场。这不是简单的自动化升级，而是一场旨在重塑万亿美元劳动力市场的深刻范式转移。如果说过去的软件变革只是将实体文件柜数字化为数据库，那么如今，它正直接扮演起曾经由人类承担的角色，执行端到端的任务，成为“劳动力”本身。\n\n历史的回响：效率工具的演进与劳动的解放\n\n回顾人类历史，我们会发现，每一次技术革新都伴随着对“劳动”的重新定义。从织布机取代手工纺织，到蒸汽机船提升航运效率，再到古腾堡印刷机和流水线作业，资本投入机器，总能带来更高效的劳动产出。然而，这些早期的自动化，无论多么先进，都仍需要人类操作者的介入——比如，操作织布机，向蒸汽船炉膛里铲煤，或者在流水线上组装零部件。人类始终是流程中的关键节点，是机器与目标之间不可或缺的桥梁。\n\n软件的初期发展也遵循着类似轨迹。早期软件公司，如Saber Systems（航空预订）、SAP（企业资源规划）、Salesforce（客户关系管理），以及图书馆的卡片目录数字化、法律案例文件电子化、会计账簿数字化，乃至医疗健康记录和人力资源管理系统，它们的核心贡献在于将过去冗杂的纸质“文件柜”转化为可搜索、可共享的“数据库”。这种转变极大地提高了信息管理的效率，但本质上，阅读、处理这些数字化信息的依然是人类。一个在1959年通过文件柜查询航班的客服人员，与2010年通过Salesforce系统处理客户信息的销售员，在核心工作性质上并无根本区别——他们都是在“阅读”和“操作”信息。软件只是改变了介质，但尚未真正取代人类的“操作”行为。\n\nAI时代的新销售主张：从“提供工具”到“完成任务”\n\n然而，AI的崛起正在彻底改变这一局面。当前软件公司的销售主张不再是“我们提供一套更强大的工具”，而是“我们将为你直接完成这项工作”。这是一个颠覆性的转变。以客户服务为例，传统的客户服务软件（如Zendesk）按“席位”收费。如果一个公司有一千名客服代表，软件就收取一千个席位的费用。但如果AI能够以惊人的效率（例如，比人类效率高9000倍）回答客户查询，那么企业还需要多少个“席位”呢？答案可能是：零。\n\n这给传统的软件商业模式带来了巨大的挑战，也蕴藏着巨大的机遇。一个拥有1000名客服人员、每年支付7500万美元人力成本的公司，可能只支付140万美元的软件费用。如果AI能接管大部分工作，Zendesk的营收可能骤降至零。但反过来，如果Zendesk能提供一个基于结果的定价模式——例如，每年收取500万美元，却帮助客户节省了7500万美元的人力成本——那么其营收可能增长三倍。这促使软件公司必须从销售“工具”转向销售“成果”，从关注“使用量”到关注“价值创造”。\n\n“成果交付”的魔力：重塑各行各业的运营逻辑\n\n当软件从“文件柜”转变为“执行者”，它就能开始“操作”这些数字化信息，并直接交付成果。这将对我们熟悉的每一个行业带来冲击：\n• 旅行预订： AI智能体可以直接为75个高中生预订复杂的行程，而无需人类旅行社介入。\n• 销售： Salesforce不再仅仅是管理客户数据的平台，而是可以直接代表企业去寻找客户、进行销售，并按照获取客户的数量收费。AI甚至可以进行30分钟的客户满意度电话访谈，了解续约意向。\n• 制造业与库存： ERP系统不再只是记录库存，而是能回答“我的关税风险有多大？”或“我的供应商能否按时发货？”这类复杂问题，并执行审计或联系供应商的任务。\n• 图书馆： 催还图书或根据受欢迎程度自动订购新书，这些本由图书馆员完成的工作，未来可由软件智能体代劳。\n• 法律与会计： 软件不再只是存储法律文件或财务数据，而是可以直接起草合同，进行复杂的应收账款催收，甚至主动拨打电话与客户沟通。\n• 医疗健康： AI护士无法进行心肺复苏或处理枪伤，但完全可以高效地致电术后患者，询问疼痛级别、是否有发烧等症状，并提供初步的健康指导。这种简单的外拨电话服务，可以按次收费。\n• 人力资源： Workday不再只是记录员工信息，而是可以自动进行背景调查、核实工作经历，甚至协助员工办理福利手续。\n\n“机会密度”框架：AI如何解锁此前不可行的商业模式\n\nAI的真正力量，不仅在于提高现有工作的效率，更在于它能够解锁那些过去因成本过高、效率低下或资源稀缺而无法开展的商业模式。这便是“机会密度”框架的核心：AI通过显著降低客户获取成本（CAC）和商品销售成本（COGS），使得许多曾经在经济上不成立的业务，如今变得可行。\n\n以“自行车版Airbnb”为例。过去，这样的平台难以规模化，因为获取拥有闲置自行车的人，或在紧急情况下处理租赁纠纷，其人力成本（例如，聘请高薪销售人员、24/7客服）远高于其潜在收益。但现在，AI销售代表可以以极低的成本联系大量潜在用户，AI客服可以在紧急时刻提供多语言支持，AI背景审查可以快速完成。AI使得这些“非AI公司”能够以全新的姿态进入市场，将“不可能”变为“可能”。\n\n此外，AI还能解决人力劳动中的几个核心痛点：\n• 间歇性需求： 面对“黑色星期五”这类高峰期或航空公司因天气导致的突发情况，AI能够灵活扩展服务能力，无需频繁招聘和培训大量临时员工。\n• 士气低落的工作： 像债务催收这样经常面对负面情绪和辱骂的工作，AI不会受到影响，可以持续高效地执行。\n• 监管确定性： 在客户服务或法律咨询等领域，严格的监管要求（如不得进行欺骗性或滥用性行为）使得人类员工容易犯错。AI可以被精确编程，确保每一次交互都符合法规，提供更高的合规确定性。\n• 语言障碍： 一个AI智能体可以瞬间支持几十种语言，从而服务更广泛的全球用户群体。这在过去几乎是不可能实现的，因为要在特定地区找到能够兼顾多语言、按需、间歇性工作的员工，成本极高。\n\n变革的浪潮：软件吞噬劳动力的全球机遇\n\n“软件吞噬劳动力”不仅仅是效率的提升，更是市场边界的极大拓展。过去，一些依赖大量人力的行业（如合规官）由于缺乏合适的软件解决方案，其软件市场规模极小。但现在，AI使软件能够直接提供端到端的合规服务，从而将巨大的“人力市场”转化为软件公司可以攫取的巨大商机。\n\n这场由AI驱动的劳动力市场重构，正在全球范围内掀起波澜。美国的劳动力市场规模高达13万亿美元，而全球市场则更为庞大。对于那些能够洞察趋势、将AI融入其软件产品，并从提供“工具”转向交付“成果”的公司而言，这无疑是前所未有的历史机遇。它们将有机会创造出能够让现有“软件市场”显得微不足道的全新商业帝国。\n\n这场变革并非旨在简单地“淘汰”人类，而是以一种更深刻的方式，重新定义人类劳动的价值与形式。它迫使我们思考：在未来，当软件承担了大部分重复性和可编程的工作后，人类的独特价值和创造力又将在何处闪耀？这场静默而深刻的产业革命，正在我们眼前展开。",
      "created_at": "Sat Oct 04 01:34:49 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974250937679057344",
        "text": "highly recommended！\n https://t.co/uPulc976Hp",
        "created_at": "Fri Oct 03 23:11:18 +0000 2025",
        "lang": "en",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1141738094485315584",
          "name": "Rainier",
          "screen_name": "mtrainier2020",
          "description": "Cogito, ergo sum",
          "followers_count": 136734,
          "friends_count": 997,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 5,
          "favorite_count": 51,
          "reply_count": 2,
          "quote_count": 1
        }
      },
      "stats": {
        "retweet_count": 26,
        "favorite_count": 151,
        "reply_count": 5,
        "quote_count": 2
      }
    },
    {
      "id": "1974289934795083811",
      "text": "特定功能、私有数据就可能需要自己的 Agent\n\n比如说我朋友给自己的Design System 定制的 Coding Agent，默认会用他们自己的技术栈、自己的设计系统生成代码，如果用 Claude Code 就有很多定制的活。\n\n再比如你要一个 Email Agent 帮你处理邮件，你又不放心第三方做的，那么就可以自己做一个，只需要额外写读取、检索邮件的工具，就可以实现定制",
      "created_at": "Sat Oct 04 01:46:15 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974281726274019603",
        "text": "@dotey 宝哥，什么场景下需要自己开发一个 Agent？有什么作用？",
        "created_at": "Sat Oct 04 01:13:38 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "2835491863",
          "name": "kane",
          "screen_name": "kane_ares",
          "description": "Equal respect",
          "followers_count": 5,
          "friends_count": 204,
          "verified": false,
          "is_blue_verified": false
        },
        "stats": {
          "retweet_count": 0,
          "favorite_count": 0,
          "reply_count": 0,
          "quote_count": 1
        }
      },
      "stats": {
        "retweet_count": 9,
        "favorite_count": 60,
        "reply_count": 5,
        "quote_count": 1
      }
    },
    {
      "id": "1974341725784334470",
      "text": "RT @dotey: @ZNKatz @DanielEdrisian Codex (0.44+) now supports custom prompt templates. You can define variables within these templates, whi…",
      "created_at": "Sat Oct 04 05:12:03 +0000 2025",
      "lang": "en",
      "media": [],
      "retweet": {
        "id": "1974341663821615105",
        "text": "Codex (0.44+) now supports custom prompt templates. You can define variables within these templates, which can later be replaced by passing arguments when invoking the prompt.\n\nTo create a new prompt template, add an .md file to the ~/.codex/prompts/ directory.\n\nHere’s an example:\n\n\"\"\"\nFind all the changes in $COMMIT. Identify EVERY SINGLE CHANGE, and ensure nothing unnecessary is included. I want this PR to be exceptionally clean.\n\nHighlight any unnecessary code, especially leftover fragments from previously removed changes.\n\nIf any code could be simplified or streamlined elegantly, point it out.\n\nAdditionally, if there are multiple tests checking slightly different scenarios, consider consolidating them into a single test.\n\nYou value beauty and elegance. You avoid placing new code arbitrarily and carefully consider the appropriate abstraction level for changes. You prefer not to introduce special-case logic unless absolutely necessary; instead, you'd rather abstract such logic into reusable modes.\n\"\"\"\n\nIn this template, a variable $COMMIT is defined. You can replace this variable when invoking the prompt like this:\n\n`/prompts:elegant2 COMMIT=\"xxxxx\"`\n\n`elegant2` is the name of the prompt template.\n\nAdditionally, positional arguments ($1, $2, $3) and the $ARGUMENTS variable are also supported .",
        "created_at": "Sat Oct 04 05:11:48 +0000 2025",
        "lang": "en",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "3178231",
          "name": "宝玉",
          "screen_name": "dotey",
          "description": "Prompt Engineer, dedicated to learning and disseminating knowledge about AI, software engineering, and engineering management.",
          "followers_count": 137079,
          "friends_count": 1423,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 1,
          "favorite_count": 5,
          "reply_count": 0,
          "quote_count": 0
        }
      },
      "quoted": null,
      "stats": {
        "retweet_count": 1,
        "favorite_count": 0,
        "reply_count": 0,
        "quote_count": 0
      }
    },
    {
      "id": "1974342572501028998",
      "text": "帮转",
      "created_at": "Sat Oct 04 05:15:25 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974316545611591857",
        "text": "招人招人招人 句乐部继续招人\n\n全栈岗位（vue3+nodejs）\n远程小团队\nremote  work  没有任何地区上的限制\n项目方向 -> AI+语言教育+游戏化\n事少 不内耗 没有开不完的会\n技术主导！\n\n要求：\n- 积极主动\n- 对技术有追求\n- 不想拧螺丝 想要在AI时代拼一把的！\n\n求靠谱小伙伴\n求各位大佬介绍\n详细的 JD： https://t.co/sHBAbkRWFo",
        "created_at": "Sat Oct 04 03:32:00 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1873211207295090689",
          "name": "阿崔cxr",
          "screen_name": "cuixr1314",
          "description": "句乐部创始人 \n一个通过句子游戏化练习英语的工具  https://t.co/x1kXQqTerV\n合作wx：cuixr1314",
          "followers_count": 7672,
          "friends_count": 84,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 15,
          "favorite_count": 148,
          "reply_count": 23,
          "quote_count": 12
        }
      },
      "stats": {
        "retweet_count": 0,
        "favorite_count": 2,
        "reply_count": 1,
        "quote_count": 0
      }
    },
    {
      "id": "1974346737805390021",
      "text": "这种 MCP 路由的方案理论上可行，为了解决工具多的工具做一个路由工具的工具，但像是为了解决官僚臃肿的问题特别成立了一个委员会而不是精简机构。\n\n存在几个问题：\n1. 你不能有效利用 Prompt Cache，把工具相关的 Prompt Cache 起来，因为所有的工具调用都是动态的\n\n2. 有哪些工具对于 LLM 来说是不透明的，它不知道自己有哪些工具能力，通过路由绕一下会极大影响决策能力。\n\n3. 对于做工具路由的模型来说也是不好做决策，因为它没有完整的上下文，上下文给多了你还不如不用，给少了又做不好。\n\n不是一个好的方案，个人不推荐。",
      "created_at": "Sat Oct 04 05:31:58 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974343398325973433",
        "text": "最近看到一个号称解决了MCP痛点的方案 @Klavis_AI 的 Strata，调研了一番。先说结论：这是为了解决MCP过量接入导致的工具索引过载和上下文爆炸。\n如果你的agent不需要接入几十甚至上百个MCP（一般来说不会有这种需求），那么你不用了解。\n但设计本身还是相当有意思的，思路可以借鉴。\n\n详细解析见此：\nStrata 是否解决了现阶段MCP的痛点？\nhttps://t.co/NeGXII6aT6\n@lyricwai @dongxi_nlp @dotey @gasikaramada 好久没写博客了，老铁们求意见。",
        "created_at": "Sat Oct 04 05:18:42 +0000 2025",
        "lang": "zh",
        "media": [
          {
            "type": "photo",
            "id": "1974342091439550464",
            "url": "https://pbs.twimg.com/media/G2ZFq_FbMAAQL7N.jpg"
          }
        ],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1443565229846118400",
          "name": "CryptoNerdCn 🦇🔊",
          "screen_name": "cryptonerdcn",
          "description": "InfoFi加分站: https://t.co/cOXs0xHtJY | Delegate of @ensdomains | 中文&日本語&English | @WasmCairo | Into @starknet by @starknetastrocn",
          "followers_count": 13519,
          "friends_count": 1397,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 15,
          "favorite_count": 106,
          "reply_count": 16,
          "quote_count": 1
        }
      },
      "stats": {
        "retweet_count": 7,
        "favorite_count": 51,
        "reply_count": 5,
        "quote_count": 0
      }
    },
    {
      "id": "1974371690412191845",
      "text": "理论上 RAG 搜索工具没问题，但这些都是简单问题复杂化，还是不推荐使用，效果不如我上面说的几种方案简单实用\n补充一条：绝大部分时候你都不需要 RAG\nhttps://t.co/1S2n2nzlWE",
      "created_at": "Sat Oct 04 07:11:07 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974370351636451428",
        "text": "@dotey 工具也是可以使用rag的。工具多了直接用rag检索最相关的工具放到上下文。",
        "created_at": "Sat Oct 04 07:05:48 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1546691843039334401",
          "name": "zhishui",
          "screen_name": "zhishui98553599",
          "description": "",
          "followers_count": 81,
          "friends_count": 76,
          "verified": false,
          "is_blue_verified": false
        },
        "stats": {
          "retweet_count": 0,
          "favorite_count": 0,
          "reply_count": 0,
          "quote_count": 1
        }
      },
      "stats": {
        "retweet_count": 1,
        "favorite_count": 4,
        "reply_count": 2,
        "quote_count": 0
      }
    },
    {
      "id": "1974379386729357695",
      "text": "功能路由而不是工具路由",
      "created_at": "Sat Oct 04 07:41:42 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974376923632710109",
        "text": "@dotey 是的，相比工具路由，功能路由更好，和用户直接交互的agent负责接收输入，选择子智能体或者工作流，返回对应的结果，再继续下一次路由，也许有的情况下也可以并发执行任务，直到它判断现有的信息足够可以返回结果给用户。对于每一个功能，它们要用到的工具应该不多。同时主agent的上下文也很少。",
        "created_at": "Sat Oct 04 07:31:55 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1546691843039334401",
          "name": "zhishui",
          "screen_name": "zhishui98553599",
          "description": "",
          "followers_count": 81,
          "friends_count": 76,
          "verified": false,
          "is_blue_verified": false
        },
        "stats": {
          "retweet_count": 0,
          "favorite_count": 1,
          "reply_count": 0,
          "quote_count": 1
        }
      },
      "stats": {
        "retweet_count": 0,
        "favorite_count": 0,
        "reply_count": 0,
        "quote_count": 0
      }
    },
    {
      "id": "1974520194807591020",
      "text": "如果你在为公司现有业务集成 AI Agent，或者迁移到 AI Agent，我自己的一点思考供参考：\n\n1. 如果你流程的路径很确定并且效率很高，那么也许你只需要在原有流程上集成一些 AI 功能就可以，并不一定要变成 Agent\n\n通常 Agent 没有固定流程，依赖于用户的输入来由 LLM 决策调用什么工具\n\n2. 为 Agent 去重新设计新的工具而不是让 Agent 去用现有的工具\n\n通常公司内部已经有一些成熟的工具，但这些工具是为人设计的而不是 Agent 设计的，当你去做 Agent，要重新为 Agent 做新的工具，什么工具是最适合 Agent 就去打造什么工具，但不是因为你有什么工具所以让 Agent 去用什么工具。\n\n另外 Agent 的工具要融入上下文管理：\n- 描述要清晰具体，让 LLM 知道什么场景该使用什么工具\n- 输入参数要明确：即让 LLM 知道该传什么参数，又要让工具有足够的数据可以执行\n- 输出结果要清晰明了，不要有太多无关上下文内容，因为工具输出的结果会加入 Agent 的上下文，有些很长的输出可以保存到外部文件按需读取\n\n3. 不要为了 MCP 而用 MCP\n\nMCP 很流行，但它的优势是让你的工具可以兼容不同的模型，不同的 AI 平台，如果你的工具只有你自己的 Agent 用，没必要做成 MCP，普通的命令行、脚本、API 都可以。你看 Claude Code 的十几个工具没有一个是 MCP。\n\n4. 工具数量不要太多，基于功能可以适当拆分子智能体\n\n由于工具的描述、输入和输出都占用上下文空间，所以工具数量不能太大，否则会影响 Agent 的能力。\n\n如果你的工具实在太多，可以考虑按照功能拆分成子智能体，让一个子智能体负责某些特定功能的任务，它可以拥有自己的工具集，主 Agent 则负责调度这些子 Agent，为子 Agent 提供独立的上下文，并收集子 Agent 返回的结果。\n\n如果子 Agent 或者工具的结果之间有依赖关系，不要并行执行任务，否则会搞乱上下文\n\n5. 需要为 Agent 重新设计交互\n\n你的软件也许已经有一套交互方式了，但当你去做 Agent 的时候，要重新思考什么是最佳交互方式。\n\nAgent 的交互和传统的软件交互是不一样的，通常以对话为主，用户可以通过对话框输入文本信息，上传文档、图片等作为上下文一部分，信息则更像聊天对话，实时可以看到 AI 返回文本、工具调用结果等。\n\n还可以是有一个主要工作区，类似于传统的软件交互，侧边栏是 Agent 聊天对话。\n\n在 Agent 交互方面，ChatGPT、Claude、Cursor、Notion、Gemini 等产品都有很多交互可以参考，多借鉴前沿主流的 Agent 交互方式",
      "created_at": "Sat Oct 04 17:01:13 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": null,
      "stats": {
        "retweet_count": 73,
        "favorite_count": 329,
        "reply_count": 9,
        "quote_count": 8
      }
    },
    {
      "id": "1974521109283574179",
      "text": "6. 前期不不需要重头去实现一个 Agent，多用现成框架\n\n市面上有很多，我唯一推荐的是 Claude Agent SDK ，即开即用，原型、POC 阶段用它就可以了，不需要花太多时间在其他上面，验证完了后再重头搭建不迟。\nhttps://t.co/fNNjmTPNEZ",
      "created_at": "Sat Oct 04 17:04:51 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1973937260220330005",
        "text": "如果你想开发一个 Agent，无论你是打算做 CLI 还是做 Web 还是 Windows，都可以考虑使用 Claude Agent SDK，和 Claude Code 共享的底层代码，Claude Code 就是基于它之上加了个 CLI 的 UI，也就是说你完全可以基于它写一个 Claude Code 出来。\n\n我昨天帮朋友花了几个小时就实现了个简单的 Agent，实现了输入提示词，就可以基于某个没训练的 Design System 写一套 UI 出来。\n\n他写的这个 Agent 原理很简单，就是把这套设计系统的所有 Markdown 文档（几百个）放到一个它可以访问的目录，然后在 Systme Prompt 里面引导它去检索这个文档目录。\n\n当用户输入提示词或者 Screenshot 要做一个 UI，Agent 就根据提示词规划可能要用到的组件，然后用 SDK 自带的 GREP 工具去检索文档库找到这些组件的 API，最后基于收集到的信息用这个 Design System 组件生成页面。\n\n这个 SDK API 很简单，但很强大，你不止是可以用它内置的工具（Task、Grep、WebFetch 等等），你还可以添加自己的工具，还可以用 MCP。并且它可以把整个交互的结果通过 API 让你可以获取到原始的请求和返回消息，这样你可以自己实现一套比 CLI 更好用的交互 UI。\n\n当然这个局限也有：\n1. 只能用 Claude 模型兼容的 API，如果你想用 GPT-5 之类模型，估计效果不会太好\n2. 只支持 Python 和 TypeScript\n3. Tokens 消耗飞快\n\n如果你只是做前期的 POC，强烈建议你试试。",
        "created_at": "Fri Oct 03 02:24:51 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "3178231",
          "name": "宝玉",
          "screen_name": "dotey",
          "description": "Prompt Engineer, dedicated to learning and disseminating knowledge about AI, software engineering, and engineering management.",
          "followers_count": 137079,
          "friends_count": 1423,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 105,
          "favorite_count": 605,
          "reply_count": 31,
          "quote_count": 20
        }
      },
      "stats": {
        "retweet_count": 6,
        "favorite_count": 21,
        "reply_count": 4,
        "quote_count": 0
      }
    },
    {
      "id": "1974546684752568544",
      "text": "RT @linexjlin: Karpathy  的最后一段说的很好。  \n\n他认为现在我们在卷 AI 训练集群规模，数据质量，各种局部调优...，本质上是在追求对人类数据的拟合精度，很可能已经陷入到另外的 “The Bitter Lesson” 当中了。这可能也是 Sutto…",
      "created_at": "Sat Oct 04 18:46:29 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": {
        "id": "1974507705000968535",
        "text": "Karpathy  的最后一段说的很好。  \n\n他认为现在我们在卷 AI 训练集群规模，数据质量，各种局部调优...，本质上是在追求对人类数据的拟合精度，很可能已经陷入到另外的 “The Bitter Lesson” 当中了。这可能也是 Sutton 否定当前 LLM 的原因。  \n\n也许，现在我们更需要的是探索如何让模型出现 “好奇心”， “内在驱动力” 这类自主学习的核心机制。 而这种机制也许也是 Sutton 在寻找的 “0 人类数据智能”关键。",
        "created_at": "Sat Oct 04 16:11:36 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": {
          "id": "1973435013875314729",
          "text": "Finally had a chance to listen through this pod with Sutton, which was interesting and amusing.\n\nAs background, Sutton's \"The Bitter Lesson\" has become a bit of biblical text in frontier LLM circles. Researchers routinely talk about and ask whether this or that approach or idea is sufficiently \"bitter lesson pilled\" (meaning arranged so that it benefits from added computation for free) as a proxy for whether it's going to work or worth even pursuing. The underlying assumption being that LLMs are of course highly \"bitter lesson pilled\" indeed, just look at LLM scaling laws where if you put compute on the x-axis, number go up and to the right. So it's amusing to see that Sutton, the author of the post, is not so sure that LLMs are \"bitter lesson pilled\" at all. They are trained on giant datasets of fundamentally human data, which is both 1) human generated and 2) finite. What do you do when you run out? How do you prevent a human bias? So there you have it, bitter lesson pilled LLM researchers taken down by the author of the bitter lesson - rough!\n\nIn some sense, Dwarkesh (who represents the LLM researchers viewpoint in the pod) and Sutton are slightly speaking past each other because Sutton has a very different architecture in mind and LLMs break a lot of its principles. He calls himself a \"classicist\" and evokes the original concept of Alan Turing of building a \"child machine\" - a system capable of learning through experience by dynamically interacting with the world. There's no giant pretraining stage of imitating internet webpages. There's also no supervised finetuning, which he points out is absent in the animal kingdom (it's a subtle point but Sutton is right in the strong sense: animals may of course observe demonstrations, but their actions are not directly forced/\"teleoperated\" by other animals). Another important note he makes is that even if you just treat pretraining as an initialization of a prior before you finetune with reinforcement learning, Sutton sees the approach as tainted with human bias and fundamentally off course, a bit like when AlphaZero (which has never seen human games of Go) beats AlphaGo (which initializes from them). In Sutton's world view, all there is is an interaction with a world via reinforcement learning, where the reward functions are partially environment specific, but also intrinsically motivated, e.g. \"fun\", \"curiosity\", and related to the quality of the prediction in your world model. And the agent is always learning at test time by default, it's not trained once and then deployed thereafter. Overall, Sutton is a lot more interested in what we have common with the animal kingdom instead of what differentiates us. \"If we understood a squirrel, we'd be almost done\".\n\nAs for my take...\n\nFirst, I should say that I think Sutton was a great guest for the pod and I like that the AI field maintains entropy of thought and that not everyone is exploiting the next local iteration LLMs. AI has gone through too many discrete transitions of the dominant approach to lose that. And I also think that his criticism of LLMs as not bitter lesson pilled is not inadequate. Frontier LLMs are now highly complex artifacts with a lot of humanness involved at all the stages - the foundation (the pretraining data) is all human text, the finetuning data is human and curated, the reinforcement learning environment mixture is tuned by human engineers. We do not in fact have an actual, single, clean, actually bitter lesson pilled, \"turn the crank\" algorithm that you could unleash upon the world and see it learn automatically from experience alone.\n\nDoes such an algorithm even exist? Finding it would of course be a huge AI breakthrough. Two \"example proofs\" are commonly offered to argue that such a thing is possible. The first example is the success of AlphaZero learning to play Go completely from scratch with no human supervision whatsoever. But the game of Go is clearly such a simple, closed, environment that it's difficult to see the analogous formulation in the messiness of reality. I love Go, but algorithmically and categorically, it is essentially a harder version of tic tac toe. The second example is that of animals, like squirrels. And here, personally, I am also quite hesitant whether it's appropriate because animals arise by a very different computational process and via different constraints than what we have practically available to us in the industry. Animal brains are nowhere near the blank slate they appear to be at birth. First, a lot of what is commonly attributed to \"learning\" is imo a lot more \"maturation\". And second, even that which clearly is \"learning\" and not maturation is a lot more \"finetuning\" on top of something clearly powerful and preexisting. Example. A baby zebra is born and within a few dozen minutes it can run around the savannah and follow its mother. This is a highly complex sensory-motor task and there is no way in my mind that this is achieved from scratch, tabula rasa. The brains of animals and the billions of parameters within have a powerful initialization encoded in the ATCGs of their DNA, trained via the \"outer loop\" optimization in the course of evolution. If the baby zebra spasmed its muscles around at random as a reinforcement learning policy would have you do at initialization, it wouldn't get very far at all. Similarly, our AIs now also have neural networks with billions of parameters. These parameters need their own rich, high information density supervision signal. We are not going to re-run evolution. But we do have mountains of internet documents. Yes it is basically supervised learning that is ~absent in the animal kingdom. But it is a way to practically gather enough soft constraints over billions of parameters, to try to get to a point where you're not starting from scratch. TLDR: Pretraining is our crappy evolution. It is one candidate solution to the cold start problem, to be followed later by finetuning on tasks that look more correct, e.g. within the reinforcement learning framework, as state of the art frontier LLM labs now do pervasively.\n\nI still think it is worth to be inspired by animals. I think there are multiple powerful ideas that LLM agents are algorithmically missing that can still be adapted from animal intelligence. And I still think the bitter lesson is correct, but I see it more as something platonic to pursue, not necessarily to reach, in our real world and practically speaking. And I say both of these with double digit percent uncertainty and cheer the work of those who disagree, especially those a lot more ambitious bitter lesson wise.\n\nSo that brings us to where we are. Stated plainly, today's frontier LLM research is not about building animals. It is about summoning ghosts. You can think of ghosts as a fundamentally different kind of point in the space of possible intelligences. They are muddled by humanity. Thoroughly engineered by it. They are these imperfect replicas, a kind of statistical distillation of humanity's documents with some sprinkle on top. They are not platonically bitter lesson pilled, but they are perhaps \"practically\" bitter lesson pilled, at least compared to a lot of what came before. It seems possibly to me that over time, we can further finetune our ghosts more and more in the direction of animals; That it's not so much a fundamental incompatibility but a matter of initialization in the intelligence space. But it's also quite possible that they diverge even further and end up permanently different, un-animal-like, but still incredibly helpful and properly world-altering. It's possible that ghosts:animals :: planes:birds.\n\nAnyway, in summary, overall and actionably, I think this pod is solid \"real talk\" from Sutton to the frontier LLM researchers, who might be gear shifted a little too much in the exploit mode. Probably we are still not sufficiently bitter lesson pilled and there is a very good chance of more powerful ideas and paradigms, other than exhaustive benchbuilding and benchmaxxing. And animals might be a good source of inspiration. Intrinsic motivation, fun, curiosity, empowerment, multi-agent self-play, culture. Use your imagination.",
          "created_at": "Wed Oct 01 17:09:06 +0000 2025",
          "lang": "en",
          "media": [],
          "retweet": null,
          "quoted": null,
          "user": {
            "id": "33836629",
            "name": "Andrej Karpathy",
            "screen_name": "karpathy",
            "description": "Building @EurekaLabsAI. Previously Director of AI @ Tesla, founding team @ OpenAI, CS231n/PhD @ Stanford. I like to train large deep neural nets.",
            "followers_count": 1407837,
            "friends_count": 1018,
            "verified": false,
            "is_blue_verified": true
          },
          "stats": {
            "retweet_count": 1223,
            "favorite_count": 9273,
            "reply_count": 415,
            "quote_count": 343
          }
        },
        "user": {
          "id": "1156074140",
          "name": "Line",
          "screen_name": "linexjlin",
          "description": "AGI, Scaling Law, MOE, RL, Parallel Thinking ...",
          "followers_count": 1172,
          "friends_count": 1426,
          "verified": false,
          "is_blue_verified": false
        },
        "stats": {
          "retweet_count": 35,
          "favorite_count": 157,
          "reply_count": 4,
          "quote_count": 2
        }
      },
      "quoted": null,
      "stats": {
        "retweet_count": 35,
        "favorite_count": 0,
        "reply_count": 0,
        "quote_count": 0
      }
    },
    {
      "id": "1974556078974066819",
      "text": "话说OpenAI这帮天才疯子，自从用ChatGPT把地球人的脑子搅成一锅粥后，就觉得不过瘾。他们憋了个大招，名叫Sora 2。这玩意儿是什么？你可以把它想象成一个装满了全世界影碟店、图书馆和动漫网站的硬盘，然后被喂给了一个患有“创作欲过剩症”的人工智能。\n\n他们没跟任何人打招呼，迪士尼的米老鼠、任天堂的马力欧、派拉蒙的派大星……有一个算一个，全被“请”进了这个数字炼丹炉。然后，伴随着山姆·奥特曼（Sam Altman）那标志性的、仿佛在说“我没做错任何事，只是世界还没准备好”的微笑，Sora 2上线了。市场宣传？ aggressive得像是要把传单塞进你家猫的嘴里。\n\n发布后的72小时，互联网疯了。\n\n这不是文艺复兴，这是文艺复“疯”。皮卡丘在CVS里零元购的监控录像，比《教父》的点击率还高；海绵宝宝梳着希特勒的小胡子在汉堡王发表演讲；盖酷家庭的彼得·格里芬和马力欧兄弟因为一根水管的所有权问题在《南方公园》的街头大打出手。\n\n这玩意儿在48小时内，像一枚文化核弹一样登顶了App Store。OpenAI内部，香槟开得比代码跑得还快。\n\n> “老板，成了！用户正在疯狂地解构人类所有的文化IP！”\n> “很好，”奥特曼可能正用Sora生成自己登上时代周刊封面的视频，“一切尽在掌握。”\n> \n\n新一轮融资光速到位，估值直接飙到$5000亿。这笔钱烫手得，仿佛是用全世界法务的眼泪铸成的金条。\n\n然而，狂欢的BGM还没结束，门外就传来了敲门声——不，是撞门声。迪士尼那群穿着西装的哥斯拉、纽约时报的笔杆子军团，以及所有你能想到的内容巨头，带着山一样高的律师函，把OpenAI的服务器围得水泄不通。\n\n面对十面埋伏，你以为奥特曼会道歉？会赔款？太天真了。他玩了一招“乾坤大挪移”，玩得比张无忌还溜。\n\n“快！把责任这坨烫手山芋扔出去！”\n\n一夜之间，一个紧急更新推送给了所有用户。打开Sora 2，一个硕大的弹窗怼到你脸上，字体大到像是怕你瞎。内容翻译过来就是：\n\n * 《你好，背锅侠》协议：\n\n   * 你！ 尊贵的用户，你发誓你上传的任何提示词、图片、想法，都不涉及任何有版权的东西。马力欧是你自己画的，皮卡丘是你家后院抓的。\n\n   * 你！ 尊贵的用户，你保证你拥有你生成内容的所有权利，迪士尼找你麻烦，跟我们OpenAI没半毛钱关系。\n\n   * 滥用？ 呵呵，你的Sora和ChatGPT账号将一起人间蒸发，永世不得超生。\n\n与此同时，奥特曼慢悠悠地发了篇博客，标题是：“请负责任地使用我们的魔法 :)”。那感觉，就像一个把军火库钥匙交给疯子的军火商，然后贴心地说：“亲，别伤到自己哦。”\n\n至于版权方？“我们提供了‘退出’机制嘛，”OpenAI的公关说，“你们可以一个一个一个一个一个地把你们的角色申请排除出去。”\n\n米老鼠、唐老鸭、高飞、艾莎、安娜、辛巴……迪士尼法务部看着那长得像电话黄页的角色列表，默默地点了根烟。\n\n用户们也不是傻子。当他们看清这份“卖身契”后，愤怒了。社交媒体上，“#DeleteSora”运动风起云涌。\n\n他们点下“删除账户”按钮，以为能潇洒离去。\n\n然后，真正的恐怖降临了。\n\n一个提示框跳了出来：“删除Sora账户将永久删除您的整个OpenAI账户，包括ChatGPT、API访问权限以及所有历史数据。此操作不可逆，您的邮箱和手机号将被永久拉黑。”\n\n所有人都愣住了。\n\n想走？可以。把你过去几年和ChatGPT培养的感情、你赖以为生的API接口、你所有的数据和心血，统统烧掉。你不仅要离开这个公园，你还要在公园的黑名单上挂一辈子。\n\n你以为你在用他的产品，其实你已经成为了他产品的一部分，一个无法移除的、自带法律责任的插件。你被困住了，被你自己的数据和习惯困在了这个数字牢笼里。\n\n这已经不是“LOL, LMAO even”了。\n\n这剧本，连好莱坞的编剧看了都得给奥特曼递根烟，说一句：“哥们，还是你们硅谷会玩儿。”",
      "created_at": "Sat Oct 04 19:23:49 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974491324947542170",
        "text": ">be OpenAI\n>train Sora 2 on every movie/show ever made without permission\n>release product with aggressive marketing \n\n>Day 1-3\n>Pikachu robbing CVS\n>Hitler SpongeBob\n>Mario, family guy, south park everywhere\n>#1 app store in 48 hours\n\n>Employee: \"ser, we won\"\n>\"users doing exactly what we planned them to do\"\n>new funding secured\n>$500B valuation\n\n>suddenly lawsuits incoming\n>disney, NYT, everyone mobilizing\n\n>Altman: \"quick, make it the users' fault\"\n>\"and copyright holders' fault too\"\n\n>emergency update ships overnight\n>*media upload agreement popup*\n>YOU agree not to upload copyrighted material\n>YOU agree you have all rights to uploads  \n>misuse = permanent banned from sora AND chatgpt\n\n>Liability successfully transferred:\n>✓ users\n>✓ copyright holders must opt out each character individually\n>✗ openai \n\n>Altman drops blog: \"please use responsibly :)\"\n\n>users try to delete sora account\n>deletes chatgpt too\n>banned from ever signing up again\n>API access? gone\n>all data? deleted\n>can't even use same email/phone\n>users realize they're trapped\n\nLOL, LMAO even.",
        "created_at": "Sat Oct 04 15:06:30 +0000 2025",
        "lang": "en",
        "media": [
          {
            "type": "photo",
            "id": "1974491320971382784",
            "url": "https://pbs.twimg.com/media/G2bNZSaXUAAwScp.jpg"
          },
          {
            "type": "photo",
            "id": "1974491320992292865",
            "url": "https://pbs.twimg.com/media/G2bNZSfWYAEqdv0.jpg"
          },
          {
            "type": "photo",
            "id": "1974491320983900160",
            "url": "https://pbs.twimg.com/media/G2bNZSdWUAAshs5.jpg"
          }
        ],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "1087640384975577089",
          "name": "NIK",
          "screen_name": "ns123abc",
          "description": "non technical member of technical staff",
          "followers_count": 43237,
          "friends_count": 4475,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 3600,
          "favorite_count": 36431,
          "reply_count": 525,
          "quote_count": 377
        }
      },
      "stats": {
        "retweet_count": 85,
        "favorite_count": 384,
        "reply_count": 18,
        "quote_count": 13
      }
    },
    {
      "id": "1974557180490809569",
      "text": "本文 Prompt：https://t.co/ZnrSLbt9F6",
      "created_at": "Sat Oct 04 19:28:11 +0000 2025",
      "lang": "ja",
      "media": [],
      "retweet": null,
      "quoted": null,
      "stats": {
        "retweet_count": 5,
        "favorite_count": 41,
        "reply_count": 3,
        "quote_count": 0
      }
    },
    {
      "id": "1974560219272278420",
      "text": "谷歌一个看似微小，实则影响巨大的变动\n上个月，谷歌悄悄地移除了 num=100 这个搜索参数。\n这意味着，你再也不能一次性查看 100 条搜索结果了。现在，默认的上限是 10 条。\n这事儿为什么这么重要？\n * 市面上绝大多数的大语言模型 (LLM)，比如 OpenAI 的模型和 Perplexity，它们获取信息都（直接或间接地）依赖谷歌索引好的搜索结果，即便它们自己也有网络爬虫。\n * 谷歌这一改，等于一夜之间把它们访问互联网“长尾”信息 (指那些不太热门、但数量庞大的搜索结果) 的能力砍掉了 90%。\n连锁反应来了\n * 根据《搜索引擎之地》（Search Engine Land）的报道，高达 88% 的网站发现自己的页面曝光量出现了下降。\n * 像 Reddit 这样的网站，之前很多内容都排在搜索结果的第 11-100 位，现在被大语言模型引用的次数直线下降。受此影响，其股价下跌了 15%。\n对创业公司来说，这简直是当头一棒。想在网上被用户发现，变得难上加难。对于 Reddit 这类网站来说，它们作为 AI 引擎优化 (AEO) (一种专门针对 AI 模型搜索进行优化的策略) 的一部分，其整个游戏规则都改变了。\n现在，仅仅做出一个好产品已经远远不够了，你必须先搞定推广渠道。因为如果人们压根发现不了你，他们就永远不会有机会去评估你的产品好坏。\n大多数工程师似乎总是忽略这个残酷的现实：一个渠道牛逼的平庸产品，总能打败一个渠道很烂的优秀产品。\n正如彼得·蒂尔（Peter Thiel）所说：\n“大多数公司连一个有效的推广渠道都做不起来：失败最常见的原因是销售不力，而不是产品不行。只要你能打通一个推广渠道，你的生意就能成了。但如果你想多管齐下，结果却一个都没做精，那你就死定了。\n强大的销售和分销本身就可以创造垄断，哪怕产品毫无特色。但反过来就不行了。无论你的产品有多牛——就算它完全符合用户习惯，谁用谁说好——你仍然必须用一个强大的分销计划来支持它。”\n结论就是：\n渠道 > 产品\n(h/t Adarsh Appaiah on LinkedIn)",
      "created_at": "Sat Oct 04 19:40:16 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": null,
      "quoted": {
        "id": "1974090092789588075",
        "text": "Google just made a subtle but massive change\n\nLast month, Google quietly removed the num=100 search parameter.\n\nThis means you can no longer view 100 results at once. The default max is now 10.\n\nWhy does this matter?\n\n- Most LLMs (OpenAI, Perplexity, etc.) rely (directly or indirectly) on Google’s indexed results, alongside their own crawlers.\n\n- Overnight, their access to the “long tail” of the internet was cut by 90%.\n\nThe fallout:\n\n- According to Search Engine Land, 88% of sites saw a drop in impressions.\n\n- Reddit, which often ranks in positions 11–100, saw its LLM citations plummet. Its stock dropped 15%.\n\nFor startups, this is brutal. Visibility just got harder. Reddit as part of AEO just changed entirely.\n\nIt’s no longer enough to build a great product you need to crack distribution first. Because if people can’t discover you, they’ll never get to evaluate you.\n\nMost engineers seem to always neglect this reality, but a mediocre product with great distribution will always beat a great product with mediocre distribution.\n\nAs Peter Thiel says:\n\n“Most businesses get zero distribution channels to work: poor sales rather than bad product is the most common cause of failure. If you can get just one distribution channel to work, you have a great business. If you try for several but don’t nail one, you’re finished.\n\nSuperior sales and distribution by itself can create a monopoly, even with no product differentiation. The converse is not true. No matter how strong your product — even if it easily fits into already established habits and anybody who tries it likes it immediately — you must still support it with a strong distribution plan.\"\n\nDistribution > Product\n\n(h/t Adarsh Appaiah on LinkedIn)",
        "created_at": "Fri Oct 03 12:32:09 +0000 2025",
        "lang": "en",
        "media": [
          {
            "type": "photo",
            "id": "1974086284202115072",
            "url": "https://pbs.twimg.com/media/G2VdBC3WgAA5Wt6.png"
          }
        ],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "2219164145",
          "name": "Nicolai Svane 🦋",
          "screen_name": "NicooSvane",
          "description": "I write about founder stories + creative growth hacks | I'm scaling a content studio to bootstrap the Cursor for content | ex-@binance\n\nlife stuff @NicoSvane",
          "followers_count": 3434,
          "friends_count": 752,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 1640,
          "favorite_count": 14007,
          "reply_count": 470,
          "quote_count": 356
        }
      },
      "stats": {
        "retweet_count": 193,
        "favorite_count": 1080,
        "reply_count": 65,
        "quote_count": 24
      }
    },
    {
      "id": "1974575048318124216",
      "text": "RT @blackanger: 在人工智能时代，知识的价值是什么？\n\n如果一个 AI 可以回答任何问题，那么人类的学习和思考还有意义吗？\n\n我的答案：有意义，因为知识不等于智慧。\n\nAI 可以在瞬间回答任何问题、检索任何事实——它让“知道”变得廉价，但也因此让“理解”变得更加珍…",
      "created_at": "Sat Oct 04 20:39:12 +0000 2025",
      "lang": "zh",
      "media": [],
      "retweet": {
        "id": "1974500106327265285",
        "text": "在人工智能时代，知识的价值是什么？\n\n如果一个 AI 可以回答任何问题，那么人类的学习和思考还有意义吗？\n\n我的答案：有意义，因为知识不等于智慧。\n\nAI 可以在瞬间回答任何问题、检索任何事实——它让“知道”变得廉价，但也因此让“理解”变得更加珍贵。\n\n知识是一种“外在的力量”，它可以被下载、被传输、被存储。\n而智慧是一种“内在的光”，它只能被体悟、被淬炼、被生活雕刻出来。\n\n当我们学习，不仅仅是在获取信息，而是在通过学习的过程，让大脑形成独立思考的结构，让心灵与世界产生共鸣。\n\nAI 可以告诉你“怎么做”，但唯有人类能理解“为什么做”，以及“值不值得做”。\n\n正如《西游记》中的唐僧，真正的“真经”并不在佛祖手中那几卷纸页上，而是在九九八十一难中的磨砺，最终拿到手的那本“无字天书”。那本无字天书，记录的不是字句，而是困境、选择与顿悟所留下的无声痕迹。\n\n在人工智能可以回答一切的时代，人类学习的意义在于，让自己的灵魂仍然有问题要问，去构建属于自己的“无字天书”。",
        "created_at": "Sat Oct 04 15:41:24 +0000 2025",
        "lang": "zh",
        "media": [],
        "retweet": null,
        "quoted": null,
        "user": {
          "id": "7755562",
          "name": "AlexZ 🦀",
          "screen_name": "blackanger",
          "description": "System in Rust, Application in AI.\nSlow down. Have fun. Live well.\nINTJ-A. No ism.\nLifelong Programmer and  Writer.\n\n🦀 保命",
          "followers_count": 12943,
          "friends_count": 1292,
          "verified": false,
          "is_blue_verified": true
        },
        "stats": {
          "retweet_count": 21,
          "favorite_count": 62,
          "reply_count": 5,
          "quote_count": 4
        }
      },
      "quoted": null,
      "stats": {
        "retweet_count": 21,
        "favorite_count": 0,
        "reply_count": 0,
        "quote_count": 0
      }
    },
    {
      "id": "1974606131575341075",
      "text": "codex 中输入 /status 就能看状态， pro 用户的 Token 用量也是有限制的，但正常还是不容易达到的，我最近一周使用强度蛮高，只用 GPT-5-Codex high，半周也才 26% https://t.co/etcJvMkkA0",
      "created_at": "Sat Oct 04 22:42:42 +0000 2025",
      "lang": "zh",
      "media": [
        {
          "type": "photo",
          "id": "1974605368790577152",
          "url": "https://pbs.twimg.com/media/G2c1HvpXcAAOBt8.jpg"
        }
      ],
      "retweet": null,
      "quoted": null,
      "stats": {
        "retweet_count": 2,
        "favorite_count": 55,
        "reply_count": 11,
        "quote_count": 1
      }
    },
    {
      "id": "1974606781666967931",
      "text": "首先 DeepSeek 模型和 DS 应用是两回事，模型的权重外部是改变不了的，能改变的是 DS 应用调用搜索引擎的搜索结果，这跟模型没关系，普通的 SEO 污染而已；\n\n然后现在很多人对 DeepSeek 不够满意不是 DS 变了，是用户的期望值变了，要知道 DS R1 是快一年前的模型，而现在用户对模型能力的要求已不是当时的标准了。",
      "created_at": "Sat Oct 04 22:45:17 +0000 2025",
      "lang": "zh",
      "media": [
        {
          "type": "photo",
          "id": "1974606512766025728",
          "url": "https://pbs.twimg.com/media/G2c2KVSXgAAVG-Z.jpg"
        }
      ],
      "retweet": null,
      "quoted": null,
      "stats": {
        "retweet_count": 2,
        "favorite_count": 32,
        "reply_count": 5,
        "quote_count": 0
      }
    }
  ]
}