#!/usr/bin/env python3
"""
æ¯æ—¥æ¨æ–‡æŠ¥å‘Šç”Ÿæˆå™¨ - å®Œæ•´çš„ç«¯åˆ°ç«¯ç³»ç»Ÿ
é›†æˆçˆ¬è™«æ•°æ®é‡‡é›†ã€æ•°æ®éªŒè¯å’ŒLLMæ™ºèƒ½æ€»ç»“
"""

import json
import os
from datetime import datetime
from pathlib import Path
from crawler import XCrawler
from summarizer import TwitterSummarizer
from realtime_validator import RealtimeValidator

class DailyReportGenerator:
    def __init__(self, config_file: str = "config.json"):
        """
        åˆå§‹åŒ–æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self.load_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.crawler = XCrawler(config_file=config_file)
        self.summarizer = TwitterSummarizer()
        self.validator = RealtimeValidator()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("daily_reports")
        self.output_dir.mkdir(exist_ok=True)
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
            return {
                "targets": {
                    "daily_tweet_count": 100,
                    "timeline_types": ["recommended"]
                }
            }
    
    def generate_daily_report(self, tweet_count: int = None) -> dict:
        """
        ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
        
        Args:
            tweet_count: ç›®æ ‡æ¨æ–‡æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®
        
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        print("ğŸ“Š å¼€å§‹ç”Ÿæˆæ¯æ—¥æ¨æ–‡æŠ¥å‘Š")
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®æˆ–å‚æ•°è¦†ç›–
        target_count = tweet_count or self.config.get("targets", {}).get("daily_tweet_count", 100)
        
        # æ­¥éª¤1: æ•°æ®é‡‡é›†
        print(f"\nğŸ” æ­¥éª¤1: é‡‡é›† {target_count} æ¡æ¨æ–‡...")
        tweets = self.crawler.crawl_daily_posts(
            timeline_type="recommended", 
            max_pages=10,  # å¢åŠ é¡µæ•°ä»¥ç¡®ä¿èƒ½æŠ“åˆ°è¶³å¤Ÿæ•°é‡
            target_count=target_count
        )
        
        if not tweets:
            print("âŒ æ•°æ®é‡‡é›†å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return {"success": False, "error": "æ•°æ®é‡‡é›†å¤±è´¥"}
        
        print(f"âœ… æˆåŠŸé‡‡é›† {len(tweets)} æ¡æ¨æ–‡")
        
        # æ­¥éª¤2: æ•°æ®è´¨é‡éªŒè¯
        print(f"\nğŸ” æ­¥éª¤2: æ•°æ®è´¨é‡éªŒè¯...")
        validation_results = self.validate_data_quality(tweets)
        
        # æ­¥éª¤3: ç”Ÿæˆæ™ºèƒ½æ€»ç»“
        print(f"\nğŸ¤– æ­¥éª¤3: ç”Ÿæˆæ™ºèƒ½æ€»ç»“...")
        summary_result = self.summarizer.generate_summary(tweets, "daily")
        
        # æ­¥éª¤4: ç”Ÿæˆå¤šç§æ ¼å¼çš„æŠ¥å‘Š
        print(f"\nğŸ“„ æ­¥éª¤4: ç”Ÿæˆå®Œæ•´æŠ¥å‘Š...")
        report_data = self.create_comprehensive_report(
            tweets, validation_results, summary_result
        )
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.save_report(report_data)
        
        print(f"\nâœ… æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“ æŠ¥å‘Šä½ç½®: {report_file}")
        print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†: {validation_results.get('overall_score', 'N/A')}")
        print(f"ğŸ¤– æ™ºèƒ½æ€»ç»“: {len(summary_result.get('summary', ''))} å­—ç¬¦")
        
        return {
            "success": True,
            "report_file": report_file,
            "tweet_count": len(tweets),
            "data_quality_score": validation_results.get("overall_score"),
            "summary": summary_result
        }
    
    def validate_data_quality(self, tweets: list) -> dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        try:
            # ä½¿ç”¨å®æ—¶éªŒè¯å™¨è¿›è¡ŒéªŒè¯
            golden_dataset = "golden_dataset/golden_dataset_20250911_220234.json"
            if os.path.exists(golden_dataset):
                validation_results = self.validator.comprehensive_validation_realtime(tweets, golden_dataset)
                
                # è½¬æ¢ValidationResultå¯¹è±¡ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
                serializable_results = {}
                for key, result in validation_results.items():
                    serializable_results[key] = {
                        "is_valid": result.is_valid,
                        "score": result.score,
                        "issues": result.issues,
                        "details": result.details
                    }
                
                # è®¡ç®—æ€»åˆ† - æ’é™¤å‚è€ƒé¡¹ç›®
                core_categories = ['text_completeness', 'retweet_integrity', 'media_accessibility', 'data_structure']
                scores = [result.score for key, result in validation_results.items() if key in core_categories]
                overall_score = sum(scores) / len(scores) if scores else 0
                
                return {
                    "overall_score": overall_score,
                    "details": serializable_results,
                    "validation_passed": overall_score >= 80
                }
            else:
                print("âš ï¸ é»„é‡‘æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¹æ¯”éªŒè¯")
                return {"overall_score": "N/A", "validation_passed": True}
        except Exception as e:
            print(f"âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return {"overall_score": "ERROR", "validation_passed": False}
    
    def create_comprehensive_report(self, tweets: list, validation: dict, summary: dict) -> dict:
        """åˆ›å»ºç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now()
        
        # åŸºç¡€ç»Ÿè®¡
        stats = self.calculate_statistics(tweets)
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Šæ•°æ®
        report = {
            "metadata": {
                "generation_time": timestamp.isoformat(),
                "report_type": "daily_comprehensive",
                "data_source": "Xæ¨èæ—¶é—´çº¿",
                "tweet_count": len(tweets),
                "data_quality_score": validation.get("overall_score", "N/A")
            },
            "data_quality": validation,
            "statistics": stats,
            "ai_summary": summary,
            "raw_tweets": tweets[:10],  # åªä¿å­˜å‰10æ¡ä½œä¸ºæ ·ä¾‹
            "full_data_reference": f"å®Œæ•´æ•°æ®åŒ…å« {len(tweets)} æ¡æ¨æ–‡"
        }
        
        return report
    
    def calculate_statistics(self, tweets: list) -> dict:
        """è®¡ç®—æ¨æ–‡ç»Ÿè®¡æ•°æ®"""
        stats = {
            "total_tweets": len(tweets),
            "original_tweets": 0,
            "retweets": 0,
            "quoted_tweets": 0,
            "media_tweets": 0,
            "total_media_files": 0,
            "avg_text_length": 0,
            "top_users": {},
            "media_types": {"photo": 0, "video": 0, "animated_gif": 0}
        }
        
        text_lengths = []
        user_counts = {}
        
        for tweet in tweets:
            # åŸºç¡€åˆ†ç±»
            if tweet.get('retweet'):
                stats["retweets"] += 1
            elif tweet.get('quoted'):
                stats["quoted_tweets"] += 1
            else:
                stats["original_tweets"] += 1
            
            # åª’ä½“ç»Ÿè®¡
            media = tweet.get('media', [])
            if media:
                stats["media_tweets"] += 1
                stats["total_media_files"] += len(media)
                
                for media_item in media:
                    media_type = media_item.get('type', 'unknown')
                    if media_type in stats["media_types"]:
                        stats["media_types"][media_type] += 1
            
            # æ–‡æœ¬é•¿åº¦
            text = tweet.get('text', '')
            text_lengths.append(len(text))
            
            # ç”¨æˆ·ç»Ÿè®¡
            user = tweet.get('user', {})
            username = user.get('screen_name', 'unknown')
            user_counts[username] = user_counts.get(username, 0) + 1
        
        # è®¡ç®—å¹³å‡å€¼
        stats["avg_text_length"] = sum(text_lengths) / len(text_lengths) if text_lengths else 0
        
        # Top 5 æ´»è·ƒç”¨æˆ·
        stats["top_users"] = dict(
            sorted(user_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        )
        
        return stats
    
    def save_report(self, report_data: dict) -> str:
        """ä¿å­˜å®Œæ•´æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼è¯¦ç»†æŠ¥å‘Š
        json_file = self.output_dir / f"daily_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        # Markdownæ ¼å¼ç”¨æˆ·å‹å¥½æŠ¥å‘Š
        md_file = self.output_dir / f"daily_report_{timestamp}.md"
        markdown_content = self.generate_markdown_report(report_data)
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Š (JSON): {json_file}")
        print(f"ğŸ“ ç”¨æˆ·æŠ¥å‘Š (Markdown): {md_file}")
        
        return str(json_file)
    
    def generate_markdown_report(self, report_data: dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„ç”¨æˆ·å‹å¥½æŠ¥å‘Š"""
        metadata = report_data["metadata"]
        stats = report_data["statistics"]
        quality = report_data["data_quality"]
        summary = report_data["ai_summary"]
        
        md_content = f"""# Xæ¨æ–‡æ¯æ—¥æŠ¥å‘Š
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- **é‡‡é›†æ—¶é—´**: {metadata['generation_time'][:10]}
- **æ•°æ®æ¥æº**: {metadata['data_source']}
- **æ¨æ–‡æ€»æ•°**: {metadata['tweet_count']} æ¡
- **æ•°æ®è´¨é‡**: {quality.get('overall_score', 'N/A')} åˆ†

## ğŸ“ˆ å†…å®¹åˆ†æ
- **åŸåˆ›æ¨æ–‡**: {stats['original_tweets']} æ¡ ({stats['original_tweets']/stats['total_tweets']*100:.1f}%)
- **è½¬æ¨**: {stats['retweets']} æ¡ ({stats['retweets']/stats['total_tweets']*100:.1f}%)
- **å¼•ç”¨æ¨æ–‡**: {stats['quoted_tweets']} æ¡ ({stats['quoted_tweets']/stats['total_tweets']*100:.1f}%)
- **å«åª’ä½“æ¨æ–‡**: {stats['media_tweets']} æ¡
- **å¹³å‡æ–‡æœ¬é•¿åº¦**: {stats['avg_text_length']:.0f} å­—ç¬¦

## ğŸ¯ æ´»è·ƒç”¨æˆ· (Top 5)
"""
        
        for username, count in stats['top_users'].items():
            md_content += f"- @{username}: {count} æ¡æ¨æ–‡\n"
        
        md_content += f"""
## ğŸ–¼ï¸ åª’ä½“å†…å®¹
- **å›¾ç‰‡**: {stats['media_types']['photo']} ä¸ª
- **è§†é¢‘**: {stats['media_types']['video']} ä¸ª  
- **åŠ¨å›¾**: {stats['media_types']['animated_gif']} ä¸ª

## ğŸ¤– AIæ™ºèƒ½æ€»ç»“
{summary.get('summary', 'æ€»ç»“ç”Ÿæˆä¸­...')}

## ğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š
"""
        if quality.get('overall_score') != 'N/A':
            md_content += f"- **æ€»ä½“è¯„åˆ†**: {quality['overall_score']:.1f}/100\n"
            md_content += f"- **éªŒè¯çŠ¶æ€**: {'âœ… é€šè¿‡' if quality.get('validation_passed') else 'âŒ æœªé€šè¿‡'}\n"
        else:
            md_content += "- æ•°æ®è´¨é‡éªŒè¯æœªæ‰§è¡Œ\n"
        
        md_content += f"""
---
*æœ¬æŠ¥å‘Šç”±Xçˆ¬è™«ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«æ•°æ®é‡‡é›†ã€è´¨é‡éªŒè¯å’ŒAIæ™ºèƒ½åˆ†æ*  
*æŠ€æœ¯æ ˆ: Python + Playwright + LLM + æ•°æ®éªŒè¯ç³»ç»Ÿ*
"""
        
        return md_content

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ¯æ—¥æŠ¥å‘Šç”Ÿæˆ"""
    print("ğŸš€ Xæ¨æ–‡æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå™¨")
    
    # æ£€æŸ¥é…ç½®
    if not os.path.exists("config.json"):
        print("âš ï¸ æœªæ‰¾åˆ°config.jsoné…ç½®æ–‡ä»¶")
        print("è¯·å¤åˆ¶ config_template.json ä¸º config.json å¹¶é…ç½®è®¤è¯ä¿¡æ¯")
        print("\nğŸ“‹ å½“å‰å°†ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæ¼”ç¤º...")
        
        # ä½¿ç”¨æµ‹è¯•æ•°æ®æ¼”ç¤ºåŠŸèƒ½
        generator = DailyReportGenerator()
        
        # æ¨¡æ‹ŸåŠ è½½çœŸå®çˆ¬å–çš„æ•°æ®è¿›è¡Œæ¼”ç¤º
        test_file = "analysis_data/api_responses/response_213433_687388.json"
        if os.path.exists(test_file):
            print(f"ğŸ“ ä½¿ç”¨æµ‹è¯•æ•°æ®: {test_file}")
            
            # ç›´æ¥ä½¿ç”¨çˆ¬è™«è§£ææµ‹è¯•æ•°æ®
            with open(test_file, 'r', encoding='utf-8') as f:
                api_response = json.load(f)
            
            # æå–æ¨æ–‡æ•°æ®
            api_data = api_response.get('data', {})
            tweets = generator.crawler.extract_tweets_from_response(api_data)
            
            if tweets:
                print(f"âœ… è§£æå¾—åˆ° {len(tweets)} æ¡æµ‹è¯•æ¨æ–‡")
                
                # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
                result = generator.create_comprehensive_report(
                    tweets,
                    generator.validate_data_quality(tweets),
                    generator.summarizer.generate_summary(tweets, "daily")
                )
                
                report_file = generator.save_report(result)
                print(f"\nğŸ‰ æ¼”ç¤ºæŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
                print(f"ğŸ“„ æŸ¥çœ‹æŠ¥å‘Š: {report_file}")
            else:
                print("âŒ æµ‹è¯•æ•°æ®è§£æå¤±è´¥")
        else:
            print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
    else:
        # æ­£å¸¸æµç¨‹
        generator = DailyReportGenerator()
        result = generator.generate_daily_report()
        
        if result["success"]:
            print("\nğŸ‰ æ¯æ—¥æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
        else:
            print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

if __name__ == "__main__":
    main()