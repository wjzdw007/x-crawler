#!/usr/bin/env python3
"""
Xçˆ¬è™«å‘½ä»¤è¡Œå·¥å…· - æ”¯æŒçµæ´»å‚æ•°é…ç½®
"""

import argparse
import json
from daily_report_generator import DailyReportGenerator

def main():
    parser = argparse.ArgumentParser(description='Xæ¨æ–‡çˆ¬è™« - ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š')
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        '-c', '--count',
        type=int,
        default=None,
        help='æŒ‡å®šè¦æŠ“å–çš„æ¨æ–‡æ•°é‡ (é»˜è®¤ä½¿ç”¨config.jsonä¸­çš„è®¾ç½®)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config.json',
        help='æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)'
    )
    
    parser.add_argument(
        '--timeline',
        type=str,
        default='recommended',
        choices=['recommended', 'following'],
        help='æ—¶é—´çº¿ç±»å‹ (é»˜è®¤: recommended)'
    )
    
    parser.add_argument(
        '--max-pages',
        type=int,
        default=10,
        help='æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: 10)'
    )
    
    parser.add_argument(
        '--test',
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼ - åªéªŒè¯æ•°æ®è´¨é‡ï¼Œä¸ç”Ÿæˆå®Œæ•´æŠ¥å‘Š'
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ Xæ¨æ–‡çˆ¬è™«ç³»ç»Ÿ")
    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {args.config}")
    if args.count:
        print(f"ğŸ¯ ç›®æ ‡æ•°é‡: {args.count} æ¡")
    print(f"ğŸ“ æ—¶é—´çº¿ç±»å‹: {args.timeline}")
    
    try:
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
        generator = DailyReportGenerator(args.config)
        
        if args.test:
            # æµ‹è¯•æ¨¡å¼
            print("\nğŸ§ª æµ‹è¯•æ¨¡å¼ - éªŒè¯æ•°æ®è´¨é‡")
            from test_crawler_with_validation import test_crawler_with_validation
            test_crawler_with_validation()
        else:
            # æ­£å¸¸æ¨¡å¼
            result = generator.generate_daily_report(tweet_count=args.count)
            
            if result["success"]:
                print(f"\nğŸ‰ æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
                print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {result['report_file']}")
                print(f"ğŸ“Š æ¨æ–‡æ•°é‡: {result['tweet_count']} æ¡")
                print(f"ğŸ† æ•°æ®è´¨é‡: {result['data_quality_score']:.1f}/100")
            else:
                print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
    except FileNotFoundError:
        print(f"\nâŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œè®¤è¯é…ç½®:")
        print("   python working_auth.py")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. éªŒè¯è®¤è¯ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ")
        print("   3. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—ä¿¡æ¯")

def show_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("""
ğŸ¯ ä½¿ç”¨ç¤ºä¾‹:

# ä½¿ç”¨é»˜è®¤é…ç½®
python run_crawler.py

# æŒ‡å®šæŠ“å–200æ¡æ¨æ–‡
python run_crawler.py --count 200

# æŠ“å–50æ¡followingæ—¶é—´çº¿æ¨æ–‡
python run_crawler.py --count 50 --timeline following

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python run_crawler.py --config my_config.json --count 150

# æµ‹è¯•æ¨¡å¼ - åªéªŒè¯æ•°æ®è´¨é‡
python run_crawler.py --test

# é«˜çº§é…ç½® - é™åˆ¶æœ€å¤§é¡µæ•°
python run_crawler.py --count 100 --max-pages 3
""")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 2 and sys.argv[1] in ['--help', '-h', 'help']:
        show_examples()
    else:
        main()