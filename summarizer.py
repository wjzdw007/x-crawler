#!/usr/bin/env python3
"""
LLMæ€»ç»“æ¨¡å— - åŸºäºæ¨æ–‡æ•°æ®ç”Ÿæˆæ™ºèƒ½æ€»ç»“
æ”¯æŒå¤šç§æ€»ç»“æ–¹å¼å’Œè¾“å‡ºæ ¼å¼
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
import hashlib

class TwitterSummarizer:
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        åˆå§‹åŒ–æ€»ç»“å™¨
        
        Args:
            api_key: LLM APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            model: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        """
        # å°è¯•ä»å¤šä¸ªç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.api_key = (
            api_key or 
            os.getenv('OPENROUTER_API_KEY') or 
            os.getenv('OPENAI_API_KEY') or
            os.getenv('LLM_API_KEY')
        )
        
        # æŒ‡å®šçš„æ¨¡å‹ï¼šå‚æ•° > ç¯å¢ƒå˜é‡ > None
        self.custom_model = model or os.getenv('OPENAI_MODEL')
        # ä¸å†åˆ›å»ºç‹¬ç«‹çš„summariesç›®å½•ï¼Œä½¿ç”¨crawler_data/user_summaries
        
        # æ€»ç»“é…ç½®
        self.config = {
            "max_tweets_per_summary": 100,
            "summary_types": ["daily", "trending", "category"],
            "languages": ["zh", "en"],
            "output_formats": ["markdown", "json", "html"]
        }
        
        # ç”¨æˆ·åˆ†æé…ç½® - æ ¹æ®ä¸åŒç”¨æˆ·çš„ç‰¹ç‚¹å®šåˆ¶åˆ†æè§’åº¦
        self.user_analysis_profiles = {
            "elonmusk": {
                "type": "entrepreneur_investor",
                "focus": "å•†ä¸šæŠ•èµ„æœºä¼š",
                "keywords": ["æŠ•èµ„", "åˆ›ä¸š", "ç§‘æŠ€è¶‹åŠ¿", "å¸‚åœºåŠ¨å‘", "æ”¿ç­–å½±å“"],
                "analysis_angles": [
                    "ğŸš€ **å•†ä¸šæœºä¼šè¯†åˆ«**ï¼šä»æ¨æ–‡ä¸­è¯†åˆ«æ½œåœ¨çš„æŠ•èµ„æ–¹å‘å’Œå•†ä¸šè¶‹åŠ¿",
                    "ğŸ’¡ **åˆ›æ–°æŠ€æœ¯æ´å¯Ÿ**ï¼šå…³æ³¨æåˆ°çš„æ–°æŠ€æœ¯ã€äº§å“æˆ–è§£å†³æ–¹æ¡ˆ",
                    "ğŸ“ˆ **å¸‚åœºä¿¡å·è§£è¯»**ï¼šåˆ†æå¯¹ç‰¹å®šè¡Œä¸šã€å…¬å¸æˆ–æ”¿ç­–çš„æ€åº¦å˜åŒ–",
                    "ğŸ¯ **æˆ˜ç•¥æ€ç»´åˆ†æ**ï¼šç†è§£å…¶å†³ç­–é€»è¾‘å’Œé•¿æœŸå¸ƒå±€æ€è·¯"
                ]
            },
            "dotey": {
                "type": "tech_educator", 
                "focus": "AIæŠ€æœ¯å­¦ä¹ ",
                "keywords": ["AI", "ç¼–ç¨‹", "æŠ€æœ¯åˆ†äº«", "å·¥å…·æ¨è", "å­¦ä¹ èµ„æº"],
                "analysis_angles": [
                    "ğŸ¤– **AIæŠ€æœ¯åŠ¨æ€**ï¼šæ•´ç†æœ€æ–°çš„AIå·¥å…·ã€æ¨¡å‹å’ŒæŠ€æœ¯å‘å±•",
                    "ğŸ“š **å­¦ä¹ èµ„æºæ•´ç†**ï¼šè¯†åˆ«å€¼å¾—æ·±å…¥å­¦ä¹ çš„æŠ€æœ¯å†…å®¹å’Œèµ„æº",
                    "ğŸ› ï¸ **å®ç”¨å·¥å…·æ¨è**ï¼šæå–æ¨èçš„å¼€å‘å·¥å…·ã€æ¡†æ¶å’Œæœ€ä½³å®è·µ",
                    "ğŸ’­ **æŠ€æœ¯è§è§£æç‚¼**ï¼šæ€»ç»“å¯¹æŠ€æœ¯è¶‹åŠ¿å’Œå‘å±•æ–¹å‘çš„ç‹¬ç‰¹è§‚ç‚¹"
                ]
            },
            "default": {
                "type": "general_content",
                "focus": "å†…å®¹æ´å¯Ÿ",
                "keywords": ["çƒ­ç‚¹", "è¶‹åŠ¿", "è§‚ç‚¹", "åˆ†äº«"],
                "analysis_angles": [
                    "ğŸ“Š **å†…å®¹ä¸»é¢˜åˆ†æ**ï¼šè¯†åˆ«ä¸»è¦è®¨è®ºçš„è¯é¢˜å’Œå…³æ³¨ç‚¹",
                    "ğŸ”¥ **çƒ­ç‚¹äº‹ä»¶è·Ÿè¸ª**ï¼šæ€»ç»“æ¶‰åŠçš„é‡è¦äº‹ä»¶å’Œç¤¾ä¼šè®®é¢˜", 
                    "ğŸ’¬ **è§‚ç‚¹ç«‹åœºæ¢³ç†**ï¼šåˆ†æè¡¨è¾¾çš„æ ¸å¿ƒè§‚ç‚¹å’Œä»·å€¼å–å‘",
                    "ğŸ“ˆ **å½±å“åŠ›è¯„ä¼°**ï¼šè¯„ä¼°å†…å®¹çš„ä¼ æ’­ä»·å€¼å’Œç¤¾ä¼šå½±å“"
                ]
            }
        }
        
        # å¯åŠ¨æ—¶åŠ è½½ä¿å­˜çš„ç”¨æˆ·é…ç½®
        self.load_user_profiles()
        
        # åŠ è½½ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
        self.load_user_prompt_templates()
        
        # LLMé…ç½®
        self.llm_config = {
            "default_model": "openai/gpt-4o",
            "fallback_models": [
                "openai/gpt-4o-mini", 
                "anthropic/claude-3-haiku",
                "meta-llama/llama-3.1-8b-instruct"
            ],
            "max_tokens": 100000,
            "temperature": 0.7
        }
        
        # æ£€æŸ¥APIçŠ¶æ€
        self.check_api_status()
    
    def check_api_status(self):
        """æ£€æŸ¥APIå¯†é’¥å’Œä¾èµ–åº“çŠ¶æ€"""
        print(f"\nğŸ”§ LLM API çŠ¶æ€æ£€æŸ¥:")
        
        # æ£€æŸ¥APIå¯†é’¥
        if self.api_key:
            masked_key = f"{self.api_key[:8]}...{self.api_key[-4:]}" if len(self.api_key) > 12 else "***"
            print(f"  âœ… APIå¯†é’¥: {masked_key}")
        else:
            print(f"  âš ï¸ APIå¯†é’¥: æœªè®¾ç½® (å°†ä½¿ç”¨æ¨¡æ‹Ÿæ€»ç»“)")
            print(f"     ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENROUTER_API_KEY")
        
        # æ£€æŸ¥ä¾èµ–åº“
        try:
            from openai import OpenAI
            print(f"  âœ… OpenAIåº“: å·²å®‰è£…")
        except ImportError:
            print(f"  âŒ OpenAIåº“: æœªå®‰è£…")
            print(f"     ğŸ’¡ è¯·å®‰è£…: pip install openai")
            
        effective_model = self.custom_model or self.llm_config['default_model']
        print(f"  ğŸ¯ ä½¿ç”¨æ¨¡å‹: {effective_model}")
        if self.custom_model:
            print(f"     (è‡ªå®šä¹‰æŒ‡å®š)")
        print(f"  ğŸ”„ å¤‡é€‰æ¨¡å‹: {len(self.llm_config['fallback_models'])} ä¸ª")
    
    def add_user_profile(self, username: str, user_type: str, focus: str, keywords: List[str], analysis_angles: List[str]):
        """æ·»åŠ æ–°ç”¨æˆ·çš„åˆ†æé…ç½®"""
        self.user_analysis_profiles[username] = {
            "type": user_type,
            "focus": focus,
            "keywords": keywords,
            "analysis_angles": analysis_angles
        }
        print(f"âœ… å·²æ·»åŠ ç”¨æˆ· @{username} çš„åˆ†æé…ç½®ï¼š{focus}")
        
        # è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        self.save_user_profiles()
    
    def save_user_profiles(self):
        """ä¿å­˜ç”¨æˆ·åˆ†æé…ç½®åˆ°æ–‡ä»¶"""
        config_file = Path("user_analysis_profiles.json")
        try:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(self.user_analysis_profiles, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç”¨æˆ·åˆ†æé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
        except Exception as e:
            error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
            print(f"âš ï¸ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {error_msg}")
    
    def load_user_profiles(self):
        """ä»æ–‡ä»¶åŠ è½½ç”¨æˆ·åˆ†æé…ç½®"""
        config_file = Path("user_analysis_profiles.json")
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    saved_profiles = json.load(f)
                    # åˆå¹¶ä¿å­˜çš„é…ç½®å’Œé»˜è®¤é…ç½®
                    self.user_analysis_profiles.update(saved_profiles)
                print(f"ğŸ“‚ å·²ä»æ–‡ä»¶åŠ è½½ç”¨æˆ·åˆ†æé…ç½®: {len(saved_profiles)} ä¸ª")
            except Exception as e:
                error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
                print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {error_msg}")
    
    def load_user_prompt_templates(self):
        """åŠ è½½ç”¨æˆ·æç¤ºè¯æ¨¡æ¿é…ç½®"""
        template_file = Path("user_prompt_templates.json")
        if template_file.exists():
            try:
                with open(template_file, 'r', encoding='utf-8') as f:
                    self.user_prompt_templates = json.load(f)
                print(f"ğŸ“ å·²åŠ è½½ç”¨æˆ·æç¤ºè¯æ¨¡æ¿: {len(self.user_prompt_templates)} ä¸ª")
            except Exception as e:
                error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
                print(f"âš ï¸ åŠ è½½ç”¨æˆ·æç¤ºè¯æ¨¡æ¿å¤±è´¥: {error_msg}")
                self.user_prompt_templates = {}
        else:
            print("ğŸ“ ç”¨æˆ·æç¤ºè¯æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
            self.user_prompt_templates = {}

    def get_user_template(self, username: str) -> str:
        """è·å–ç”¨æˆ·çš„æç¤ºè¯æ¨¡æ¿"""
        if hasattr(self, 'user_prompt_templates') and username in self.user_prompt_templates:
            return self.user_prompt_templates[username]['template']
        elif hasattr(self, 'user_prompt_templates') and 'default' in self.user_prompt_templates:
            return self.user_prompt_templates['default']['template']
        else:
            # åå¤‡é»˜è®¤æ¨¡æ¿
            return "è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·çš„æ¨æ–‡ï¼š\n\nç”¨æˆ·ä¿¡æ¯ï¼š{user_info}\n\næ¨æ–‡å†…å®¹ï¼š\n{tweet_content}\n\nè¯·ç”¨ä¸­æ–‡æ€»ç»“è¦ç‚¹ã€‚"
    
    def set_model(self, model: str):
        """åŠ¨æ€è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹"""
        self.custom_model = model
        print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model}")
    
    def list_templates(self):
        """åˆ—å‡ºå¯ç”¨çš„æç¤ºè¯æ¨¡æ¿"""
        if hasattr(self, 'prompt_templates'):
            print("ğŸ“ å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿:")
            for template_name, config in self.prompt_templates.items():
                print(f"  â€¢ {template_name}: {config.get('description', 'No description')}")
                print(f"    æœ€å¤§token: {config.get('max_tokens', 'N/A')}")
        else:
            print("âš ï¸ æœªåŠ è½½æç¤ºè¯æ¨¡æ¿")
    
    def set_custom_template(self, template_name: str, template_content: str, description: str = "", max_tokens: int = 8000):
        """è®¾ç½®è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿"""
        if not hasattr(self, 'prompt_templates'):
            self.prompt_templates = {}
        
        self.prompt_templates[template_name] = {
            "template": template_content,
            "max_tokens": max_tokens,
            "description": description
        }
        print(f"âœ… å·²æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿: {template_name}")
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        try:
            with open("prompt_templates.json", 'w', encoding='utf-8') as f:
                json.dump(self.prompt_templates, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ æ¨¡æ¿å·²ä¿å­˜åˆ° prompt_templates.json")
        except Exception as e:
            error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
            print(f"âš ï¸ ä¿å­˜æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {error_msg}")
    
    def optimize_tweet_structure(self, tweet: Dict) -> Dict:
        """å°†åŸå§‹æ¨æ–‡æ•°æ®è½¬æ¢ä¸ºä¼˜åŒ–çš„åµŒå¥—ç»“æ„"""
        def extract_core_info(tweet_data, is_main_tweet=False):
            """æå–æ¨æ–‡æ ¸å¿ƒä¿¡æ¯"""
            if not tweet_data:
                return None
                
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = tweet_data.get('user', {})
            
            # å¦‚æœæ˜¯ä¸»æ¨æ–‡ä¸”æ²¡æœ‰userå­—æ®µï¼Œä½¿ç”¨å½“å‰ç”¨æˆ·ä¿¡æ¯
            if is_main_tweet and (not user_info or not user_info.get('screen_name')):
                if hasattr(self, '_current_user') and self._current_user:
                    user_info = self._current_user
            
            # è·å–ç”¨æˆ·åï¼Œä¼˜å…ˆä»user_infoè·å–
            screen_name = user_info.get('screen_name', 'unknown')
            
            return {
                "id": tweet_data.get('id'),
                "author": f"@{screen_name}",
                "text": tweet_data.get('text', '').strip(),
                "timestamp": tweet_data.get('created_at', ''),
                "media": [m.get('type') for m in tweet_data.get('media', [])]
            }
        
        # æ„å»ºä¸»æ¨æ–‡ç»“æ„ - è¿™æ˜¯ä¸»æ¨æ–‡ï¼Œæ ‡è®°ä¸ºis_main_tweet=True
        optimized = extract_core_info(tweet, is_main_tweet=True)
        if not optimized:
            return None
            
        # å¤„ç†è½¬æ¨
        if tweet.get('retweet'):
            optimized["type"] = "retweet"
            optimized["original_content"] = extract_core_info(tweet['retweet'], is_main_tweet=False)
            
            # å¤„ç†è½¬æ¨ä¸­çš„å¼•ç”¨
            if tweet['retweet'].get('quoted'):
                optimized["original_content"]["type"] = "quote_tweet"
                optimized["original_content"]["quoted_content"] = extract_core_info(tweet['retweet']['quoted'], is_main_tweet=False)
                optimized["original_content"]["quoted_content"]["type"] = "original"
            else:
                optimized["original_content"]["type"] = "original"
                
        # å¤„ç†ç›´æ¥å¼•ç”¨
        elif tweet.get('quoted'):
            optimized["type"] = "quote_tweet"
            optimized["quoted_content"] = extract_core_info(tweet['quoted'], is_main_tweet=False)
            optimized["quoted_content"]["type"] = "original"
        else:
            optimized["type"] = "original"
            
        return optimized
    
    def save_optimized_structure(self, tweets: List[Dict], filename: str = None, user_info: Dict = None):
        """ä¿å­˜ä¼˜åŒ–åçš„æ¨æ–‡ç»“æ„ç”¨äºæŸ¥çœ‹"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"optimized_tweets_{timestamp}.json"
        
        # è®¾ç½®å½“å‰ç”¨æˆ·ä¿¡æ¯ä¾›å†…éƒ¨å‡½æ•°ä½¿ç”¨
        if user_info:
            self._current_user = user_info
        
        # è½¬æ¢æ‰€æœ‰æ¨æ–‡ä¸ºä¼˜åŒ–ç»“æ„
        optimized_tweets = []
        for tweet in tweets[:10]:  # åªå¤„ç†å‰10æ¡ç”¨äºæŸ¥çœ‹
            optimized = self.optimize_tweet_structure(tweet)
            if optimized:
                optimized_tweets.append(optimized)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        filepath = Path("analysis_data") / filename
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                "total_tweets": len(optimized_tweets),
                "sample_tweets": optimized_tweets,
                "structure_info": {
                    "original_tweets": len([t for t in optimized_tweets if t.get('type') == 'original']),
                    "retweets": len([t for t in optimized_tweets if t.get('type') == 'retweet']),
                    "quote_tweets": len([t for t in optimized_tweets if t.get('type') == 'quote_tweet']),
                    "complex_retweets": len([t for t in optimized_tweets if t.get('type') == 'retweet' and t.get('original_content', {}).get('type') == 'quote_tweet'])
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¼˜åŒ–ç»“æ„å·²ä¿å­˜: {filepath}")
        return str(filepath)
    
    def prepare_tweet_data(self, tweets: List[Dict], user_info: Dict = None) -> str:
        """å‡†å¤‡æ¨æ–‡æ•°æ®ç”¨äºLLMæ€»ç»“ - ä½¿ç”¨ä¼˜åŒ–çš„åµŒå¥—ç»“æ„"""
        if not tweets:
            return "æ— æ¨æ–‡æ•°æ®"
        
        # è®¾ç½®ç”¨æˆ·ä¿¡æ¯ä¾›ä¼˜åŒ–ç»“æ„ä½¿ç”¨
        if user_info:
            self._current_user = user_info
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        
        # è½¬æ¢ä¸ºä¼˜åŒ–çš„åµŒå¥—ç»“æ„
        optimized_tweets = []
        for tweet in sorted_tweets[:self.config["max_tweets_per_summary"]]:
            optimized = self.optimize_tweet_structure(tweet)
            if optimized:
                optimized_tweets.append(optimized)
        
        # æ„å»ºJSONæ ¼å¼çš„ç»“æ„åŒ–æ–‡æœ¬ç”¨äºLLMåˆ†æ
        tweet_data_json = {
            "total_tweets": len(optimized_tweets),
            "sample_tweets": optimized_tweets,
            "structure_info": {
                "original_tweets": len([t for t in optimized_tweets if t.get('type') == 'original']),
                "retweets": len([t for t in optimized_tweets if t.get('type') == 'retweet']),
                "quote_tweets": len([t for t in optimized_tweets if t.get('type') == 'quote_tweet']),
                "complex_retweets": len([t for t in optimized_tweets if t.get('type') == 'retweet' and t.get('original_content', {}).get('type') == 'quote_tweet'])
            }
        }
        
        # æ„å»ºç»™LLMçš„æ–‡æœ¬æè¿°
        prepared_text = "æ¨æ–‡æ•°æ®åˆ†æ - ä¼˜åŒ–åµŒå¥—ç»“æ„ï¼š\n\n"
        prepared_text += "ä»¥ä¸‹æ˜¯ç»è¿‡ç»“æ„ä¼˜åŒ–çš„æ¨æ–‡æ•°æ®ï¼Œé‡‡ç”¨åµŒå¥—JSONæ ¼å¼ä¾¿äºç†è§£æ¨æ–‡é—´çš„å¤æ‚å…³ç³»ï¼š\n\n"
        prepared_text += "```json\n"
        prepared_text += json.dumps(tweet_data_json, ensure_ascii=False, indent=2)
        prepared_text += "\n```\n\n"
        
        # æ·»åŠ ç»“æ„è¯´æ˜
        prepared_text += "æ•°æ®ç»“æ„è¯´æ˜ï¼š\n"
        prepared_text += "- type: æ¨æ–‡ç±»å‹ (original/retweet/quote_tweet)\n"
        prepared_text += "- original_content: è½¬æ¨çš„åŸå§‹å†…å®¹\n"
        prepared_text += "- quoted_content: å¼•ç”¨çš„æ¨æ–‡å†…å®¹\n"
        prepared_text += "- media: åª’ä½“ç±»å‹åˆ—è¡¨ (photo/videoç­‰)\n"
        prepared_text += "- ä¸ºèŠ‚çœç©ºé—´ï¼Œå·²çœç•¥è¯¦ç»†çš„äº’åŠ¨æ•°æ®\n\n"
        
        # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
        total_tweets = len(tweets)
        original_tweets = len([t for t in tweets if not t.get('retweet')])
        retweets = total_tweets - original_tweets
        media_tweets = len([t for t in tweets if t.get('media')])
        
        prepared_text += f"å®Œæ•´æ•°æ®ç»Ÿè®¡ï¼š\n"
        prepared_text += f"æ€»æ¨æ–‡æ•°ï¼š{total_tweets}\n"
        prepared_text += f"åŸåˆ›æ¨æ–‡ï¼š{original_tweets}\n"
        prepared_text += f"è½¬æ¨ï¼š{retweets}\n"
        prepared_text += f"å«åª’ä½“ï¼š{media_tweets}\n"
        
        return prepared_text
    
    def prepare_simple_tweet_data(self, tweets: List[Dict], user_info: Dict = None) -> str:
        """å‡†å¤‡ç®€åŒ–çš„æ¨æ–‡æ•°æ®ç”¨äºç®€æ´æ¨¡æ¿"""
        if not tweets:
            return "æ— æ¨æ–‡æ•°æ®"
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€å¤š20æ¡
        sorted_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        limited_tweets = sorted_tweets[:20]
        
        # è·å–é»˜è®¤ç”¨æˆ·å 
        default_username = user_info.get('screen_name', 'unknown') if user_info else 'unknown'
        
        # ç®€åŒ–æ ¼å¼
        simple_tweets = []
        for i, tweet in enumerate(limited_tweets, 1):
            # æå–æ ¸å¿ƒä¿¡æ¯
            text = tweet.get('text', '') or tweet.get('full_text', '')
            
            # ä¼˜å…ˆä½¿ç”¨æ¨æ–‡ä¸­çš„ç”¨æˆ·ä¿¡æ¯ï¼Œå¦åˆ™ä½¿ç”¨ä¼ å…¥çš„user_info
            author = tweet.get('user', {}).get('screen_name') if tweet.get('user') else default_username
            created_at = tweet.get('created_at', '')
            
            # å¤„ç†è½¬æ¨
            if tweet.get('retweeted_status'):
                rt_text = tweet['retweeted_status'].get('text', '') or tweet['retweeted_status'].get('full_text', '')
                rt_author = tweet['retweeted_status'].get('user', {}).get('screen_name', 'unknown')
                text = f"RT @{rt_author}: {rt_text}"
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            if len(text) > 200:
                text = text[:197] + "..."
                
            simple_tweets.append(f"{i}. @{author} ({created_at})\n{text}")
        
        return "\n\n".join(simple_tweets)

    
    def generate_simple_prompt(self, tweets: List[Dict], user_info: Dict = None) -> str:
        """ç”Ÿæˆç®€æ´çš„ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯"""

        # è·å–ç”¨æˆ·ä¿¡æ¯
        user_name = user_info.get('screen_name', '') if user_info else 'unknown'

        # å‡†å¤‡ç”¨æˆ·ä¿¡æ¯å­—ç¬¦ä¸²ï¼ˆå»é™¤é‡å¤çš„ç”¨æˆ·æ•°æ®ï¼‰
        user_info_str = f"ç”¨æˆ·å: @{user_name}"
        if user_info:
            if user_info.get('name'):
                user_info_str += f"\næ˜¾ç¤ºå: {user_info['name']}"
            if user_info.get('description'):
                user_info_str += f"\nç®€ä»‹: {user_info['description']}"
            if user_info.get('followers_count'):
                user_info_str += f"\nå…³æ³¨è€…: {user_info['followers_count']}"

        # æ¸…ç†æ¨æ–‡æ•°æ®ä¸­çš„é‡å¤ç”¨æˆ·ä¿¡æ¯
        cleaned_tweets = []
        for tweet in tweets:
            cleaned_tweet = tweet.copy()
            # åˆ é™¤æ¨æ–‡ä¸­çš„ç”¨æˆ·ä¿¡æ¯ï¼Œé¿å…é‡å¤
            if 'user' in cleaned_tweet:
                del cleaned_tweet['user']
            # å¤„ç†è½¬æ¨ä¸­çš„ç”¨æˆ·ä¿¡æ¯é‡å¤
            if cleaned_tweet.get('retweeted_status') and 'user' in cleaned_tweet['retweeted_status']:
                cleaned_tweet['retweeted_status'] = cleaned_tweet['retweeted_status'].copy()
                # ä¿ç•™è½¬æ¨åŸä½œè€…ä¿¡æ¯ï¼Œå› ä¸ºè¿™æ˜¯å¿…è¦çš„
                pass
            # å¤„ç†å¼•ç”¨æ¨æ–‡ä¸­çš„ç”¨æˆ·ä¿¡æ¯
            if cleaned_tweet.get('quoted_status') and 'user' in cleaned_tweet['quoted_status']:
                cleaned_tweet['quoted_status'] = cleaned_tweet['quoted_status'].copy()
                # ä¿ç•™å¼•ç”¨æ¨æ–‡ä½œè€…ä¿¡æ¯ï¼Œå› ä¸ºè¿™æ˜¯å¿…è¦çš„
                pass
            cleaned_tweets.append(cleaned_tweet)

        # ä½¿ç”¨åŸå§‹æ¨æ–‡æ•°æ®çš„JSONæ ¼å¼
        tweet_content = json.dumps(cleaned_tweets, ensure_ascii=False, indent=2)

        # è·å–ç”¨æˆ·çš„æ¨¡æ¿
        template = self.get_user_template(user_name)

        # åªä½¿ç”¨ä¸¤ä¸ªåŸºæœ¬å˜é‡å¡«å……æ¨¡æ¿
        prompt = template.format(
            user_info=user_info_str,
            tweet_content=tweet_content
        )

        return prompt
    
    def save_prompt_to_file(self, prompt: str, summary_type: str, tweets: List[Dict], user_info: Dict = None) -> str:
        """ä¿å­˜å®Œæ•´çš„LLM promptåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç¡®å®šç”¨æˆ·åæ ‡è¯†
        if user_info and user_info.get('screen_name'):
            user_name = user_info.get('screen_name')
        elif summary_type.endswith('_mixed'):
            user_name = 'mixed_users'  # æ··åˆç”¨æˆ·æ•°æ®
        else:
            user_name = 'unknown'
        
        # åˆ›å»ºpromptsç›®å½•ï¼ˆåœ¨crawler_dataä¸‹ï¼‰
        prompts_dir = Path("crawler_data/prompts")
        prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{summary_type}_{user_name}_{timestamp}_prompt.txt"
        filepath = prompts_dir / filename
        
        # æ„å»ºå®Œæ•´çš„promptä¿¡æ¯
        user_display = f"@{user_name}" if user_name != 'mixed_users' else "æ··åˆç”¨æˆ·æ•°æ®"
        full_prompt_content = f"""# LLM Prompt ä¿å­˜è®°å½•
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ€»ç»“ç±»å‹: {summary_type}
ç”¨æˆ·: {user_display}
æ¨æ–‡æ•°é‡: {len(tweets)}
æ•°æ®å“ˆå¸Œ: {hashlib.md5(str(tweets).encode()).hexdigest()[:8]}

{'='*80}
å®Œæ•´Promptå†…å®¹:
{'='*80}

{prompt}

{'='*80}
Promptç»“æŸ
{'='*80}

# ä½¿ç”¨è¯´æ˜
è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†å‘é€ç»™LLMçš„å®Œæ•´promptå†…å®¹ï¼Œä½ å¯ä»¥ï¼š
1. ç›´æ¥å¤åˆ¶åˆ°å…¶ä»–LLMæœåŠ¡ï¼ˆClaudeã€ChatGPTç­‰ï¼‰
2. è°ƒè¯•å’Œä¼˜åŒ–promptç»“æ„
3. ä½œä¸ºè®­ç»ƒæ•°æ®æˆ–ç¤ºä¾‹ä½¿ç”¨
4. åˆ†æLLMè¾“å…¥è¾“å‡ºçš„å¯¹åº”å…³ç³»

# æ•°æ®æ¥æºä¿¡æ¯
- åŸå§‹æ¨æ–‡æ•°æ®æ¥æº: {user_display}çš„æ¨æ–‡æ—¶é—´çº¿
- æ•°æ®å¤„ç†: ç»è¿‡ä¼˜åŒ–åµŒå¥—ç»“æ„è½¬æ¢ï¼Œä¾¿äºLLMç†è§£
- ç»“æ„ç‰¹ç‚¹: æ”¯æŒå¤æ‚çš„è½¬æ¨ã€å¼•ç”¨ã€å¤šå±‚åµŒå¥—å…³ç³»åˆ†æ
"""
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(full_prompt_content)
        
        print(f"ğŸ“ å®Œæ•´promptå·²ä¿å­˜: {filepath}")
        return str(filepath)
    
    def call_llm_api(self, prompt: str, model: str = None) -> str:
        """è°ƒç”¨LLM APIç”Ÿæˆæ€»ç»“ - é€šè¿‡OpenRouterè®¿é—®"""
        
        if not self.api_key:
            print("âš ï¸ æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ€»ç»“")
            return self.generate_mock_summary()
        
        # ç¡®ä¿promptæ˜¯UTF-8å­—ç¬¦ä¸²
        if isinstance(prompt, str):
            prompt = prompt.encode('utf-8', errors='ignore').decode('utf-8')
        
        # æ¨¡å‹ä¼˜å…ˆçº§ï¼šæ–¹æ³•å‚æ•° > å®ä¾‹è‡ªå®šä¹‰ > é»˜è®¤é…ç½®
        target_model = model or self.custom_model or self.llm_config["default_model"]
        models_to_try = [target_model] + self.llm_config["fallback_models"]
        
        try:
            from openai import OpenAI
        except ImportError:
            print("âŒ ç¼ºå°‘openaiåº“ï¼Œè¯·å®‰è£…: pip install openai")
            return self.generate_mock_summary()
            
        # åˆ›å»ºOpenRouterå®¢æˆ·ç«¯
        client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key,
        )
        
        print(f"ğŸ“ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # å°è¯•å¤šä¸ªæ¨¡å‹
        for i, current_model in enumerate(models_to_try):
            try:
                print(f"ğŸ¤– å°è¯•æ¨¡å‹ [{i+1}/{len(models_to_try)}]: {current_model}")
                
                # è°ƒç”¨API
                completion = client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://github.com/anthropics/claude-code", 
                        "X-Title": "X-Tweet-Analysis-System",
                    },
                    model=current_model,
                    messages=[
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    max_tokens=self.llm_config["max_tokens"],
                    temperature=self.llm_config["temperature"]
                )
                
                result = completion.choices[0].message.content
                print(f"âœ… LLMå“åº”å®Œæˆ: {len(result)} å­—ç¬¦ (æ¨¡å‹: {current_model})")
                return result
                
            except Exception as e:
                error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
                print(f"âŒ æ¨¡å‹ {current_model} å¤±è´¥: {error_msg}")
                if i < len(models_to_try) - 1:
                    print(f"ğŸ”„ å°è¯•å¤‡é€‰æ¨¡å‹...")
                    continue
                else:
                    print(f"ğŸ”„ æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œé™çº§åˆ°æ¨¡æ‹Ÿæ€»ç»“")
                    return self.generate_mock_summary()
    
    def generate_mock_summary(self) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ€»ç»“ï¼ˆå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        return f"""# Xæ¨æ–‡æ—¥æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- åˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}
- æ•°æ®æ¥æºï¼šXæ¨èæ—¶é—´çº¿
- æ¨æ–‡æ€»æ•°ï¼šå¤„ç†ä¸­...
- æ•°æ®è´¨é‡ï¼šç»è¿‡å®Œæ•´æ€§éªŒè¯ï¼Œè´¨é‡è‰¯å¥½

## ğŸ”¥ çƒ­é—¨è¯é¢˜
ç”±äºAPIé…ç½®é™åˆ¶ï¼Œå½“å‰æ˜¾ç¤ºæ¨¡æ‹Ÿå†…å®¹ã€‚å®é™…éƒ¨ç½²æ—¶å°†æ˜¾ç¤ºï¼š
- AIæŠ€æœ¯å‘å±•åŠ¨æ€
- ç§‘æŠ€è¡Œä¸šèµ„è®¯  
- ç¤¾äº¤åª’ä½“è¶‹åŠ¿
- ç”¨æˆ·å…³æ³¨çƒ­ç‚¹

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ
- ç”¨æˆ·æ´»è·ƒæ—¶é—´åˆ†å¸ƒ
- å†…å®¹ç±»å‹åå¥½ç»Ÿè®¡
- äº’åŠ¨æ¨¡å¼åˆ†æ
- ä¼ æ’­è·¯å¾„è¯†åˆ«

## ğŸ’¡ å…³é”®æ´å¯Ÿ
- åŸºäºçœŸå®æ•°æ®çš„æ·±åº¦åˆ†æ
- è¡Œä¸šè¶‹åŠ¿é¢„æµ‹
- ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¯†åˆ«
- å†…å®¹ç­–ç•¥å»ºè®®

## ğŸ“ æ¨æ–‡ç²¾é€‰
å°†æ ¹æ®äº’åŠ¨æ•°æ®å’Œå†…å®¹è´¨é‡è‡ªåŠ¨ç­›é€‰æœ€å…·ä»·å€¼çš„æ¨æ–‡è¿›è¡Œå±•ç¤º

---
*æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼ŒåŸºäºå®æ—¶æ¨æ–‡æ•°æ®åˆ†æ*
*é…ç½®LLM APIå¯†é’¥åå°†æ˜¾ç¤ºè¯¦ç»†æ™ºèƒ½åˆ†æç»“æœ*"""
    
    def generate_summary(self, tweets: List[Dict], summary_type: str = "daily", user_info: Dict = None, 
                       template_type: str = "auto", custom_instructions: str = "") -> Dict[str, Any]:
        """ç”Ÿæˆæ¨æ–‡æ€»ç»“"""
        print(f"ğŸ¤– å¼€å§‹ç”Ÿæˆ{summary_type}æ€»ç»“...")
        
        if not tweets:
            return {
                "error": "æ²¡æœ‰æ¨æ–‡æ•°æ®å¯ä¾›æ€»ç»“",
                "summary": "",
                "metadata": {"tweet_count": 0}
            }
        
        # ç›´æ¥ç”Ÿæˆç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
        prompt = self.generate_simple_prompt(tweets, user_info)
        
        # ä¿å­˜å®Œæ•´çš„promptåˆ°æ–‡ä»¶
        self.save_prompt_to_file(prompt, summary_type, tweets, user_info)
        
        # è°ƒç”¨LLM
        summary_text = self.call_llm_api(prompt)
        
        # æ„å»ºç»“æœ
        result = {
            "summary_type": summary_type,
            "generation_time": datetime.now().isoformat(),
            "tweet_count": len(tweets),
            "summary": summary_text,
            "metadata": {
                "original_tweets": len([t for t in tweets if not t.get('retweet')]),
                "retweets": len([t for t in tweets if t.get('retweet')]),
                "media_tweets": len([t for t in tweets if t.get('media')]),
                "data_hash": hashlib.md5(str(tweets).encode()).hexdigest()[:8],
                "user": user_info.get('screen_name') if user_info and user_info.get('screen_name') else ('mixed_users' if summary_type.endswith('_mixed') else 'unknown')
            }
        }
        
        # ä¿å­˜æ€»ç»“
        self.save_summary(result)
        
        print(f"âœ… æ€»ç»“ç”Ÿæˆå®Œæˆï¼ŒåŒ…å«{len(tweets)}æ¡æ¨æ–‡")
        return result
    
    def save_summary(self, summary_data: Dict[str, Any], format_type: str = "json") -> str:
        """ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶ - ä»…ç”¨äºæµ‹è¯•"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_type = summary_data.get('summary_type', 'general')

        # ä½¿ç”¨crawler_data/user_summariesç›®å½•
        output_dir = Path("crawler_data/user_summaries")
        output_dir.mkdir(parents=True, exist_ok=True)

        if format_type == "json":
            filename = f"{summary_type}_summary_{timestamp}.json"
            filepath = output_dir / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)

        elif format_type == "markdown":
            filename = f"{summary_type}_summary_{timestamp}.md"
            filepath = output_dir / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(summary_data['summary'])

        print(f"ğŸ’¾ æ€»ç»“å·²ä¿å­˜: {filepath}")
        return str(filepath)
    
    def generate_trending_summary(self, tweets: List[Dict], user_info: Dict = None) -> Dict[str, Any]:
        """ç”Ÿæˆçƒ­é—¨è¯é¢˜æ€»ç»“"""
        # æŒ‰äº’åŠ¨æ•°æ®æ’åº
        trending_tweets = sorted(
            tweets, 
            key=lambda x: x.get('stats', {}).get('favorite_count', 0) + 
                         x.get('stats', {}).get('retweet_count', 0), 
            reverse=True
        )
        
        return self.generate_summary(trending_tweets[:20], "trending", user_info)
    
    def generate_category_summary(self, tweets: List[Dict], category: str = "tech", user_info: Dict = None) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ç±»æ€»ç»“"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ†ç±»é€»è¾‘ï¼Œæ ¹æ®å…³é”®è¯æˆ–å…¶ä»–ç‰¹å¾ç­›é€‰æ¨æ–‡
        # æš‚æ—¶ä½¿ç”¨å…¨éƒ¨æ¨æ–‡
        return self.generate_summary(tweets, f"{category}_category", user_info)

def main():
    """æµ‹è¯•æ€»ç»“åŠŸèƒ½"""
    print("ğŸ¤– LLMæ€»ç»“æ¨¡å—æµ‹è¯•")
    
    # åˆ›å»ºæ€»ç»“å™¨
    summarizer = TwitterSummarizer()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_file = "crawler_data/daily_posts"
    
    # ç”±äºæ²¡æœ‰å®é™…çš„æ¨æ–‡æ•°æ®ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•
    mock_tweets = [
        {
            "id": "123456789",
            "text": "äººå·¥æ™ºèƒ½çš„å‘å±•é€Ÿåº¦ä»¤äººæƒŠå¹ï¼ŒGPT-4çš„èƒ½åŠ›å·²ç»è¶…å‡ºäº†å¾ˆå¤šäººçš„é¢„æœŸã€‚æœªæ¥AIå°†å¦‚ä½•æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Ÿ",
            "user": {"name": "ç§‘æŠ€è§‚å¯Ÿå‘˜", "screen_name": "tech_observer"},
            "stats": {"favorite_count": 1250, "retweet_count": 890, "reply_count": 234},
            "media": [],
            "created_at": "Mon Sep 11 12:00:00 +0000 2025"
        },
        {
            "id": "123456790",
            "text": "åˆšåˆšå‘å¸ƒäº†æ–°ç‰ˆæœ¬çš„å¼€æºé¡¹ç›®ï¼Œæ¬¢è¿å¤§å®¶è¯•ç”¨å’Œåé¦ˆï¼GitHubé“¾æ¥åœ¨è¯„è®ºä¸­ã€‚",
            "user": {"name": "å¼€å‘è€…å°ç‹", "screen_name": "dev_wang"},
            "stats": {"favorite_count": 45, "retweet_count": 12, "reply_count": 8},
            "media": [{"type": "photo", "url": "https://example.com/image.jpg"}],
            "created_at": "Mon Sep 11 10:30:00 +0000 2025"
        }
    ]
    
    # ç”Ÿæˆæ€»ç»“
    daily_summary = summarizer.generate_summary(mock_tweets, "daily")
    
    print(f"\nğŸ“„ æ€»ç»“é¢„è§ˆ:")
    print(daily_summary['summary'][:500] + "...")
    
    # ä¿å­˜ä¸ºMarkdownæ ¼å¼
    md_file = summarizer.save_summary(daily_summary, "markdown")
    print(f"ğŸ“ Markdownæ–‡ä»¶: {md_file}")

if __name__ == "__main__":
    main()