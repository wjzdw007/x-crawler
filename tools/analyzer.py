#!/usr/bin/env python3
"""
X APIåˆ†æå·¥å…· - ä½¿ç”¨Playwrightæ•è·å’Œåˆ†æAPIè¯·æ±‚
åŸºäºå®é™…è¯·æ±‚å’Œå“åº”æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸åšä»»ä½•å‡è®¾
"""

import json
import asyncio
import os
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

class XAPIAnalyzer:
    def __init__(self, data_dir="analysis_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºåˆ†ææ•°æ®å­ç›®å½•
        for subdir in ["api_responses", "baseline_tweets", "comparison_results", "problem_cases"]:
            (self.data_dir / subdir).mkdir(exist_ok=True)
            
        self.captured_requests = []
        self.analysis_results = {}
    
    async def setup_request_interception(self, page):
        """è®¾ç½®è¯·æ±‚æ‹¦æˆªï¼Œæ•è·æ‰€æœ‰XHRå’Œfetchè¯·æ±‚"""
        
        async def handle_request(request):
            # åªå…³æ³¨Xçš„APIè¯·æ±‚
            if any(domain in request.url for domain in ["twitter.com", "x.com"]):
                if request.method in ["POST", "GET"] and "/api/" in request.url:
                    print(f"ğŸ” æ•è·è¯·æ±‚: {request.method} {request.url}")
                    
                    # è®°å½•è¯·æ±‚ä¿¡æ¯
                    request_data = {
                        "timestamp": datetime.now().isoformat(),
                        "method": request.method,
                        "url": request.url,
                        "headers": await self.get_request_headers(request),
                        "post_data": request.post_data if request.method == "POST" else None
                    }
                    
                    self.captured_requests.append(request_data)
        
        async def handle_response(response):
            # æ•è·APIå“åº”
            if any(domain in response.url for domain in ["twitter.com", "x.com"]):
                if "/api/" in response.url and response.status == 200:
                    try:
                        response_data = await response.json()
                        
                        # ä¿å­˜å“åº”æ•°æ®
                        filename = f"response_{datetime.now().strftime('%H%M%S_%f')}.json"
                        filepath = self.data_dir / "api_responses" / filename
                        
                        analysis_data = {
                            "url": response.url,
                            "timestamp": datetime.now().isoformat(),
                            "status": response.status,
                            "headers": dict(response.headers),
                            "data": response_data
                        }
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
                        
                        print(f"ğŸ’¾ ä¿å­˜å“åº”: {filename}")
                        
                        # ç«‹å³åˆ†æå“åº”ç»“æ„
                        await self.analyze_response_structure(response_data, response.url)
                        
                    except Exception as e:
                        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
        
        page.on("request", handle_request)
        page.on("response", handle_response)
    
    async def get_request_headers(self, request):
        """å®‰å…¨è·å–è¯·æ±‚å¤´"""
        try:
            return dict(request.headers)
        except:
            return {}
    
    async def analyze_response_structure(self, data, url):
        """å®æ—¶åˆ†æå“åº”æ•°æ®ç»“æ„"""
        print(f"\nğŸ“Š åˆ†æå“åº”ç»“æ„: {url}")
        
        # æ£€æµ‹æ¨æ–‡æ•°æ®
        tweets = self.extract_tweets_from_response(data)
        if tweets:
            print(f"  å‘ç° {len(tweets)} æ¡æ¨æ–‡")
            
            for i, tweet in enumerate(tweets[:3]):  # åªåˆ†æå‰3æ¡
                analysis = self.analyze_single_tweet(tweet)
                print(f"  æ¨æ–‡ {i+1}: {analysis}")
    
    def extract_tweets_from_response(self, data):
        """ä»å“åº”ä¸­æå–æ¨æ–‡æ•°æ®"""
        tweets = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„æ•°æ®ç»“æ„
        possible_paths = [
            data.get('data', {}).get('home', {}).get('home_timeline_urt', {}).get('instructions', []),
            data.get('data', {}).get('user', {}).get('result', {}).get('timeline_v2', {}).get('timeline', {}).get('instructions', []),
            data.get('globalObjects', {}).get('tweets', {}),
            data.get('timeline', {}).get('instructions', [])
        ]
        
        for path in possible_paths:
            if isinstance(path, list):
                # GraphQLç±»å‹å“åº”
                for instruction in path:
                    if instruction.get('type') == 'TimelineAddEntries':
                        entries = instruction.get('entries', [])
                        for entry in entries:
                            if 'tweet' in entry.get('entryId', ''):
                                tweet_data = entry.get('content', {}).get('itemContent', {}).get('tweet_results', {}).get('result', {})
                                if tweet_data:
                                    tweets.append(tweet_data)
            elif isinstance(path, dict):
                # ç›´æ¥çš„tweetså¯¹è±¡
                for tweet_id, tweet_data in path.items():
                    tweets.append(tweet_data)
        
        return tweets
    
    def analyze_single_tweet(self, tweet):
        """åˆ†æå•æ¡æ¨æ–‡çš„å®Œæ•´æ€§"""
        analysis = {
            'has_full_text': 'full_text' in tweet,
            'has_text': 'text' in tweet,
            'has_legacy': 'legacy' in tweet,
            'text_length': 0,
            'has_media': False,
            'has_retweet': False,
            'potential_issues': []
        }
        
        # åˆ†ææ–‡æœ¬å†…å®¹
        text_field = tweet.get('full_text') or tweet.get('text') or tweet.get('legacy', {}).get('full_text', '')
        analysis['text_length'] = len(text_field)
        
        # æ£€æŸ¥æˆªæ–­æŒ‡æ ‡
        if 'truncated' in tweet and tweet['truncated']:
            analysis['potential_issues'].append('text_truncated')
        
        # æ£€æŸ¥åª’ä½“å†…å®¹
        media = tweet.get('entities', {}).get('media', []) or tweet.get('legacy', {}).get('entities', {}).get('media', [])
        analysis['has_media'] = len(media) > 0
        
        # æ£€æŸ¥è½¬æ¨
        if 'retweeted_status' in tweet or tweet.get('legacy', {}).get('retweeted_status_id_str'):
            analysis['has_retweet'] = True
        
        # æ£€æŸ¥å¯èƒ½çš„æ•°æ®ç¼ºå¤±
        essential_fields = ['id_str', 'created_at', 'user']
        for field in essential_fields:
            if field not in tweet and field not in tweet.get('legacy', {}):
                analysis['potential_issues'].append(f'missing_{field}')
        
        return analysis
    
    async def start_analysis_session(self):
        """å¼€å§‹åˆ†æä¼šè¯"""
        print("ğŸš€ å¯åŠ¨X APIåˆ†æå·¥å…·")
        print("ğŸ“‹ åˆ†æç›®æ ‡:")
        print("  1. å¸–å­å†…å®¹å®Œæ•´æ€§")
        print("  2. æ–‡æœ¬æˆªæ–­é—®é¢˜")
        print("  3. è½¬æ¨æ•°æ®ç»“æ„")
        print("  4. åª’ä½“æ–‡ä»¶URL")
        print("  5. APIè¯·æ±‚æ¨¡å¼")
        print("\nè¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•Xå¹¶è®¿é—®æ¨èæ—¶é—´çº¿...")
        
        # ä½¿ç”¨ç»å¯¹è·¯å¾„å­˜å‚¨ç”¨æˆ·æ•°æ®
        user_data_dir = os.path.join(os.getcwd(), "browser_data")
        
        async with async_playwright() as p:
            # ä½¿ç”¨launch_persistent_contextæ­£ç¡®ä¿æŒç™»å½•çŠ¶æ€
            context = await p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            # è·å–æˆ–åˆ›å»ºé¡µé¢
            if context.pages:
                page = context.pages[0]
            else:
                page = await context.new_page()
            
            # è®¾ç½®è¯·æ±‚æ‹¦æˆª
            await self.setup_request_interception(page)
            
            # è®¿é—®Xä¸»é¡µ
            print("\nğŸŒ æ­£åœ¨è®¿é—® x.com...")
            await page.goto("https://x.com/home")
            
            print("\nâœ‹ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œ:")
            print("  1. å¦‚æœªç™»å½•ï¼Œè¯·å…ˆç™»å½•")
            print("  2. æµè§ˆæ¨èæ—¶é—´çº¿")
            print("  3. ç‚¹å‡»å‡ æ¡ä¸åŒç±»å‹çš„æ¨æ–‡ï¼ˆæ™®é€šã€è½¬æ¨ã€é•¿æ–‡ã€åª’ä½“ï¼‰")
            print("  4. å±•å¼€ä¸€äº›å›å¤å’Œè¯„è®º")
            print("  5. å®Œæˆååœ¨ç»ˆç«¯æŒ‰ Ctrl+C ç»“æŸåˆ†æ")
            
            try:
                # ç­‰å¾…ç”¨æˆ·æ“ä½œ
                await asyncio.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿ
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ åˆ†æä¼šè¯ç»“æŸ")
            
            await context.close()
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            await self.generate_analysis_report()
    
    async def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“ˆ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        print(f"  æ•è·è¯·æ±‚æ•°: {len(self.captured_requests)}")
        
        # ç»Ÿè®¡APIå“åº”æ–‡ä»¶
        response_files = list((self.data_dir / "api_responses").glob("*.json"))
        print(f"  ä¿å­˜å“åº”æ•°: {len(response_files)}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        report = {
            "analysis_session": {
                "timestamp": datetime.now().isoformat(),
                "captured_requests": len(self.captured_requests),
                "response_files": len(response_files)
            },
            "api_endpoints": [],
            "data_structure_patterns": {},
            "potential_issues": []
        }
        
        # åˆ†æAPIç«¯ç‚¹æ¨¡å¼
        endpoints = set()
        for req in self.captured_requests:
            endpoint = req['url'].split('?')[0]  # å»æ‰æŸ¥è¯¢å‚æ•°
            endpoints.add(endpoint)
        
        report["api_endpoints"] = list(endpoints)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir.absolute()}")

if __name__ == "__main__":
    analyzer = XAPIAnalyzer()
    asyncio.run(analyzer.start_analysis_session())