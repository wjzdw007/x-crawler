#!/usr/bin/env python3
"""
é»„é‡‘æ•°æ®é›†æ„å»ºå·¥å…· - åŸºäºå·²æœ‰åˆ†ææ•°æ®åˆ›å»ºéªŒè¯åŸºå‡†
é€šè¿‡åˆ†æAPIå“åº”æ•°æ®ï¼Œè‡ªåŠ¨ç”ŸæˆéªŒè¯æ£€æŸ¥ç‚¹
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

class GoldenDatasetBuilder:
    def __init__(self, analysis_data_dir="analysis_data", golden_dir="golden_dataset"):
        self.analysis_data_dir = Path(analysis_data_dir)
        self.golden_dir = Path(golden_dir)
        self.golden_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        for subdir in ["baseline_data", "verification_samples", "test_cases"]:
            (self.golden_dir / subdir).mkdir(exist_ok=True)
    
    def extract_tweets_from_api_response(self, response_file: str) -> List[Dict]:
        """ä»APIå“åº”ä¸­æå–æ¨æ–‡æ•°æ®"""
        try:
            with open(response_file, 'r', encoding='utf-8') as f:
                response_data = json.load(f)
            
            data = response_data.get('data', {})
            tweets = []
            
            # ä¿®æ­£æ•°æ®è·¯å¾„ - åŸºäºè°ƒè¯•å‘ç°çš„ç»“æ„
            inner_data = data.get('data', {})
            home_timeline = inner_data.get('home', {}).get('home_timeline_urt', {})
            instructions = home_timeline.get('instructions', [])
            
            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])
                    
                    for entry in entries:
                        entry_id = entry.get('entryId', '')
                        
                        if 'tweet-' in entry_id:
                            content = entry.get('content', {})
                            item_content = content.get('itemContent', {})
                            tweet_results = item_content.get('tweet_results', {})
                            tweet_data = tweet_results.get('result', {})
                            
                            if tweet_data.get('__typename') == 'Tweet':
                                tweets.append(tweet_data)
            
            return tweets
            
        except Exception as e:
            print(f"âŒ æå–æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    def create_baseline_tweet(self, tweet_data: Dict) -> Dict:
        """åˆ›å»ºåŸºå‡†æ¨æ–‡æ•°æ® - æ­£ç¡®çš„è§£æç»“æœ"""
        try:
            # åŸºç¡€ä¿¡æ¯
            baseline = {
                "tweet_id": tweet_data.get('rest_id'),
                "source_data_path": "",
                "expected_text": "",
                "expected_created_at": tweet_data.get('legacy', {}).get('created_at'),
                "expected_lang": tweet_data.get('legacy', {}).get('lang'),
            }
            
            # æå–æ–‡æœ¬å†…å®¹ - å¤„ç†é•¿æ–‡æ¨æ–‡å’Œæ™®é€šæ¨æ–‡
            if 'note_tweet' in tweet_data:
                # é•¿æ–‡æ¨æ–‡
                note_tweet_result = tweet_data.get('note_tweet', {}).get('note_tweet_results', {}).get('result', {})
                if note_tweet_result:
                    baseline["expected_text"] = note_tweet_result.get('text', '')
                    baseline["source_data_path"] = "note_tweet.note_tweet_results.result.text"
            
            if not baseline["expected_text"]:
                # æ™®é€šæ¨æ–‡ - ä½¿ç”¨ legacy.full_text
                baseline["expected_text"] = tweet_data.get('legacy', {}).get('full_text', '')
                baseline["source_data_path"] = "legacy.full_text"
            
            # ç”¨æˆ·ä¿¡æ¯ - ä¿®æ­£å­—æ®µè·¯å¾„
            user_results = tweet_data.get('core', {}).get('user_results', {}).get('result', {})
            if user_results:
                baseline["expected_user"] = {
                    "id": user_results.get('rest_id'),
                    "name": user_results.get('core', {}).get('name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    "screen_name": user_results.get('core', {}).get('screen_name'),  # ä¿®æ­£ï¼šä»coreè·å–
                    "description": user_results.get('legacy', {}).get('description'),
                    "followers_count": user_results.get('legacy', {}).get('followers_count', 0),
                    "friends_count": user_results.get('legacy', {}).get('friends_count', 0),
                    "verified": user_results.get('verification', {}).get('verified', False),  # ä¿®æ­£è·¯å¾„
                    "is_blue_verified": user_results.get('is_blue_verified', False)
                }
            
            # ç»Ÿè®¡æ•°æ®
            legacy_data = tweet_data.get('legacy', {})
            baseline["expected_stats"] = {
                "retweet_count": legacy_data.get('retweet_count', 0),
                "favorite_count": legacy_data.get('favorite_count', 0),
                "reply_count": legacy_data.get('reply_count', 0),
                "quote_count": legacy_data.get('quote_count', 0)
            }
            
            # åª’ä½“æ–‡ä»¶
            extended_entities = legacy_data.get('extended_entities', {})
            if 'media' in extended_entities:
                baseline["expected_media"] = []
                for media_item in extended_entities['media']:
                    media_info = {
                        "type": media_item.get('type'),
                        "id": media_item.get('id_str'),
                    }
                    
                    if media_item['type'] == 'video':
                        variants = media_item.get('video_info', {}).get('variants', [])
                        # æ‰¾åˆ°æœ€é«˜è´¨é‡çš„è§†é¢‘
                        best_variant = None
                        highest_bitrate = 0
                        for variant in variants:
                            if variant.get('content_type') == 'video/mp4':
                                bitrate = variant.get('bitrate', 0)
                                if bitrate > highest_bitrate:
                                    highest_bitrate = bitrate
                                    best_variant = variant
                        
                        if best_variant:
                            media_info["expected_url"] = best_variant['url']
                            media_info["expected_bitrate"] = best_variant.get('bitrate')
                    
                    elif media_item['type'] in ['photo', 'animated_gif']:
                        media_info["expected_url"] = media_item.get('media_url_https')
                    
                    baseline["expected_media"].append(media_info)
            
            # è½¬æ¨æ•°æ®
            if 'retweeted_status_result' in legacy_data:
                retweet_result = legacy_data['retweeted_status_result'].get('result', {})
                if retweet_result:
                    baseline["is_retweet"] = True
                    baseline["expected_retweet_text"] = retweet_result.get('legacy', {}).get('full_text', '')
                    
                    # è½¬æ¨ç”¨æˆ·ä¿¡æ¯
                    retweet_user = retweet_result.get('core', {}).get('user_results', {}).get('result', {})
                    if retweet_user:
                        baseline["expected_retweet_user"] = {
                            "name": retweet_user.get('legacy', {}).get('name'),
                            "screen_name": retweet_user.get('legacy', {}).get('screen_name')
                        }
            else:
                baseline["is_retweet"] = False
            
            # å¼•ç”¨æ¨æ–‡
            if 'quoted_status_result' in tweet_data:
                quoted_result = tweet_data['quoted_status_result'].get('result', {})
                if quoted_result:
                    baseline["is_quoted"] = True
                    baseline["expected_quoted_text"] = quoted_result.get('legacy', {}).get('full_text', '')
            else:
                baseline["is_quoted"] = False
            
            # éªŒè¯æ£€æŸ¥ç‚¹
            baseline["validation_checkpoints"] = {
                "text_not_empty": len(baseline["expected_text"]) > 0,
                "has_user_info": "expected_user" in baseline,
                "timestamp_valid": baseline["expected_created_at"] is not None,
                "media_urls_valid": all(
                    m.get("expected_url", "").startswith("https://") 
                    for m in baseline.get("expected_media", [])
                ),
                "retweet_data_complete": (
                    not baseline["is_retweet"] or 
                    (len(baseline.get("expected_retweet_text", "")) > 0)
                )
            }
            
            return baseline
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºåŸºå‡†æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def build_golden_dataset_from_responses(self) -> str:
        """ä»APIå“åº”æ–‡ä»¶æ„å»ºé»„é‡‘æ•°æ®é›†"""
        print("ğŸ—ï¸ ä»APIå“åº”æ„å»ºé»„é‡‘æ•°æ®é›†...")
        
        # æŸ¥æ‰¾å“åº”æ–‡ä»¶ - æ£€æŸ¥å¤šä¸ªå¯èƒ½ä½ç½®
        possible_dirs = [
            self.analysis_data_dir / "api_responses",
            Path("."),  # å½“å‰ç›®å½•
            Path("analysis_data/api_responses")
        ]
        
        response_files = []
        for response_dir in possible_dirs:
            if response_dir.exists():
                found_files = list(response_dir.glob("response_*.json"))
                if found_files:
                    response_files.extend(found_files)
                    print(f"ğŸ“‚ åœ¨ {response_dir} å‘ç° {len(found_files)} ä¸ªå“åº”æ–‡ä»¶")
                    break
        
        print(f"ğŸ“‚ æ€»å…±å‘ç° {len(response_files)} ä¸ªå“åº”æ–‡ä»¶")
        
        all_baselines = []
        processed_tweet_ids = set()  # é¿å…é‡å¤
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„æ—¶é—´çº¿APIæ–‡ä»¶
        timeline_files = []
        for response_file in response_files:
            try:
                with open(response_file, 'r', encoding='utf-8') as f:
                    response_data = json.load(f)
                url = response_data.get('url', '')
                if 'HomeTimeline' in url or 'HomeLatestTimeline' in url:
                    timeline_files.append(response_file)
            except:
                continue
        
        print(f"ğŸ“ å‘ç° {len(timeline_files)} ä¸ªæ—¶é—´çº¿APIæ–‡ä»¶")
        
        for response_file in timeline_files[:3]:  # å¤„ç†å‰3ä¸ªæ—¶é—´çº¿æ–‡ä»¶
            print(f"  å¤„ç†: {response_file.name}")
            tweets = self.extract_tweets_from_api_response(str(response_file))
            
            for tweet_data in tweets:
                tweet_id = tweet_data.get('rest_id')
                if tweet_id and tweet_id not in processed_tweet_ids:
                    baseline = self.create_baseline_tweet(tweet_data)
                    if baseline:
                        baseline["source_file"] = response_file.name
                        all_baselines.append(baseline)
                        processed_tweet_ids.add(tweet_id)
        
        # åˆ›å»ºé»„é‡‘æ•°æ®é›†
        golden_dataset = {
            "creation_time": datetime.now().isoformat(),
            "description": "åŸºäºAPIå“åº”æ•°æ®è‡ªåŠ¨ç”Ÿæˆçš„é»„é‡‘æ•°æ®é›†",
            "source_files": [f.name for f in response_files[:5]],
            "total_tweets": len(all_baselines),
            "baseline_tweets": all_baselines,
            "validation_rules": {
                "text_completeness": {
                    "description": "éªŒè¯æ¨æ–‡æ–‡æœ¬å®Œæ•´æ€§",
                    "critical_fields": ["expected_text", "expected_created_at"]
                },
                "user_data_integrity": {
                    "description": "éªŒè¯ç”¨æˆ·ä¿¡æ¯å®Œæ•´æ€§",
                    "critical_fields": ["expected_user.name", "expected_user.screen_name"]
                },
                "media_accessibility": {
                    "description": "éªŒè¯åª’ä½“æ–‡ä»¶URLæœ‰æ•ˆæ€§",
                    "critical_fields": ["expected_media[].expected_url"]
                },
                "retweet_completeness": {
                    "description": "éªŒè¯è½¬æ¨æ•°æ®å®Œæ•´æ€§",
                    "critical_fields": ["expected_retweet_text", "expected_retweet_user"]
                }
            }
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_tweets": len(all_baselines),
            "retweets": sum(1 for b in all_baselines if b.get("is_retweet")),
            "quoted_tweets": sum(1 for b in all_baselines if b.get("is_quoted")),
            "media_tweets": sum(1 for b in all_baselines if b.get("expected_media")),
            "valid_checkpoints": sum(
                1 for b in all_baselines 
                if all(b.get("validation_checkpoints", {}).values())
            )
        }
        
        golden_dataset["statistics"] = stats
        
        # ä¿å­˜é»„é‡‘æ•°æ®é›†
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        golden_file = self.golden_dir / f"golden_dataset_{timestamp}.json"
        
        with open(golden_file, 'w', encoding='utf-8') as f:
            json.dump(golden_dataset, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é»„é‡‘æ•°æ®é›†å·²åˆ›å»º: {golden_file}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        return str(golden_file)
    
    def create_test_cases(self, golden_dataset_file: str) -> str:
        """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
        print("ğŸ§ª åˆ›å»ºæµ‹è¯•ç”¨ä¾‹...")
        
        try:
            with open(golden_dataset_file, 'r', encoding='utf-8') as f:
                golden_data = json.load(f)
        except:
            print("âŒ æ— æ³•è¯»å–é»„é‡‘æ•°æ®é›†")
            return ""
        
        baselines = golden_data.get("baseline_tweets", [])
        
        test_cases = {
            "creation_time": datetime.now().isoformat(),
            "golden_dataset_source": golden_dataset_file,
            "test_scenarios": []
        }
        
        # åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•ç”¨ä¾‹
        scenarios = [
            {
                "name": "æ™®é€šæ¨æ–‡æ–‡æœ¬å®Œæ•´æ€§æµ‹è¯•",
                "description": "æµ‹è¯•æ™®é€šæ¨æ–‡çš„æ–‡æœ¬æ˜¯å¦å®Œæ•´æå–",
                "filter": lambda b: not b.get("is_retweet") and not b.get("expected_media"),
                "key_validations": ["expected_text", "expected_user", "expected_created_at"]
            },
            {
                "name": "è½¬æ¨æ•°æ®å®Œæ•´æ€§æµ‹è¯•",
                "description": "æµ‹è¯•è½¬æ¨æ¨æ–‡çš„åŸæ–‡å’Œç”¨æˆ·ä¿¡æ¯æå–",
                "filter": lambda b: b.get("is_retweet"),
                "key_validations": ["expected_retweet_text", "expected_retweet_user"]
            },
            {
                "name": "åª’ä½“æ–‡ä»¶URLæµ‹è¯•",
                "description": "æµ‹è¯•å›¾ç‰‡å’Œè§†é¢‘URLçš„æå–",
                "filter": lambda b: b.get("expected_media"),
                "key_validations": ["expected_media"]
            }
        ]
        
        for scenario in scenarios:
            matching_baselines = [b for b in baselines if scenario["filter"](b)]
            
            if matching_baselines:
                test_case = {
                    "scenario_name": scenario["name"],
                    "description": scenario["description"],
                    "sample_count": len(matching_baselines[:10]),  # æ¯ç±»å–10ä¸ªæ ·æœ¬
                    "samples": matching_baselines[:10],
                    "key_validations": scenario["key_validations"]
                }
                test_cases["test_scenarios"].append(test_case)
        
        # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        test_file = self.golden_dir / "test_cases" / f"test_cases_{timestamp}.json"
        
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_cases, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æµ‹è¯•ç”¨ä¾‹å·²åˆ›å»º: {test_file}")
        print(f"ğŸ“‹ åˆ›å»ºäº† {len(test_cases['test_scenarios'])} ä¸ªæµ‹è¯•åœºæ™¯")
        
        return str(test_file)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—ï¸ é»„é‡‘æ•°æ®é›†æ„å»ºå·¥å…·")
    
    builder = GoldenDatasetBuilder()
    
    # æ„å»ºé»„é‡‘æ•°æ®é›†
    golden_file = builder.build_golden_dataset_from_responses()
    
    if golden_file:
        # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        test_file = builder.create_test_cases(golden_file)
        
        print(f"\nğŸ‰ å®Œæˆ!")
        print(f"é»„é‡‘æ•°æ®é›†: {golden_file}")
        print(f"æµ‹è¯•ç”¨ä¾‹: {test_file}")
        
        print(f"\nğŸ“– ä½¿ç”¨æ–¹æ³•:")
        print(f"1. ä½¿ç”¨é»„é‡‘æ•°æ®é›†éªŒè¯çˆ¬è™«è¾“å‡º:")
        print(f"   validator.comprehensive_validation(tweets, '{golden_file}')")
        print(f"2. è¿è¡Œæµ‹è¯•ç”¨ä¾‹:")
        print(f"   ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•")
    else:
        print("âŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå“åº”æ•°æ®")

if __name__ == "__main__":
    main()