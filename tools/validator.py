#!/usr/bin/env python3
"""
æ•°æ®å®Œæ•´æ€§éªŒè¯ç³»ç»Ÿ - ç¡®ä¿æŠ“å–æ•°æ®æ­£ç¡®æ€§å’Œå®Œæ•´æ€§
åŸºäºé»„é‡‘æ•°æ®é›†å’Œå¤šç»´åº¦éªŒè¯æœºåˆ¶
"""

import json
import os
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import requests
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    is_valid: bool
    score: float  # 0-100åˆ†
    issues: List[str]
    details: Dict[str, Any]

class DataValidator:
    def __init__(self, data_dir="analysis_data", golden_dataset_dir="golden_dataset"):
        self.data_dir = Path(data_dir)
        self.golden_dir = Path(golden_dataset_dir)
        self.golden_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºéªŒè¯æ•°æ®ç›®å½•
        for subdir in ["baseline_data", "validation_reports", "comparison_results"]:
            (self.golden_dir / subdir).mkdir(exist_ok=True)
        
        # éªŒè¯è§„åˆ™é…ç½®
        self.validation_rules = {
            "text_completeness": {
                "min_avg_length": 20,  # æ¨æ–‡å¹³å‡é•¿åº¦ä¸åº”è¿‡çŸ­
                "truncated_ratio_threshold": 0.05,  # æˆªæ–­æ¨æ–‡æ¯”ä¾‹ä¸è¶…è¿‡5%
                "empty_text_threshold": 0.01  # ç©ºæ–‡æœ¬æ¯”ä¾‹ä¸è¶…è¿‡1%
            },
            "retweet_integrity": {
                "retweet_original_text_required": True,
                "nested_retweet_support": True,
                "retweet_user_info_required": True
            },
            "media_completeness": {
                "media_url_accessibility": True,
                "video_quality_variants": True,
                "image_url_format_check": True
            },
            "data_structure": {
                "required_fields": ["id", "text", "created_at", "user"],
                "user_required_fields": ["name", "screen_name"],
                "timestamp_format_check": True
            }
        }
    
    def create_golden_dataset(self, browser_visible_posts: List[Dict], api_response_file: str) -> str:
        """åˆ›å»ºé»„é‡‘æ•°æ®é›† - äººå·¥éªŒè¯çš„æ­£ç¡®æ•°æ®"""
        print("ğŸ† åˆ›å»ºé»„é‡‘æ•°æ®é›†...")
        
        # åŠ è½½APIå“åº”æ•°æ®
        with open(api_response_file, 'r', encoding='utf-8') as f:
            api_data = json.load(f)
        
        golden_dataset = {
            "creation_time": datetime.now().isoformat(),
            "source_api_file": api_response_file,
            "verification_method": "manual_browser_comparison",
            "posts": [],
            "validation_checkpoints": {
                "text_integrity": {},
                "media_files": {},
                "retweet_data": {},
                "user_info": {}
            }
        }
        
        for i, browser_post in enumerate(browser_visible_posts):
            # åˆ›å»ºéªŒè¯æ£€æŸ¥ç‚¹
            checkpoint = {
                "browser_visible_text": browser_post["text"],
                "browser_visible_media": browser_post.get("media", []),
                "browser_user_info": browser_post["user"],
                "browser_stats": browser_post.get("stats", {}),
                "api_extraction_path": browser_post.get("api_path", ""),
                "validation_status": "verified",
                "known_issues": browser_post.get("known_issues", [])
            }
            
            golden_dataset["posts"].append(checkpoint)
        
        # ä¿å­˜é»„é‡‘æ•°æ®é›†
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        golden_file = self.golden_dir / f"golden_dataset_{timestamp}.json"
        
        with open(golden_file, 'w', encoding='utf-8') as f:
            json.dump(golden_dataset, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ é»„é‡‘æ•°æ®é›†å·²ä¿å­˜: {golden_file}")
        return str(golden_file)
    
    def validate_text_completeness(self, tweets: List[Dict]) -> ValidationResult:
        """éªŒè¯æ–‡æœ¬å®Œæ•´æ€§"""
        issues = []
        details = {}
        
        if not tweets:
            return ValidationResult(False, 0, ["æ²¡æœ‰æ¨æ–‡æ•°æ®"], {})
        
        # ç»Ÿè®¡æ–‡æœ¬é•¿åº¦
        text_lengths = [len(tweet.get('text', '')) for tweet in tweets]
        avg_length = sum(text_lengths) / len(text_lengths)
        
        empty_texts = sum(1 for length in text_lengths if length == 0)
        empty_ratio = empty_texts / len(tweets)
        
        # æ£€æŸ¥æˆªæ–­æ ‡è¯†
        truncated_count = sum(1 for tweet in tweets if tweet.get('truncated', False))
        truncated_ratio = truncated_count / len(tweets)
        
        details = {
            "total_tweets": len(tweets),
            "avg_text_length": avg_length,
            "empty_text_count": empty_texts,
            "empty_text_ratio": empty_ratio,
            "truncated_count": truncated_count,
            "truncated_ratio": truncated_ratio,
            "min_length": min(text_lengths) if text_lengths else 0,
            "max_length": max(text_lengths) if text_lengths else 0
        }
        
        score = 100
        
        # è¯„åˆ†è§„åˆ™
        if avg_length < self.validation_rules["text_completeness"]["min_avg_length"]:
            issues.append(f"å¹³å‡æ–‡æœ¬é•¿åº¦è¿‡çŸ­: {avg_length:.1f}")
            score -= 30
        
        if empty_ratio > self.validation_rules["text_completeness"]["empty_text_threshold"]:
            issues.append(f"ç©ºæ–‡æœ¬æ¯”ä¾‹è¿‡é«˜: {empty_ratio:.3f}")
            score -= 40
        
        if truncated_ratio > self.validation_rules["text_completeness"]["truncated_ratio_threshold"]:
            issues.append(f"æˆªæ–­æ–‡æœ¬æ¯”ä¾‹è¿‡é«˜: {truncated_ratio:.3f}")
            score -= 25
        
        return ValidationResult(
            is_valid=len(issues) == 0,
            score=max(0, score),
            issues=issues,
            details=details
        )
    
    def validate_retweet_integrity(self, tweets: List[Dict]) -> ValidationResult:
        """éªŒè¯è½¬æ¨å®Œæ•´æ€§"""
        issues = []
        details = {}
        
        retweets = [tweet for tweet in tweets if tweet.get('retweet')]
        details['total_retweets'] = len(retweets)
        details['retweet_ratio'] = len(retweets) / len(tweets) if tweets else 0
        
        if not retweets:
            return ValidationResult(True, 100, [], details)
        
        # æ£€æŸ¥è½¬æ¨åŸæ–‡å®Œæ•´æ€§
        missing_original_text = 0
        missing_original_user = 0
        nested_retweets = 0
        
        for tweet in retweets:
            retweet_data = tweet.get('retweet', {})
            
            # æ£€æŸ¥åŸæ–‡
            if not retweet_data.get('text'):
                missing_original_text += 1
            
            # æ£€æŸ¥åŸç”¨æˆ·ä¿¡æ¯
            if not retweet_data.get('user'):
                missing_original_user += 1
            
            # æ£€æŸ¥åµŒå¥—è½¬æ¨
            if retweet_data.get('retweet'):
                nested_retweets += 1
        
        details.update({
            "missing_original_text": missing_original_text,
            "missing_original_user": missing_original_user,
            "nested_retweets": nested_retweets
        })
        
        score = 100
        
        if missing_original_text > 0:
            issues.append(f"ç¼ºå¤±è½¬æ¨åŸæ–‡: {missing_original_text} æ¡")
            score -= (missing_original_text / len(retweets)) * 50
        
        if missing_original_user > 0:
            issues.append(f"ç¼ºå¤±è½¬æ¨åŸç”¨æˆ·ä¿¡æ¯: {missing_original_user} æ¡")
            score -= (missing_original_user / len(retweets)) * 30
        
        return ValidationResult(
            is_valid=len(issues) == 0,
            score=max(0, score),
            issues=issues,
            details=details
        )
    
    def validate_media_accessibility(self, tweets: List[Dict]) -> ValidationResult:
        """éªŒè¯åª’ä½“æ–‡ä»¶å¯è®¿é—®æ€§"""
        issues = []
        details = {}
        
        media_tweets = [tweet for tweet in tweets if tweet.get('media')]
        all_media = []
        
        for tweet in media_tweets:
            all_media.extend(tweet.get('media', []))
        
        details['total_media_files'] = len(all_media)
        details['media_tweets'] = len(media_tweets)
        
        if not all_media:
            return ValidationResult(True, 100, [], details)
        
        # æ£€æŸ¥åª’ä½“URLæ ¼å¼
        valid_urls = 0
        invalid_urls = 0
        accessible_urls = 0
        video_files = 0
        image_files = 0
        
        for media in all_media:
            url = media.get('url')
            media_type = media.get('type')
            
            if media_type == 'video':
                video_files += 1
            elif media_type in ['photo', 'animated_gif']:
                image_files += 1
            
            if not url:
                invalid_urls += 1
                continue
            
            # æ£€æŸ¥URLæ ¼å¼
            if url.startswith('https://') and ('twimg.com' in url or 'twitter.com' in url):
                valid_urls += 1
                
                # æµ‹è¯•å¯è®¿é—®æ€§ï¼ˆé‡‡æ ·æ£€æŸ¥ï¼Œé¿å…è¿‡å¤šè¯·æ±‚ï¼‰
                if len(all_media) <= 10 or valid_urls <= 3:
                    try:
                        response = requests.head(url, timeout=5)
                        if response.status_code == 200:
                            accessible_urls += 1
                    except:
                        pass
            else:
                invalid_urls += 1
        
        details.update({
            "valid_urls": valid_urls,
            "invalid_urls": invalid_urls,
            "accessible_urls": accessible_urls,
            "video_files": video_files,
            "image_files": image_files
        })
        
        score = 100
        
        if invalid_urls > 0:
            invalid_ratio = invalid_urls / len(all_media)
            issues.append(f"æ— æ•ˆåª’ä½“URL: {invalid_urls} ä¸ª ({invalid_ratio:.2%})")
            score -= invalid_ratio * 60
        
        return ValidationResult(
            is_valid=len(issues) == 0,
            score=max(0, score),
            issues=issues,
            details=details
        )
    
    def validate_data_structure(self, tweets: List[Dict]) -> ValidationResult:
        """éªŒè¯æ•°æ®ç»“æ„å®Œæ•´æ€§"""
        issues = []
        details = {}
        
        if not tweets:
            return ValidationResult(False, 0, ["æ²¡æœ‰æ¨æ–‡æ•°æ®"], {})
        
        required_fields = self.validation_rules["data_structure"]["required_fields"]
        user_required_fields = self.validation_rules["data_structure"]["user_required_fields"]
        
        missing_field_counts = {field: 0 for field in required_fields}
        missing_user_field_counts = {field: 0 for field in user_required_fields}
        
        valid_timestamps = 0
        
        for tweet in tweets:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in required_fields:
                if field not in tweet or tweet[field] is None:
                    missing_field_counts[field] += 1
            
            # æ£€æŸ¥ç”¨æˆ·å­—æ®µ
            user = tweet.get('user', {})
            if user:
                for field in user_required_fields:
                    if field not in user or user[field] is None:
                        missing_user_field_counts[field] += 1
            
            # æ£€æŸ¥æ—¶é—´æˆ³æ ¼å¼
            created_at = tweet.get('created_at')
            if created_at:
                try:
                    # Xçš„æ—¶é—´æ ¼å¼: "Wed Oct 05 22:34:12 +0000 2011"
                    from dateutil.parser import parse
                    parse(created_at)
                    valid_timestamps += 1
                except:
                    pass
        
        details = {
            "total_tweets": len(tweets),
            "missing_fields": missing_field_counts,
            "missing_user_fields": missing_user_field_counts,
            "valid_timestamps": valid_timestamps,
            "timestamp_validity_ratio": valid_timestamps / len(tweets)
        }
        
        score = 100
        
        # è¯„åˆ†
        for field, count in missing_field_counts.items():
            if count > 0:
                ratio = count / len(tweets)
                issues.append(f"ç¼ºå¤±{field}å­—æ®µ: {count} æ¡ ({ratio:.2%})")
                score -= ratio * 30
        
        for field, count in missing_user_field_counts.items():
            if count > 0:
                ratio = count / len(tweets)
                issues.append(f"ç¼ºå¤±ç”¨æˆ·{field}å­—æ®µ: {count} æ¡ ({ratio:.2%})")
                score -= ratio * 20
        
        timestamp_invalid_ratio = 1 - (valid_timestamps / len(tweets))
        if timestamp_invalid_ratio > 0.1:  # è¶…è¿‡10%çš„æ—¶é—´æˆ³æ— æ•ˆ
            issues.append(f"æ—¶é—´æˆ³æ ¼å¼é”™è¯¯æ¯”ä¾‹: {timestamp_invalid_ratio:.2%}")
            score -= timestamp_invalid_ratio * 25
        
        return ValidationResult(
            is_valid=len(issues) == 0,
            score=max(0, score),
            issues=issues,
            details=details
        )
    
    def compare_with_golden_dataset(self, tweets: List[Dict], golden_dataset_file: str) -> ValidationResult:
        """ä¸é»„é‡‘æ•°æ®é›†å¯¹æ¯”"""
        issues = []
        details = {}
        
        try:
            with open(golden_dataset_file, 'r', encoding='utf-8') as f:
                golden_data = json.load(f)
        except:
            return ValidationResult(False, 0, ["æ— æ³•è¯»å–é»„é‡‘æ•°æ®é›†"], {})
        
        # ä¿®æ­£ï¼šæˆ‘ä»¬çš„é»„é‡‘æ•°æ®é›†ä¸­å­—æ®µåæ˜¯baseline_tweetsï¼Œä¸æ˜¯posts
        golden_posts = golden_data.get('baseline_tweets', [])
        
        if len(tweets) < len(golden_posts) * 0.8:  # å…è®¸20%çš„æ•°æ®å·®å¼‚
            issues.append(f"æ¨æ–‡æ•°é‡æ˜¾è‘—å‡å°‘: {len(tweets)} vs {len(golden_posts)}")
        
        # æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆé‡‡æ ·å‰10æ¡ï¼‰
        text_match_count = 0
        sample_size = min(10, len(tweets), len(golden_posts))
        
        for i in range(sample_size):
            tweet_text = tweets[i].get('text', '').strip()
            # ä¿®æ­£ï¼šæˆ‘ä»¬çš„é»„é‡‘æ•°æ®é›†ä¸­å­—æ®µåæ˜¯expected_textï¼Œä¸æ˜¯browser_visible_text
            golden_text = golden_posts[i].get('expected_text', '').strip()
            
            # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥
            if tweet_text and golden_text:
                if tweet_text == golden_text:
                    text_match_count += 1
                elif tweet_text in golden_text or golden_text in tweet_text:
                    text_match_count += 0.5
        
        text_match_ratio = text_match_count / sample_size if sample_size > 0 else 0
        
        details = {
            "golden_posts_count": len(golden_posts),
            "current_posts_count": len(tweets),
            "sample_text_match_ratio": text_match_ratio,
            "sample_size": sample_size
        }
        
        score = 100
        
        if text_match_ratio < 0.8:
            issues.append(f"æ–‡æœ¬åŒ¹é…ç‡è¿‡ä½: {text_match_ratio:.2%}")
            score -= (1 - text_match_ratio) * 50
        
        return ValidationResult(
            is_valid=text_match_ratio >= 0.8,
            score=max(0, score),
            issues=issues,
            details=details
        )
    
    def comprehensive_validation(self, tweets: List[Dict], golden_dataset_file: Optional[str] = None) -> Dict[str, ValidationResult]:
        """ç»¼åˆéªŒè¯"""
        print("ğŸ” å¼€å§‹ç»¼åˆæ•°æ®éªŒè¯...")
        
        results = {}
        
        # æ–‡æœ¬å®Œæ•´æ€§éªŒè¯
        print("  1. éªŒè¯æ–‡æœ¬å®Œæ•´æ€§...")
        results['text_completeness'] = self.validate_text_completeness(tweets)
        
        # è½¬æ¨å®Œæ•´æ€§éªŒè¯
        print("  2. éªŒè¯è½¬æ¨å®Œæ•´æ€§...")
        results['retweet_integrity'] = self.validate_retweet_integrity(tweets)
        
        # åª’ä½“æ–‡ä»¶éªŒè¯
        print("  3. éªŒè¯åª’ä½“æ–‡ä»¶...")
        results['media_accessibility'] = self.validate_media_accessibility(tweets)
        
        # æ•°æ®ç»“æ„éªŒè¯
        print("  4. éªŒè¯æ•°æ®ç»“æ„...")
        results['data_structure'] = self.validate_data_structure(tweets)
        
        # é»„é‡‘æ•°æ®é›†å¯¹æ¯”
        if golden_dataset_file and os.path.exists(golden_dataset_file):
            print("  5. å¯¹æ¯”é»„é‡‘æ•°æ®é›†...")
            results['golden_comparison'] = self.compare_with_golden_dataset(tweets, golden_dataset_file)
        
        return results
    
    def generate_validation_report(self, validation_results: Dict[str, ValidationResult], tweets: List[Dict]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.golden_dir / "validation_reports" / f"validation_report_{timestamp}.json"
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        scores = [result.score for result in validation_results.values()]
        overall_score = sum(scores) / len(scores) if scores else 0
        
        all_issues = []
        for category, result in validation_results.items():
            all_issues.extend([f"[{category}] {issue}" for issue in result.issues])
        
        report = {
            "validation_time": datetime.now().isoformat(),
            "tweet_count": len(tweets),
            "overall_score": overall_score,
            "overall_status": "PASS" if overall_score >= 80 else "FAIL",
            "category_results": {},
            "all_issues": all_issues,
            "recommendations": []
        }
        
        # è¯¦ç»†ç»“æœ
        for category, result in validation_results.items():
            report["category_results"][category] = {
                "score": result.score,
                "is_valid": result.is_valid,
                "issues": result.issues,
                "details": result.details
            }
        
        # ç”Ÿæˆå»ºè®®
        if overall_score < 80:
            report["recommendations"] = self.generate_improvement_recommendations(validation_results)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)
    
    def generate_improvement_recommendations(self, results: Dict[str, ValidationResult]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        for category, result in results.items():
            if result.score < 80:
                if category == 'text_completeness':
                    recommendations.append("æ£€æŸ¥æ–‡æœ¬æå–è·¯å¾„ï¼Œç¡®ä¿ä½¿ç”¨ legacy.full_text å­—æ®µ")
                elif category == 'retweet_integrity':
                    recommendations.append("æ£€æŸ¥è½¬æ¨è§£æé€»è¾‘ï¼Œç¡®ä¿æå– retweeted_status_result æ•°æ®")
                elif category == 'media_accessibility':
                    recommendations.append("æ£€æŸ¥åª’ä½“URLæå–è·¯å¾„ï¼Œç¡®ä¿ä» extended_entities.media è·å–")
                elif category == 'data_structure':
                    recommendations.append("æ£€æŸ¥å¿…éœ€å­—æ®µçš„æå–é€»è¾‘ï¼Œç¡®ä¿æ•°æ®ç»“æ„å®Œæ•´")
        
        return recommendations
    
    def print_validation_summary(self, validation_results: Dict[str, ValidationResult]):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        print("\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦:")
        print("=" * 50)
        
        scores = []
        for category, result in validation_results.items():
            status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
            print(f"{category:20} {status:8} åˆ†æ•°: {result.score:5.1f}")
            scores.append(result.score)
        
        overall_score = sum(scores) / len(scores) if scores else 0
        overall_status = "âœ… PASS" if overall_score >= 80 else "âŒ FAIL"
        
        print("-" * 50)
        print(f"{'æ€»ä½“è¯„åˆ†':20} {overall_status:8} åˆ†æ•°: {overall_score:5.1f}")
        
        # æ˜¾ç¤ºä¸»è¦é—®é¢˜
        all_issues = []
        for result in validation_results.values():
            all_issues.extend(result.issues)
        
        if all_issues:
            print(f"\nğŸš¨ å‘ç° {len(all_issues)} ä¸ªé—®é¢˜:")
            for issue in all_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  â€¢ {issue}")
            
            if len(all_issues) > 10:
                print(f"  ... è¿˜æœ‰ {len(all_issues) - 10} ä¸ªé—®é¢˜")

def main():
    """éªŒè¯å·¥å…·ä¸»å‡½æ•°"""
    print("ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯å·¥å…·")
    
    validator = DataValidator()
    
    # ç¤ºä¾‹ï¼šåˆ›å»ºé»„é‡‘æ•°æ®é›†
    print("\n1. åˆ›å»ºé»„é‡‘æ•°æ®é›† (éœ€è¦æ‰‹åŠ¨éªŒè¯çš„æµè§ˆå™¨æ•°æ®)")
    print("è¯·å‡†å¤‡ä»¥ä¸‹æ•°æ®:")
    print("- browser_visible_posts: æµè§ˆå™¨ä¸­çœ‹åˆ°çš„æ¨æ–‡æ•°æ®")
    print("- api_response_file: å¯¹åº”çš„APIå“åº”æ–‡ä»¶")
    
    # ç¤ºä¾‹ï¼šéªŒè¯æ•°æ®
    print("\n2. æ•°æ®éªŒè¯ç¤ºä¾‹")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("validator.comprehensive_validation(tweets, golden_dataset_file)")

if __name__ == "__main__":
    main()